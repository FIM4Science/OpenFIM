{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook tries to fit tick's EM model to the real world datasets to get a feeling what their kernels look like. Because the EM algorithm takes a single (long) path, we concatenate the paths of the real world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tick.hawkes import HawkesEM\n",
    "from tick.plot import plot_hawkes_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/evaluation/hawkes/mimic_II.pkl\", \"rb\") as f:\n",
    "    mimic = pickle.load(f)\n",
    "\n",
    "with open(\"../../data/evaluation/hawkes/mooc.pkl\", \"rb\") as f:\n",
    "    mooc = pickle.load(f)\n",
    "\n",
    "with open(\"../../data/evaluation/hawkes/stackOverflow.pkl\", \"rb\") as f:\n",
    "    stack = pickle.load(f)\n",
    "\n",
    "with open(\"../../data/evaluation/hawkes/retweet.pkl\", \"rb\") as f:\n",
    "    retweet = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['timestamps', 'types', 'lengths', 'timeintervals'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweet.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweet[\"timestamps\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweet[\"timestamps\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_marks(types_data):\n",
    "    return np.unique(np.concatenate(types_data)).max() + 1\n",
    "\n",
    "\n",
    "def merge_to_single_path(dataset):\n",
    "    \"\"\"\n",
    "    Merge all paths to a single path.\n",
    "    We return the timestamps for every mark separately.\n",
    "    \"\"\"\n",
    "    num_marks = get_number_of_marks(dataset[\"types\"])\n",
    "    res = [[] for _ in range(num_marks)]\n",
    "    for path_idx in range(len(dataset[\"types\"])):\n",
    "        marks = np.unique(dataset[\"types\"][path_idx])\n",
    "        for mark in marks:\n",
    "            prev_time = 0\n",
    "            if len(res[mark]) > 0 and res[mark][-1] != 0:\n",
    "                prev_time = res[mark][-1]\n",
    "            time_stamps = dataset[\"timestamps\"][path_idx][dataset[\"types\"][path_idx] == mark]\n",
    "            # Add the previous time to the timestamps\n",
    "            time_stamps = [time + prev_time for time in time_stamps]\n",
    "            res[mark] += time_stamps\n",
    "\n",
    "    for i in range(len(res)):\n",
    "        res[i] = np.array(res[i])\n",
    "    return res\n",
    "\n",
    "\n",
    "def normalize_timestamps(timestamps):\n",
    "    \"\"\"\n",
    "    Normalize the timestamps to have max delta time of 1\n",
    "    \"\"\"\n",
    "    flattened_times = np.concatenate(timestamps)\n",
    "    max_time = np.diff(flattened_times).max()\n",
    "    return [time / max_time for time in timestamps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em = HawkesEM(1, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "# timestamps = merge_to_single_path(mimic)\n",
    "# # timestamps = normalize_timestamps(timestamps)\n",
    "\n",
    "# num_marks_to_consider = 6\n",
    "# em.fit(timestamps[:num_marks_to_consider])\n",
    "\n",
    "# fig = plot_hawkes_kernels(em, show=True)\n",
    "# fig.set_size_inches(15, 10)  # Adjust the size as needed\n",
    "# plt.tight_layout()  # Adjust the layout to prevent overlap\n",
    "\n",
    "# plt.show()\n",
    "# plt.savefig(\"mimic.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em = HawkesEM(0.04, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "# timestamps = merge_to_single_path(mooc)\n",
    "# # timestamps = normalize_timestamps(timestamps)\n",
    "\n",
    "# num_marks_to_consider = 6\n",
    "# em.fit(timestamps[:num_marks_to_consider])\n",
    "\n",
    "# fig = plot_hawkes_kernels(em, show=True)\n",
    "# fig.set_size_inches(15, 10)  # Adjust the size as needed\n",
    "# plt.tight_layout()  # Adjust the layout to prevent overlap\n",
    "\n",
    "# plt.show()\n",
    "# plt.savefig(\"mooc.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em = HawkesEM(30, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "# timestamps = merge_to_single_path(stack)\n",
    "# # timestamps = normalize_timestamps(timestamps)\n",
    "\n",
    "# num_marks_to_consider = 6\n",
    "# em.fit(timestamps[:num_marks_to_consider])\n",
    "\n",
    "# fig = plot_hawkes_kernels(em, show=True)\n",
    "# fig.set_size_inches(15, 10)  # Adjust the size as needed\n",
    "# plt.tight_layout()  # Adjust the layout to prevent overlap\n",
    "\n",
    "# plt.show()\n",
    "# plt.savefig(\"stack.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em = HawkesEM(150, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "# timestamps = merge_to_single_path(retweet)\n",
    "# # timestamps = normalize_timestamps(timestamps)\n",
    "\n",
    "# num_marks_to_consider = 3\n",
    "# em.fit(timestamps[:num_marks_to_consider])\n",
    "\n",
    "# fig = plot_hawkes_kernels(em, show=True)\n",
    "# fig.set_size_inches(15, 10)  # Adjust the size as needed\n",
    "# plt.tight_layout()  # Adjust the layout to prevent overlap\n",
    "\n",
    "# plt.show()\n",
    "# plt.savefig(\"retweet.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for EasyTPP-format datasets (Hugging Face)\n",
    "\n",
    "\n",
    "def get_num_marks_easytpp(ds_dict):\n",
    "    if len(ds_dict.get(\"type_event\", [])) == 0:\n",
    "        return 0\n",
    "    max_type = -1\n",
    "    for seq_types in ds_dict[\"type_event\"]:\n",
    "        if len(seq_types):\n",
    "            max_type = max(max_type, int(np.max(seq_types)))\n",
    "    return max_type + 1 if max_type >= 0 else 0\n",
    "\n",
    "\n",
    "def merge_easytpp_to_single_path(ds_dict, max_sequences=None):\n",
    "    \"\"\"\n",
    "    Merge EasyTPP dict-of-lists into a single path of per-mark timestamps\n",
    "    suitable for tick.HawkesEM.\n",
    "\n",
    "    Returns: list[np.ndarray] of length M (num marks)\n",
    "    \"\"\"\n",
    "    num_seqs = len(ds_dict.get(\"seq_len\", []))\n",
    "    if num_seqs == 0:\n",
    "        return []\n",
    "    M = get_num_marks_easytpp(ds_dict)\n",
    "    res = [[] for _ in range(M)]\n",
    "\n",
    "    limit = num_seqs if max_sequences is None else min(num_seqs, int(max_sequences))\n",
    "    for i in range(limit):\n",
    "        times = np.array(ds_dict[\"time_since_start\"][i], dtype=float)\n",
    "        types = np.array(ds_dict[\"type_event\"][i], dtype=int)\n",
    "        if times.size == 0:\n",
    "            continue\n",
    "        marks_in_seq = np.unique(types).astype(int)\n",
    "        for m in marks_in_seq:\n",
    "            idx = types == m\n",
    "            ts_m = times[idx]\n",
    "            if ts_m.size == 0:\n",
    "                continue\n",
    "            prev_time = res[m][-1] if len(res[m]) > 0 else 0.0\n",
    "            shifted = (ts_m + prev_time).tolist()\n",
    "            res[m].extend(shifted)\n",
    "\n",
    "    for j in range(M):\n",
    "        res[j] = np.array(res[j], dtype=float)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define manual EasyTPP loader above usage to avoid NameError\n",
    "import inspect\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import snapshot_download\n",
    "except Exception:\n",
    "    snapshot_download = None\n",
    "\n",
    "\n",
    "# ---------- file discovery / parsing ----------\n",
    "\n",
    "\n",
    "def _find_split_files(base_dir: str, split: str) -> List[str]:\n",
    "    split_low = split.lower()\n",
    "    hits = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for name in files:\n",
    "            low = name.lower()\n",
    "            if split_low in low and (\n",
    "                low.endswith(\".json\") or low.endswith(\".jsonl\") or low.endswith(\".json.gz\") or low.endswith(\".jsonl.gz\")\n",
    "            ):\n",
    "                hits.append(os.path.join(root, name))\n",
    "    return sorted(hits)\n",
    "\n",
    "\n",
    "def _open_textmaybe_gz(path: str):\n",
    "    if path.endswith(\".gz\"):\n",
    "        import gzip\n",
    "\n",
    "        return gzip.open(path, \"rt\", encoding=\"utf-8\")\n",
    "    return io.open(path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def _read_json_records(path: str) -> List[Dict[str, Any]]:\n",
    "    with _open_textmaybe_gz(path) as f:\n",
    "        head = f.read(2048)\n",
    "        f.seek(0)\n",
    "        if head.lstrip().startswith(\"[\"):\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, dict):\n",
    "                for v in data.values():\n",
    "                    if isinstance(v, list):\n",
    "                        data = v\n",
    "                        break\n",
    "            return data if isinstance(data, list) else []\n",
    "        else:\n",
    "            recs = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    recs.append(json.loads(line))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return recs\n",
    "\n",
    "\n",
    "def _records_to_easytpp_dict(records: List[Dict[str, Any]]) -> Dict[str, List[List[Any]]]:\n",
    "    res = {\"time_since_start\": [], \"time_since_last_event\": [], \"type_event\": [], \"seq_len\": []}\n",
    "    for r in records:\n",
    "        times = None\n",
    "        types = None\n",
    "        if isinstance(r, dict):\n",
    "            if \"time_since_start\" in r and \"type_event\" in r:\n",
    "                times = r.get(\"time_since_start\")\n",
    "                types = r.get(\"type_event\")\n",
    "            elif \"timestamps\" in r and \"types\" in r:\n",
    "                times = r.get(\"timestamps\")\n",
    "                types = r.get(\"types\")\n",
    "            elif \"time\" in r and \"type\" in r:\n",
    "                times = r.get(\"time\")\n",
    "                types = r.get(\"type\")\n",
    "        if not isinstance(times, (list, tuple)) or not isinstance(types, (list, tuple)):\n",
    "            continue\n",
    "        if len(times) != len(types) or len(times) == 0:\n",
    "            continue\n",
    "        times = [float(t) for t in times]\n",
    "        types = [int(c) for c in types]\n",
    "        deltas = [0.0] + [times[i] - times[i - 1] for i in range(1, len(times))]\n",
    "        res[\"time_since_start\"].append(times)\n",
    "        res[\"time_since_last_event\"].append(deltas)\n",
    "        res[\"type_event\"].append(types)\n",
    "        res[\"seq_len\"].append(len(times))\n",
    "    return res\n",
    "\n",
    "\n",
    "# ---------- HTTP fallback (no git, no Arrow) ----------\n",
    "\n",
    "\n",
    "def _http_try_download_split(repo_id: str, split: str, token: str) -> str:\n",
    "    import requests\n",
    "\n",
    "    repo = repo_id.split(\"datasets/\")[-1]\n",
    "    # common filenames\n",
    "    cands = [\n",
    "        f\"{split}.jsonl\",\n",
    "        f\"{split}.json\",\n",
    "        f\"{split}.jsonl.gz\",\n",
    "        f\"{split}.json.gz\",\n",
    "        f\"data/{split}.jsonl\",\n",
    "        f\"data/{split}.json\",\n",
    "        f\"data/{split}.jsonl.gz\",\n",
    "        f\"data/{split}.json.gz\",\n",
    "    ]\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"} if token else {}\n",
    "    cache_dir = os.path.join(os.path.expanduser(\"~/.cache\"), \"hf_ds_http\", repo.replace(\"/\", \"_\"))\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    base_urls = [\n",
    "        f\"https://huggingface.co/datasets/{repo}/resolve/main/\",\n",
    "        f\"https://huggingface.co/{repo}/resolve/main/\",\n",
    "    ]\n",
    "    for base in base_urls:\n",
    "        for cand in cands:\n",
    "            url = base + cand\n",
    "            r = requests.get(url, headers=headers, stream=True)\n",
    "            if r.status_code == 200:\n",
    "                local_path = os.path.join(cache_dir, cand.replace(\"/\", \"_\"))\n",
    "                with open(local_path, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                return local_path\n",
    "    raise RuntimeError(\"Could not fetch split file via HTTP for %s (%s)\" % (repo_id, split))\n",
    "\n",
    "\n",
    "# ---------- snapshot or HTTP ----------\n",
    "\n",
    "\n",
    "def _snapshot_dataset(repo_id: str, token: str) -> str:\n",
    "    # Try huggingface_hub if present\n",
    "    if snapshot_download is not None:\n",
    "        try:\n",
    "            sig = inspect.signature(snapshot_download)\n",
    "            if \"repo_type\" in sig.parameters:\n",
    "                return snapshot_download(repo_id, repo_type=\"dataset\", use_auth_token=token)\n",
    "        except Exception:\n",
    "            pass\n",
    "        for rid in ((\"datasets/\" + repo_id) if not repo_id.startswith(\"datasets/\") else repo_id, repo_id):\n",
    "            try:\n",
    "                return snapshot_download(rid, use_auth_token=token)\n",
    "            except Exception:\n",
    "                continue\n",
    "    # Fall back to HTTP single-file download into a temp dir\n",
    "    local_file = _http_try_download_split(repo_id, split=\"train\", token=token)\n",
    "    base_dir = os.path.dirname(local_file)\n",
    "    return base_dir\n",
    "\n",
    "\n",
    "def load_easytpp_as_dict(repo_id: str, split: str) -> Dict[str, List[List[Any]]]:\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "    local_dir = _snapshot_dataset(repo_id, token)\n",
    "    candidates = _find_split_files(local_dir, split)\n",
    "    if not candidates:\n",
    "        # try HTTP direct for this split\n",
    "        path = _http_try_download_split(repo_id, split, token)\n",
    "        candidates = [path]\n",
    "    for path in candidates:\n",
    "        try:\n",
    "            recs = _read_json_records(path)\n",
    "            if isinstance(recs, list) and recs:\n",
    "                out = _records_to_easytpp_dict(recs)\n",
    "                if out[\"seq_len\"]:\n",
    "                    return out\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise RuntimeError(\"Failed to parse any JSON/JSONL for split '%s' in %s\" % (split, local_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to plot Hawkes kernels at a readable size based on number of marks\n",
    "plt.rcParams.update({\"font.size\": 12, \"figure.dpi\": 120})\n",
    "\n",
    "\n",
    "def plot_kernels_large(em, per_subplot=4.5, extra_w=8.0, extra_h=4.5, dpi=380):\n",
    "    try:\n",
    "        num_marks = int(len(em.baseline))\n",
    "    except Exception:\n",
    "        num_marks = 1\n",
    "    fig = plot_hawkes_kernels(em, show=False)\n",
    "    # Avoid extreme sizes: clamp marks and figure dims\n",
    "    marks = max(1, min(num_marks, 18))\n",
    "    width = per_subplot * marks + extra_w\n",
    "    height = per_subplot * marks + extra_h\n",
    "    fig.set_size_inches(max(12.0, width), max(9.0, height))\n",
    "    try:\n",
    "        fig.tight_layout()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return fig, dpi\n",
    "\n",
    "\n",
    "# Convenience wrapper to show+save\n",
    "\n",
    "\n",
    "def show_and_save(fig, path, dpi):\n",
    "    fig.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    try:\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "    except Exception:\n",
    "        pass\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Tight layout not applied. tight_layout cannot make axes width small enough to accommodate all axes decorations\n"
     ]
    }
   ],
   "source": [
    "# Load and fit EM on easytpp/taobao (Arrow bypass)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load using manual snapshot + JSON parser to avoid Arrow\n",
    "taobao_train = load_easytpp_as_dict(\"easytpp/taobao\", split=\"train\")\n",
    "\n",
    "# Merge train into a single multi-mark timestamp list\n",
    "taobao_timestamps = merge_easytpp_to_single_path(taobao_train)\n",
    "\n",
    "em = HawkesEM(1.0, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "num_marks_to_consider = len(taobao_timestamps)\n",
    "em.fit(taobao_timestamps[:num_marks_to_consider])\n",
    "\n",
    "fig = plot_hawkes_kernels(em, show=True)\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"taobao_hf.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified loader shim: delegate to load_easytpp_as_dict defined above\n",
    "\n",
    "\n",
    "def load_easytpp(repo_id: str, split: str):\n",
    "    return load_easytpp_as_dict(repo_id, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon via robust loader (manual JSON parsing fallback)\n",
    "import matplotlib.pyplot as plt\n",
    "from tick.hawkes import HawkesEM\n",
    "from tick.plot import plot_hawkes_kernels\n",
    "\n",
    "\n",
    "amazon_train = load_easytpp_as_dict(\"easytpp/amazon\", split=\"train\")\n",
    "\n",
    "amazon_timestamps = merge_easytpp_to_single_path(amazon_train)\n",
    "\n",
    "em = HawkesEM(1.0, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "num_marks_to_consider = len(amazon_timestamps)\n",
    "em.fit(amazon_timestamps[:num_marks_to_consider])\n",
    "\n",
    "fig, dpi = plot_kernels_large(em)\n",
    "show_and_save(fig, \"amazon_hf.jpg\", dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxi via robust loader (manual JSON parsing fallback)\n",
    "import matplotlib.pyplot as plt\n",
    "from tick.hawkes import HawkesEM\n",
    "from tick.plot import plot_hawkes_kernels\n",
    "\n",
    "\n",
    "taxi_train = load_easytpp_as_dict(\"easytpp/taxi\", split=\"train\")\n",
    "\n",
    "taxi_timestamps = merge_easytpp_to_single_path(taxi_train)\n",
    "\n",
    "em = HawkesEM(1.0, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "num_marks_to_consider = len(taxi_timestamps)\n",
    "em.fit(taxi_timestamps[:num_marks_to_consider])\n",
    "\n",
    "fig, dpi = plot_kernels_large(em)\n",
    "show_and_save(fig, \"taxi_hf.jpg\", dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taobao via robust loader (manual JSON parsing fallback)\n",
    "import matplotlib.pyplot as plt\n",
    "from tick.hawkes import HawkesEM\n",
    "from tick.plot import plot_hawkes_kernels\n",
    "\n",
    "\n",
    "taobao_train = load_easytpp_as_dict(\"easytpp/taobao\", split=\"train\")\n",
    "\n",
    "taobao_timestamps = merge_easytpp_to_single_path(taobao_train)\n",
    "\n",
    "em = HawkesEM(1.0, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "num_marks_to_consider = len(taobao_timestamps)\n",
    "em.fit(taobao_timestamps[:num_marks_to_consider])\n",
    "\n",
    "fig, dpi = plot_kernels_large(em)\n",
    "show_and_save(fig, \"taobao_hf.jpg\", dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual JSON loader fallback that bypasses Arrow\n",
    "# import os, json, io\n",
    "# from typing import List, Dict, Any\n",
    "\n",
    "# try:\n",
    "#     from huggingface_hub import snapshot_download\n",
    "# except Exception:\n",
    "#     snapshot_download = None\n",
    "\n",
    "\n",
    "# def _find_split_files(base_dir: str, split: str) -> List[str]:\n",
    "#     split_low = split.lower()\n",
    "#     hits = []\n",
    "#     for root, _, files in os.walk(base_dir):\n",
    "#         for name in files:\n",
    "#             low = name.lower()\n",
    "#             if split_low in low and (low.endswith(\".json\") or low.endswith(\".jsonl\")):\n",
    "#                 hits.append(os.path.join(root, name))\n",
    "#     return sorted(hits)\n",
    "\n",
    "\n",
    "# def _read_json_records(path: str) -> List[Dict[str, Any]]:\n",
    "#     with io.open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         head = f.read(2048)\n",
    "#         f.seek(0)\n",
    "#         if head.lstrip().startswith(\"[\"):\n",
    "#             data = json.load(f)\n",
    "#             if isinstance(data, dict):\n",
    "#                 # If top-level dict, pick first list value\n",
    "#                 for v in data.values():\n",
    "#                     if isinstance(v, list):\n",
    "#                         data = v\n",
    "#                         break\n",
    "#             if isinstance(data, list):\n",
    "#                 return data\n",
    "#             return []\n",
    "#         else:\n",
    "#             # JSONL\n",
    "#             recs = []\n",
    "#             for line in f:\n",
    "#                 line = line.strip()\n",
    "#                 if not line:\n",
    "#                     continue\n",
    "#                 try:\n",
    "#                     recs.append(json.loads(line))\n",
    "#                 except Exception:\n",
    "#                     pass\n",
    "#             return recs\n",
    "\n",
    "\n",
    "# def _records_to_easytpp_dict(records: List[Dict[str, Any]]) -> Dict[str, List[List[Any]]]:\n",
    "#     res = {\"time_since_start\": [], \"time_since_last_event\": [], \"type_event\": [], \"seq_len\": []}\n",
    "#     for r in records:\n",
    "#         times = None\n",
    "#         types = None\n",
    "#         # Common EasyTPP-style keys\n",
    "#         if isinstance(r, dict):\n",
    "#             if \"time_since_start\" in r and \"type_event\" in r:\n",
    "#                 times = r.get(\"time_since_start\")\n",
    "#                 types = r.get(\"type_event\")\n",
    "#             elif \"timestamps\" in r and \"types\" in r:\n",
    "#                 times = r.get(\"timestamps\")\n",
    "#                 types = r.get(\"types\")\n",
    "#             elif \"time\" in r and \"type\" in r:\n",
    "#                 times = r.get(\"time\")\n",
    "#                 types = r.get(\"type\")\n",
    "#         if not isinstance(times, (list, tuple)) or not isinstance(types, (list, tuple)):\n",
    "#             continue\n",
    "#         if len(times) != len(types) or len(times) == 0:\n",
    "#             continue\n",
    "#         times = [float(t) for t in times]\n",
    "#         types = [int(c) for c in types]\n",
    "#         deltas = [0.0] + [times[i] - times[i - 1] for i in range(1, len(times))]\n",
    "#         res[\"time_since_start\"].append(times)\n",
    "#         res[\"time_since_last_event\"].append(deltas)\n",
    "#         res[\"type_event\"].append(types)\n",
    "#         res[\"seq_len\"].append(len(times))\n",
    "#     return res\n",
    "\n",
    "\n",
    "# def load_easytpp_as_dict(repo_id: str, split: str) -> Dict[str, List[List[Any]]]:\n",
    "#     \"\"\"\n",
    "#     Return dict-of-lists (time_since_start, time_since_last_event, type_event, seq_len).\n",
    "#     Tries datasets API first; falls back to snapshot + manual JSON parsing to avoid Arrow.\n",
    "#     \"\"\"\n",
    "#     token = os.getenv(\"HF_TOKEN\")\n",
    "#     # Try datasets API (if available and supports remote code)\n",
    "#     try:\n",
    "#         from datasets import load_dataset\n",
    "#         try:\n",
    "#             ds = load_dataset(repo_id, split=split, trust_remote_code=True, use_auth_token=token)\n",
    "#         except TypeError:\n",
    "#             ds = load_dataset(repo_id, split=split, use_auth_token=token)\n",
    "#         # Convert to plain dict-of-lists\n",
    "#         return ds[: len(ds)]\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "#     # Fallback to snapshot + manual parsing\n",
    "#     if snapshot_download is None:\n",
    "#         raise RuntimeError(\"huggingface_hub.snapshot_download not available in this environment\")\n",
    "#     local_dir = snapshot_download(repo_id, repo_type=\"dataset\", use_auth_token=token)\n",
    "#     candidates = _find_split_files(local_dir, split)\n",
    "#     if not candidates:\n",
    "#         raise RuntimeError(\"No JSON/JSONL files found for split '%s' in %s\" % (split, local_dir))\n",
    "#     # Try candidates until parse succeeds\n",
    "#     for path in candidates:\n",
    "#         try:\n",
    "#             recs = _read_json_records(path)\n",
    "#             if isinstance(recs, list) and recs:\n",
    "#                 out = _records_to_easytpp_dict(recs)\n",
    "#                 if out[\"seq_len\"]:\n",
    "#                     return out\n",
    "#         except Exception:\n",
    "#             continue\n",
    "#     raise RuntimeError(\"Failed to parse any JSON/JSONL for split '%s' in %s\" % (split, local_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36tick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
