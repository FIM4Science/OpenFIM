{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook tries to fit tick's EM model to the real world datasets to get a feeling what their kernels look like. Because the EM algorithm takes a single (long) path, we concatenate the paths of the real world datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tick.hawkes import HawkesEM\n",
    "from tick.plot import plot_hawkes_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/evaluation/hawkes/mimic_II.pkl\", \"rb\") as f:\n",
    "    mimic = pickle.load(f)\n",
    "\n",
    "with open(\"../../data/evaluation/hawkes/mooc.pkl\", \"rb\") as f:\n",
    "    mooc = pickle.load(f)\n",
    "\n",
    "with open(\"../../data/evaluation/hawkes/stackOverflow.pkl\", \"rb\") as f:\n",
    "    stack = pickle.load(f)\n",
    "\n",
    "with open(\"../../data/evaluation/hawkes/retweet.pkl\", \"rb\") as f:\n",
    "    retweet = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['timestamps', 'types', 'lengths', 'timeintervals'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweet.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweet[\"timestamps\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweet[\"timestamps\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_marks(types_data):\n",
    "    return np.unique(np.concatenate(types_data)).max() + 1\n",
    "\n",
    "\n",
    "def merge_to_single_path(dataset):\n",
    "    \"\"\"\n",
    "    Merge all paths to a single path.\n",
    "    We return the timestamps for every mark separately.\n",
    "    \"\"\"\n",
    "    num_marks = get_number_of_marks(dataset[\"types\"])\n",
    "    res = [[] for _ in range(num_marks)]\n",
    "    for path_idx in range(len(dataset[\"types\"])):\n",
    "        marks = np.unique(dataset[\"types\"][path_idx])\n",
    "        for mark in marks:\n",
    "            prev_time = 0\n",
    "            if len(res[mark]) > 0 and res[mark][-1] != 0:\n",
    "                prev_time = res[mark][-1]\n",
    "            time_stamps = dataset[\"timestamps\"][path_idx][dataset[\"types\"][path_idx] == mark]\n",
    "            # Add the previous time to the timestamps\n",
    "            time_stamps = [time + prev_time for time in time_stamps]\n",
    "            res[mark] += time_stamps\n",
    "\n",
    "    for i in range(len(res)):\n",
    "        res[i] = np.array(res[i])\n",
    "    return res\n",
    "\n",
    "\n",
    "def normalize_timestamps(timestamps):\n",
    "    \"\"\"\n",
    "    Normalize the timestamps to have max delta time of 1\n",
    "    \"\"\"\n",
    "    flattened_times = np.concatenate(timestamps)\n",
    "    max_time = np.diff(flattened_times).max()\n",
    "    return [time / max_time for time in timestamps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em = HawkesEM(1, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "# timestamps = merge_to_single_path(mimic)\n",
    "# # timestamps = normalize_timestamps(timestamps)\n",
    "\n",
    "# num_marks_to_consider = 6\n",
    "# em.fit(timestamps[:num_marks_to_consider])\n",
    "\n",
    "# fig = plot_hawkes_kernels(em, show=True)\n",
    "# fig.set_size_inches(15, 10)  # Adjust the size as needed\n",
    "# plt.tight_layout()  # Adjust the layout to prevent overlap\n",
    "\n",
    "# plt.show()\n",
    "# plt.savefig(\"mimic.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em = HawkesEM(0.04, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "# timestamps = merge_to_single_path(mooc)\n",
    "# # timestamps = normalize_timestamps(timestamps)\n",
    "\n",
    "# num_marks_to_consider = 6\n",
    "# em.fit(timestamps[:num_marks_to_consider])\n",
    "\n",
    "# fig = plot_hawkes_kernels(em, show=True)\n",
    "# fig.set_size_inches(15, 10)  # Adjust the size as needed\n",
    "# plt.tight_layout()  # Adjust the layout to prevent overlap\n",
    "\n",
    "# plt.show()\n",
    "# plt.savefig(\"mooc.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em = HawkesEM(30, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "# timestamps = merge_to_single_path(stack)\n",
    "# # timestamps = normalize_timestamps(timestamps)\n",
    "\n",
    "# num_marks_to_consider = 6\n",
    "# em.fit(timestamps[:num_marks_to_consider])\n",
    "\n",
    "# fig = plot_hawkes_kernels(em, show=True)\n",
    "# fig.set_size_inches(15, 10)  # Adjust the size as needed\n",
    "# plt.tight_layout()  # Adjust the layout to prevent overlap\n",
    "\n",
    "# plt.show()\n",
    "# plt.savefig(\"stack.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em = HawkesEM(150, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "# timestamps = merge_to_single_path(retweet)\n",
    "# # timestamps = normalize_timestamps(timestamps)\n",
    "\n",
    "# num_marks_to_consider = 3\n",
    "# em.fit(timestamps[:num_marks_to_consider])\n",
    "\n",
    "# fig = plot_hawkes_kernels(em, show=True)\n",
    "# fig.set_size_inches(15, 10)  # Adjust the size as needed\n",
    "# plt.tight_layout()  # Adjust the layout to prevent overlap\n",
    "\n",
    "# plt.show()\n",
    "# plt.savefig(\"retweet.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for EasyTPP-format datasets (Hugging Face)\n",
    "\n",
    "\n",
    "def get_num_marks_easytpp(ds_dict):\n",
    "    if len(ds_dict.get(\"type_event\", [])) == 0:\n",
    "        return 0\n",
    "    max_type = -1\n",
    "    for seq_types in ds_dict[\"type_event\"]:\n",
    "        if len(seq_types):\n",
    "            max_type = max(max_type, int(np.max(seq_types)))\n",
    "    return max_type + 1 if max_type >= 0 else 0\n",
    "\n",
    "\n",
    "def merge_easytpp_to_single_path(ds_dict, max_sequences=None):\n",
    "    \"\"\"\n",
    "    Merge EasyTPP dict-of-lists into a single path of per-mark timestamps\n",
    "    suitable for tick.HawkesEM.\n",
    "\n",
    "    Returns: list[np.ndarray] of length M (num marks)\n",
    "    \"\"\"\n",
    "    num_seqs = len(ds_dict.get(\"seq_len\", []))\n",
    "    if num_seqs == 0:\n",
    "        return []\n",
    "    M = get_num_marks_easytpp(ds_dict)\n",
    "    res = [[] for _ in range(M)]\n",
    "\n",
    "    limit = num_seqs if max_sequences is None else min(num_seqs, int(max_sequences))\n",
    "    for i in range(limit):\n",
    "        times = np.array(ds_dict[\"time_since_start\"][i], dtype=float)\n",
    "        types = np.array(ds_dict[\"type_event\"][i], dtype=int)\n",
    "        if times.size == 0:\n",
    "            continue\n",
    "        marks_in_seq = np.unique(types).astype(int)\n",
    "        for m in marks_in_seq:\n",
    "            idx = types == m\n",
    "            ts_m = times[idx]\n",
    "            if ts_m.size == 0:\n",
    "                continue\n",
    "            prev_time = res[m][-1] if len(res[m]) > 0 else 0.0\n",
    "            shifted = (ts_m + prev_time).tolist()\n",
    "            res[m].extend(shifted)\n",
    "\n",
    "    for j in range(M):\n",
    "        res[j] = np.array(res[j], dtype=float)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define manual EasyTPP loader above usage to avoid NameError\n",
    "import inspect\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import snapshot_download\n",
    "except Exception:\n",
    "    snapshot_download = None\n",
    "\n",
    "\n",
    "# ---------- file discovery / parsing ----------\n",
    "\n",
    "\n",
    "def _find_split_files(base_dir: str, split: str) -> List[str]:\n",
    "    split_low = split.lower()\n",
    "    hits = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for name in files:\n",
    "            low = name.lower()\n",
    "            if split_low in low and (\n",
    "                low.endswith(\".json\") or low.endswith(\".jsonl\") or low.endswith(\".json.gz\") or low.endswith(\".jsonl.gz\")\n",
    "            ):\n",
    "                hits.append(os.path.join(root, name))\n",
    "    return sorted(hits)\n",
    "\n",
    "\n",
    "def _open_textmaybe_gz(path: str):\n",
    "    if path.endswith(\".gz\"):\n",
    "        import gzip\n",
    "\n",
    "        return gzip.open(path, \"rt\", encoding=\"utf-8\")\n",
    "    return io.open(path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def _read_json_records(path: str) -> List[Dict[str, Any]]:\n",
    "    with _open_textmaybe_gz(path) as f:\n",
    "        head = f.read(2048)\n",
    "        f.seek(0)\n",
    "        if head.lstrip().startswith(\"[\"):\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, dict):\n",
    "                for v in data.values():\n",
    "                    if isinstance(v, list):\n",
    "                        data = v\n",
    "                        break\n",
    "            return data if isinstance(data, list) else []\n",
    "        else:\n",
    "            recs = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    recs.append(json.loads(line))\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return recs\n",
    "\n",
    "\n",
    "def _records_to_easytpp_dict(records: List[Dict[str, Any]]) -> Dict[str, List[List[Any]]]:\n",
    "    res = {\"time_since_start\": [], \"time_since_last_event\": [], \"type_event\": [], \"seq_len\": []}\n",
    "    for r in records:\n",
    "        times = None\n",
    "        types = None\n",
    "        if isinstance(r, dict):\n",
    "            if \"time_since_start\" in r and \"type_event\" in r:\n",
    "                times = r.get(\"time_since_start\")\n",
    "                types = r.get(\"type_event\")\n",
    "            elif \"timestamps\" in r and \"types\" in r:\n",
    "                times = r.get(\"timestamps\")\n",
    "                types = r.get(\"types\")\n",
    "            elif \"time\" in r and \"type\" in r:\n",
    "                times = r.get(\"time\")\n",
    "                types = r.get(\"type\")\n",
    "        if not isinstance(times, (list, tuple)) or not isinstance(types, (list, tuple)):\n",
    "            continue\n",
    "        if len(times) != len(types) or len(times) == 0:\n",
    "            continue\n",
    "        times = [float(t) for t in times]\n",
    "        types = [int(c) for c in types]\n",
    "        deltas = [0.0] + [times[i] - times[i - 1] for i in range(1, len(times))]\n",
    "        res[\"time_since_start\"].append(times)\n",
    "        res[\"time_since_last_event\"].append(deltas)\n",
    "        res[\"type_event\"].append(types)\n",
    "        res[\"seq_len\"].append(len(times))\n",
    "    return res\n",
    "\n",
    "\n",
    "# ---------- HTTP fallback (no git, no Arrow) ----------\n",
    "\n",
    "\n",
    "def _http_try_download_split(repo_id: str, split: str, token: str) -> str:\n",
    "    import requests\n",
    "\n",
    "    repo = repo_id.split(\"datasets/\")[-1]\n",
    "    # common filenames\n",
    "    cands = [\n",
    "        f\"{split}.jsonl\",\n",
    "        f\"{split}.json\",\n",
    "        f\"{split}.jsonl.gz\",\n",
    "        f\"{split}.json.gz\",\n",
    "        f\"data/{split}.jsonl\",\n",
    "        f\"data/{split}.json\",\n",
    "        f\"data/{split}.jsonl.gz\",\n",
    "        f\"data/{split}.json.gz\",\n",
    "    ]\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"} if token else {}\n",
    "    cache_dir = os.path.join(os.path.expanduser(\"~/.cache\"), \"hf_ds_http\", repo.replace(\"/\", \"_\"))\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    base_urls = [\n",
    "        f\"https://huggingface.co/datasets/{repo}/resolve/main/\",\n",
    "        f\"https://huggingface.co/{repo}/resolve/main/\",\n",
    "    ]\n",
    "    for base in base_urls:\n",
    "        for cand in cands:\n",
    "            url = base + cand\n",
    "            r = requests.get(url, headers=headers, stream=True)\n",
    "            if r.status_code == 200:\n",
    "                local_path = os.path.join(cache_dir, cand.replace(\"/\", \"_\"))\n",
    "                with open(local_path, \"wb\") as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                return local_path\n",
    "    raise RuntimeError(\"Could not fetch split file via HTTP for %s (%s)\" % (repo_id, split))\n",
    "\n",
    "\n",
    "# ---------- snapshot or HTTP ----------\n",
    "\n",
    "\n",
    "def _snapshot_dataset(repo_id: str, token: str) -> str:\n",
    "    # Try huggingface_hub if present\n",
    "    if snapshot_download is not None:\n",
    "        try:\n",
    "            sig = inspect.signature(snapshot_download)\n",
    "            if \"repo_type\" in sig.parameters:\n",
    "                return snapshot_download(repo_id, repo_type=\"dataset\", use_auth_token=token)\n",
    "        except Exception:\n",
    "            pass\n",
    "        for rid in ((\"datasets/\" + repo_id) if not repo_id.startswith(\"datasets/\") else repo_id, repo_id):\n",
    "            try:\n",
    "                return snapshot_download(rid, use_auth_token=token)\n",
    "            except Exception:\n",
    "                continue\n",
    "    # Fall back to HTTP single-file download into a temp dir\n",
    "    local_file = _http_try_download_split(repo_id, split=\"train\", token=token)\n",
    "    base_dir = os.path.dirname(local_file)\n",
    "    return base_dir\n",
    "\n",
    "\n",
    "def load_easytpp_as_dict(repo_id: str, split: str) -> Dict[str, List[List[Any]]]:\n",
    "    token = os.getenv(\"HF_TOKEN\")\n",
    "    local_dir = _snapshot_dataset(repo_id, token)\n",
    "    candidates = _find_split_files(local_dir, split)\n",
    "    if not candidates:\n",
    "        # try HTTP direct for this split\n",
    "        path = _http_try_download_split(repo_id, split, token)\n",
    "        candidates = [path]\n",
    "    for path in candidates:\n",
    "        try:\n",
    "            recs = _read_json_records(path)\n",
    "            if isinstance(recs, list) and recs:\n",
    "                out = _records_to_easytpp_dict(recs)\n",
    "                if out[\"seq_len\"]:\n",
    "                    return out\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise RuntimeError(\"Failed to parse any JSON/JSONL for split '%s' in %s\" % (split, local_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to plot Hawkes kernels at a readable size based on number of marks\n",
    "plt.rcParams.update({\"font.size\": 12, \"figure.dpi\": 120})\n",
    "\n",
    "\n",
    "def plot_kernels_large(em, per_subplot=4.5, extra_w=8.0, extra_h=4.5, dpi=380):\n",
    "    try:\n",
    "        num_marks = int(len(em.baseline))\n",
    "    except Exception:\n",
    "        num_marks = 1\n",
    "    fig = plot_hawkes_kernels(em, show=False)\n",
    "    # Avoid extreme sizes: clamp marks and figure dims\n",
    "    marks = max(1, min(num_marks, 18))\n",
    "    width = per_subplot * marks + extra_w\n",
    "    height = per_subplot * marks + extra_h\n",
    "    fig.set_size_inches(max(12.0, width), max(9.0, height))\n",
    "    try:\n",
    "        fig.tight_layout()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return fig, dpi\n",
    "\n",
    "\n",
    "# Convenience wrapper to show+save\n",
    "\n",
    "\n",
    "def show_and_save(fig, path, dpi):\n",
    "    fig.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    try:\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "    except Exception:\n",
    "        pass\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-43f85b6887d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load using manual snapshot + JSON parser to avoid Arrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtaobao_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_easytpp_as_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"easytpp/taobao\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Merge train into a single multi-mark timestamp list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-3438649f7d0f>\u001b[0m in \u001b[0;36mload_easytpp_as_dict\u001b[0;34m(repo_id, split)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_easytpp_as_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mlocal_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snapshot_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_split_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-3438649f7d0f>\u001b[0m in \u001b[0;36m_snapshot_dataset\u001b[0;34m(repo_id, token)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Fall back to HTTP single-file download into a temp dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mlocal_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_http_try_download_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-3438649f7d0f>\u001b[0m in \u001b[0;36m_http_try_download_split\u001b[0;34m(repo_id, split, token)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcands\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mlocal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    527\u001b[0m         }\n\u001b[1;32m    528\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m                 )\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m             )\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    466\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load and fit EM on easytpp/taobao (Arrow bypass)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load using manual snapshot + JSON parser to avoid Arrow\n",
    "taobao_train = load_easytpp_as_dict(\"easytpp/taobao\", split=\"train\")\n",
    "\n",
    "# Merge train into a single multi-mark timestamp list\n",
    "taobao_timestamps = merge_easytpp_to_single_path(taobao_train)\n",
    "\n",
    "em = HawkesEM(1.0, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "num_marks_to_consider = len(taobao_timestamps)\n",
    "em.fit(taobao_timestamps[:num_marks_to_consider])\n",
    "\n",
    "fig = plot_hawkes_kernels(em, show=True)\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"taobao_hf.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified loader shim: delegate to load_easytpp_as_dict defined above\n",
    "\n",
    "\n",
    "def load_easytpp(repo_id: str, split: str):\n",
    "    return load_easytpp_as_dict(repo_id, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e086e92e64d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHawkesEM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnum_marks_to_consider\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazon_timestamps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazon_timestamps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_marks_to_consider\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_kernels_large\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/tick/hawkes/inference/hawkes_em.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, events, end_times, baseline_start, kernel_start)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \"\"\"\n\u001b[1;32m    142\u001b[0m         \u001b[0mLearnerHawkesNoParam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseline_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/tick/solver/base/solver.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/tick/hawkes/inference/hawkes_em.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(self, baseline_start, kernel_start)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mprev_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_kernels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_record_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/berghaus/anaconda3/envs/py36tick/lib/python3.6/site-packages/tick/hawkes/inference/build/hawkes_inference.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, mu, kernels)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ArrayDouble &'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ArrayDouble2d &'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"void\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;34mr\"\"\"solve(HawkesEM self, ArrayDouble & mu, ArrayDouble2d & kernels)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_hawkes_inference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHawkesEM_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_kernel_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ArrayDouble2d &'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"SArrayDouble2dPtr\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Amazon via robust loader (manual JSON parsing fallback)\n",
    "import matplotlib.pyplot as plt\n",
    "from tick.hawkes import HawkesEM\n",
    "from tick.plot import plot_hawkes_kernels\n",
    "\n",
    "\n",
    "amazon_train = load_easytpp_as_dict(\"easytpp/amazon\", split=\"train\")\n",
    "\n",
    "amazon_timestamps = merge_easytpp_to_single_path(amazon_train)\n",
    "\n",
    "em = HawkesEM(1.0, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "num_marks_to_consider = len(amazon_timestamps)\n",
    "em.fit(amazon_timestamps[:num_marks_to_consider])\n",
    "\n",
    "fig, dpi = plot_kernels_large(em)\n",
    "show_and_save(fig, \"amazon_hf.jpg\", dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxi via robust loader (manual JSON parsing fallback)\n",
    "import matplotlib.pyplot as plt\n",
    "from tick.hawkes import HawkesEM\n",
    "from tick.plot import plot_hawkes_kernels\n",
    "\n",
    "\n",
    "taxi_train = load_easytpp_as_dict(\"easytpp/taxi\", split=\"train\")\n",
    "\n",
    "taxi_timestamps = merge_easytpp_to_single_path(taxi_train)\n",
    "\n",
    "em = HawkesEM(1.0, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "num_marks_to_consider = len(taxi_timestamps)\n",
    "em.fit(taxi_timestamps[:num_marks_to_consider])\n",
    "\n",
    "fig, dpi = plot_kernels_large(em)\n",
    "show_and_save(fig, \"taxi_hf.jpg\", dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taobao via robust loader (manual JSON parsing fallback)\n",
    "import matplotlib.pyplot as plt\n",
    "from tick.hawkes import HawkesEM\n",
    "from tick.plot import plot_hawkes_kernels\n",
    "\n",
    "\n",
    "taobao_train = load_easytpp_as_dict(\"easytpp/taobao\", split=\"train\")\n",
    "\n",
    "taobao_timestamps = merge_easytpp_to_single_path(taobao_train)\n",
    "\n",
    "em = HawkesEM(1.0, kernel_size=100, n_threads=8, verbose=False, tol=1e-3)\n",
    "num_marks_to_consider = len(taobao_timestamps)\n",
    "em.fit(taobao_timestamps[:num_marks_to_consider])\n",
    "\n",
    "fig, dpi = plot_kernels_large(em)\n",
    "show_and_save(fig, \"taobao_hf.jpg\", dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual JSON loader fallback that bypasses Arrow\n",
    "# import os, json, io\n",
    "# from typing import List, Dict, Any\n",
    "\n",
    "# try:\n",
    "#     from huggingface_hub import snapshot_download\n",
    "# except Exception:\n",
    "#     snapshot_download = None\n",
    "\n",
    "\n",
    "# def _find_split_files(base_dir: str, split: str) -> List[str]:\n",
    "#     split_low = split.lower()\n",
    "#     hits = []\n",
    "#     for root, _, files in os.walk(base_dir):\n",
    "#         for name in files:\n",
    "#             low = name.lower()\n",
    "#             if split_low in low and (low.endswith(\".json\") or low.endswith(\".jsonl\")):\n",
    "#                 hits.append(os.path.join(root, name))\n",
    "#     return sorted(hits)\n",
    "\n",
    "\n",
    "# def _read_json_records(path: str) -> List[Dict[str, Any]]:\n",
    "#     with io.open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         head = f.read(2048)\n",
    "#         f.seek(0)\n",
    "#         if head.lstrip().startswith(\"[\"):\n",
    "#             data = json.load(f)\n",
    "#             if isinstance(data, dict):\n",
    "#                 # If top-level dict, pick first list value\n",
    "#                 for v in data.values():\n",
    "#                     if isinstance(v, list):\n",
    "#                         data = v\n",
    "#                         break\n",
    "#             if isinstance(data, list):\n",
    "#                 return data\n",
    "#             return []\n",
    "#         else:\n",
    "#             # JSONL\n",
    "#             recs = []\n",
    "#             for line in f:\n",
    "#                 line = line.strip()\n",
    "#                 if not line:\n",
    "#                     continue\n",
    "#                 try:\n",
    "#                     recs.append(json.loads(line))\n",
    "#                 except Exception:\n",
    "#                     pass\n",
    "#             return recs\n",
    "\n",
    "\n",
    "# def _records_to_easytpp_dict(records: List[Dict[str, Any]]) -> Dict[str, List[List[Any]]]:\n",
    "#     res = {\"time_since_start\": [], \"time_since_last_event\": [], \"type_event\": [], \"seq_len\": []}\n",
    "#     for r in records:\n",
    "#         times = None\n",
    "#         types = None\n",
    "#         # Common EasyTPP-style keys\n",
    "#         if isinstance(r, dict):\n",
    "#             if \"time_since_start\" in r and \"type_event\" in r:\n",
    "#                 times = r.get(\"time_since_start\")\n",
    "#                 types = r.get(\"type_event\")\n",
    "#             elif \"timestamps\" in r and \"types\" in r:\n",
    "#                 times = r.get(\"timestamps\")\n",
    "#                 types = r.get(\"types\")\n",
    "#             elif \"time\" in r and \"type\" in r:\n",
    "#                 times = r.get(\"time\")\n",
    "#                 types = r.get(\"type\")\n",
    "#         if not isinstance(times, (list, tuple)) or not isinstance(types, (list, tuple)):\n",
    "#             continue\n",
    "#         if len(times) != len(types) or len(times) == 0:\n",
    "#             continue\n",
    "#         times = [float(t) for t in times]\n",
    "#         types = [int(c) for c in types]\n",
    "#         deltas = [0.0] + [times[i] - times[i - 1] for i in range(1, len(times))]\n",
    "#         res[\"time_since_start\"].append(times)\n",
    "#         res[\"time_since_last_event\"].append(deltas)\n",
    "#         res[\"type_event\"].append(types)\n",
    "#         res[\"seq_len\"].append(len(times))\n",
    "#     return res\n",
    "\n",
    "\n",
    "# def load_easytpp_as_dict(repo_id: str, split: str) -> Dict[str, List[List[Any]]]:\n",
    "#     \"\"\"\n",
    "#     Return dict-of-lists (time_since_start, time_since_last_event, type_event, seq_len).\n",
    "#     Tries datasets API first; falls back to snapshot + manual JSON parsing to avoid Arrow.\n",
    "#     \"\"\"\n",
    "#     token = os.getenv(\"HF_TOKEN\")\n",
    "#     # Try datasets API (if available and supports remote code)\n",
    "#     try:\n",
    "#         from datasets import load_dataset\n",
    "#         try:\n",
    "#             ds = load_dataset(repo_id, split=split, trust_remote_code=True, use_auth_token=token)\n",
    "#         except TypeError:\n",
    "#             ds = load_dataset(repo_id, split=split, use_auth_token=token)\n",
    "#         # Convert to plain dict-of-lists\n",
    "#         return ds[: len(ds)]\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "#     # Fallback to snapshot + manual parsing\n",
    "#     if snapshot_download is None:\n",
    "#         raise RuntimeError(\"huggingface_hub.snapshot_download not available in this environment\")\n",
    "#     local_dir = snapshot_download(repo_id, repo_type=\"dataset\", use_auth_token=token)\n",
    "#     candidates = _find_split_files(local_dir, split)\n",
    "#     if not candidates:\n",
    "#         raise RuntimeError(\"No JSON/JSONL files found for split '%s' in %s\" % (split, local_dir))\n",
    "#     # Try candidates until parse succeeds\n",
    "#     for path in candidates:\n",
    "#         try:\n",
    "#             recs = _read_json_records(path)\n",
    "#             if isinstance(recs, list) and recs:\n",
    "#                 out = _records_to_easytpp_dict(recs)\n",
    "#                 if out[\"seq_len\"]:\n",
    "#                     return out\n",
    "#         except Exception:\n",
    "#             continue\n",
    "#     raise RuntimeError(\"Failed to parse any JSON/JSONL for split '%s' in %s\" % (split, local_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36tick",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
