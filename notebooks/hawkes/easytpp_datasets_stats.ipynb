{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "volcano_data = load_dataset(\"easytpp/volcano\")\n",
    "taobao_data = load_dataset(\"easytpp/taobao\")\n",
    "retweet_data = load_dataset(\"easytpp/retweet\")\n",
    "stackoverflow_data = load_dataset(\"easytpp/stackoverflow\")\n",
    "taxi_data = load_dataset(\"easytpp/taxi\")\n",
    "amazon_data = load_dataset(\"easytpp/amazon\")\n",
    "earthquake_data = load_dataset(\"easytpp/earthquake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.708316327447124"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(max(taobao_data[\"test\"][\"time_since_start\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_times_to_1(dataset):\n",
    "    normalized_dataset = {}\n",
    "\n",
    "    # Process each split separately\n",
    "    for split in dataset.keys():\n",
    "        # Find max time for this specific split\n",
    "        all_times = []\n",
    "        for example in dataset[split]:\n",
    "            all_times.extend(example[\"time_since_start\"])\n",
    "\n",
    "        max_time = max(all_times)\n",
    "\n",
    "        # Define normalization function for this split\n",
    "        def normalize_example(example):\n",
    "            return {\n",
    "                \"seq_len\": example[\"seq_len\"],\n",
    "                \"type_event\": example[\"type_event\"],\n",
    "                \"seq_idx\": example[\"seq_idx\"],\n",
    "                \"time_since_start\": [time / max_time for time in example[\"time_since_start\"]],\n",
    "                \"time_since_last_event\": [time / max_time for time in example[\"time_since_last_event\"]],\n",
    "                \"dim_process\": example[\"dim_process\"],\n",
    "            }\n",
    "\n",
    "        # Apply normalization while preserving Dataset type\n",
    "        normalized_dataset[split] = dataset[split].map(normalize_example)\n",
    "\n",
    "    return normalized_dataset\n",
    "\n",
    "\n",
    "volcano_data = normalize_times_to_1(volcano_data)\n",
    "taobao_data = normalize_times_to_1(taobao_data)\n",
    "retweet_data = normalize_times_to_1(retweet_data)\n",
    "stackoverflow_data = normalize_times_to_1(stackoverflow_data)\n",
    "taxi_data = normalize_times_to_1(taxi_data)\n",
    "amazon_data = normalize_times_to_1(amazon_data)\n",
    "earthquake_data = normalize_times_to_1(earthquake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(dataset, split=\"test\"):\n",
    "    dataset = dataset[split]\n",
    "    stats = {\n",
    "        \"num_sequences\": len(dataset),\n",
    "        \"max_sequence_length\": 0,\n",
    "        \"min_sequence_length\": 1000000,\n",
    "        \"max_event_time\": 0,\n",
    "        \"min_event_time\": 1000000,\n",
    "    }\n",
    "    seq_lengths = [len(seq[\"time_since_last_event\"]) for seq in dataset]\n",
    "    times_since_last_event = [time for seq in dataset for time in seq[\"time_since_last_event\"]]\n",
    "    min_delta_event_time = min(times_since_last_event)\n",
    "    max_delta_event_time = max(times_since_last_event)\n",
    "    min_seq_length = min(seq_lengths)\n",
    "    max_seq_length = max(seq_lengths)\n",
    "    avg_seq_length = sum(seq_lengths) / len(seq_lengths)\n",
    "    avg_delta_event_time = sum(times_since_last_event) / len(times_since_last_event)\n",
    "    stats[\"max_sequence_length\"] = max_seq_length\n",
    "    stats[\"min_sequence_length\"] = min_seq_length\n",
    "    stats[\"num_sequences\"] = len(seq_lengths)\n",
    "    stats[\"avg_sequence_length\"] = avg_seq_length\n",
    "    stats[\"max_event_time\"] = max_delta_event_time\n",
    "    stats[\"min_event_time\"] = min_delta_event_time\n",
    "    stats[\"avg_event_time\"] = avg_delta_event_time\n",
    "    encoutered_marks = set()\n",
    "    for seq in dataset:\n",
    "        for mark in seq[\"type_event\"]:\n",
    "            if mark not in encoutered_marks:\n",
    "                encoutered_marks.add(mark)\n",
    "    stats[\"num_marks\"] = len(encoutered_marks)\n",
    "    pprint(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Stats Prints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TaoBao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(taobao_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(retweet_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(stackoverflow_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(taxi_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(amazon_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(dataset, splits=[\"train\"]):\n",
    "    # Extract the data for each split\n",
    "    split_data = {split: dataset[split] for split in splits}\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(len(splits) + 1, 2, figsize=(20, 7 * len(splits)))\n",
    "\n",
    "    for i, split in enumerate(splits):\n",
    "        data = split_data[split]\n",
    "        seq_len = data[\"seq_len\"]\n",
    "        time_since_last_event = data[\"time_since_last_event\"]\n",
    "\n",
    "        # Histogram of the seq_len\n",
    "        axes[i, 0].hist(seq_len, bins=20, edgecolor=\"k\")\n",
    "        axes[i, 0].set_title(f\"Histogram of Sequence Length ({split})\")\n",
    "        axes[i, 0].set_xlabel(\"Sequence Length\")\n",
    "        axes[i, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "        # Histogram of the time_since_last_event\n",
    "        axes[i, 1].hist(time_since_last_event, bins=20, edgecolor=\"k\")\n",
    "        axes[i, 1].set_title(f\"Histogram of Time Since Last Event ({split})\")\n",
    "        axes[i, 1].set_xlabel(\"Time Since Last Event\")\n",
    "        axes[i, 1].set_ylabel(\"Frequency\")\n",
    "        # axes[i, 0].set_xscale('log')\n",
    "\n",
    "        # axes[i, 0].set_yscale('log')\n",
    "        # axes[i, 1].set_xscale('log')\n",
    "        axes[i, 1].set_yscale(\"log\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats_per_event_type(dataset, splits=[\"train\"]):\n",
    "    # Extract the data for each split\n",
    "    split_data = {split: dataset[split] for split in splits}\n",
    "\n",
    "    # Create subplots\n",
    "    num_events = dataset[\"train\"][0][\"dim_process\"]\n",
    "    fig, axes = plt.subplots(len(splits) * num_events, 2, figsize=(20, 5 * len(splits) * num_events))\n",
    "\n",
    "    for i, split in enumerate(splits):\n",
    "        data = split_data[split]\n",
    "\n",
    "        for event_type in range(num_events):\n",
    "            print(f\"Event Type {event_type} for split {split}\")\n",
    "            event_mask = [[d == event_type for d in x[\"type_event\"]] for x in data]\n",
    "            time_since_last_event = [[e for e, m in zip(d[\"time_since_last_event\"], e_m) if m] for d, e_m in zip(data, event_mask)]\n",
    "            seq_len = [len(x) for x in time_since_last_event]\n",
    "            # Histogram of the seq_len\n",
    "            axes[i * num_events + event_type, 0].hist(seq_len, bins=20, edgecolor=\"k\", alpha=0.5, label=f\"Event Type {event_type}\")\n",
    "            axes[i * num_events + event_type, 0].set_title(f\"Histogram of Sequence Length ({split})\")\n",
    "            axes[i * num_events + event_type, 0].set_xlabel(\"Sequence Length\")\n",
    "            axes[i * num_events + event_type, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "            # Histogram of the time_since_last_event\n",
    "            axes[i * num_events + event_type, 1].hist(\n",
    "                time_since_last_event, bins=20, edgecolor=\"k\", alpha=0.5, label=f\"Event Type {event_type}\"\n",
    "            )\n",
    "            axes[i * num_events + event_type, 1].set_title(f\"Histogram of Time Since Last Event ({split})\")\n",
    "            axes[i * num_events + event_type, 1].set_xlabel(\"Time Since Last Event\")\n",
    "            axes[i * num_events + event_type, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "        axes[i * num_events + event_type, 0].legend()\n",
    "        axes[i * num_events + event_type, 1].legend()\n",
    "        axes[i * num_events + event_type, 1].set_yscale(\"log\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# --- Helper Function ---\n",
    "\n",
    "\n",
    "def get_max_time(dataset, split=\"train\"):\n",
    "    \"\"\"\n",
    "    Calculates the maximum event time ('time_since_start') across all trajectories\n",
    "    in a split of a Hugging Face Dataset object.\n",
    "    \"\"\"\n",
    "    max_t = 0\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"Warning: Split '{split}' not found in dataset.\")\n",
    "        return 1.0  # Default max time\n",
    "\n",
    "    # Check if 'time_since_start' column exists\n",
    "    if \"time_since_start\" not in data_split.column_names:\n",
    "        print(f\"Error: 'time_since_start' column not found in dataset split '{split}'. Cannot determine max time.\")\n",
    "        return 1.0  # Default max time\n",
    "\n",
    "    for trajectory in data_split:\n",
    "        times = trajectory.get(\"time_since_start\")\n",
    "        # Ensure times is a non-empty list or array\n",
    "        if times and hasattr(times, \"__len__\") and len(times) > 0:\n",
    "            # Check for potential nested lists or other issues if max fails\n",
    "            try:\n",
    "                current_max = np.max(times)\n",
    "                if isinstance(current_max, (int, float)):  # Ensure it's a number\n",
    "                    max_t = max(max_t, current_max)\n",
    "            except (TypeError, ValueError) as e:\n",
    "                print(f\"Warning: Could not compute max time for a trajectory: {e}. Skipping.\")\n",
    "                print(f\"Problematic 'time_since_start' value: {times}\")\n",
    "\n",
    "    return max_t if max_t > 0 else 1.0  # Avoid max_time being 0\n",
    "\n",
    "\n",
    "# --- Visualization Functions ---\n",
    "\n",
    "\n",
    "def plot_aggregate_rate(dataset, split=\"train\", n_bins=50, max_time=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plots the aggregate event rate over time using a Hugging Face Dataset.\n",
    "    Assumes 'time_since_start' contains absolute event times.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): Dictionary containing Hugging Face Dataset objects per split.\n",
    "        split (str): The dataset split to use (e.g., 'train').\n",
    "        n_bins (int): Number of time bins.\n",
    "        max_time (float, optional): Maximum time for the plot range. If None, calculated from data.\n",
    "        ax (matplotlib.axes.Axes, optional): Matplotlib axes to plot on. If None, creates a new figure.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig_created = True\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    else:\n",
    "        fig_created = False\n",
    "\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"No data found for split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    num_trajectories = len(data_split)\n",
    "    if num_trajectories == 0:\n",
    "        print(f\"No trajectories in split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    if \"time_since_start\" not in data_split.column_names:\n",
    "        print(f\"Error: 'time_since_start' column not found in dataset split '{split}'. Cannot plot aggregate rate.\")\n",
    "        return\n",
    "\n",
    "    if max_time is None:\n",
    "        max_time = get_max_time(dataset, split)\n",
    "\n",
    "    # Define time bins\n",
    "    bins = np.linspace(0, max_time, n_bins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_width = bins[1] - bins[0]\n",
    "    if bin_width == 0:  # Avoid division by zero if max_time is 0 or n_bins is 1\n",
    "        print(\"Warning: Cannot calculate rate, time bin width is zero.\")\n",
    "        return\n",
    "\n",
    "    # Count events in each bin\n",
    "    counts = np.zeros(n_bins)\n",
    "\n",
    "    for trajectory in data_split:\n",
    "        times = trajectory.get(\"time_since_start\", [])\n",
    "        # Ensure times is a list/array of numbers\n",
    "        if times and isinstance(times, (list, np.ndarray)):\n",
    "            try:\n",
    "                # Filter out non-numeric types if necessary, though HF datasets usually handle this\n",
    "                numeric_times = [t for t in times if isinstance(t, (int, float))]\n",
    "                if numeric_times:\n",
    "                    hist, _ = np.histogram(numeric_times, bins=bins)\n",
    "                    counts += hist\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process times for a trajectory: {e}\")\n",
    "                print(f\"Problematic 'time_since_start': {times}\")\n",
    "\n",
    "    # Calculate rate (average events per trajectory per unit time)\n",
    "    rate = counts / (num_trajectories * bin_width)\n",
    "\n",
    "    # Plotting\n",
    "    ax.plot(bin_centers, rate, marker=\".\", linestyle=\"-\")\n",
    "    ax.set_title(f\"Aggregate Event Rate ({split} split)\")\n",
    "    ax.set_xlabel(\"Time (from time_since_start)\")\n",
    "    ax.set_ylabel(\"Avg. Events / Trajectory / Time Unit\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_xlim(0, max_time)\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "    if fig_created:  # Only show if we created the figure\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_rate_per_mark(dataset, split=\"train\", n_bins=50, max_time=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plots the event rate per mark over time using a Hugging Face Dataset.\n",
    "    Assumes 'time_since_start' contains absolute event times and 'type_event' contains marks.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): Dictionary containing Hugging Face Dataset objects per split.\n",
    "        split (str): The dataset split to use.\n",
    "        n_bins (int): Number of time bins.\n",
    "        max_time (float, optional): Maximum time for the plot range. If None, calculated from data.\n",
    "        ax (matplotlib.axes.Axes, optional): Matplotlib axes to plot on. If None, creates a new figure.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig_created = True\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    else:\n",
    "        fig_created = False\n",
    "\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"No data found for split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    num_trajectories = len(data_split)\n",
    "    if num_trajectories == 0:\n",
    "        print(f\"No trajectories in split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [\"dim_process\", \"time_since_start\", \"type_event\"]\n",
    "    if not all(col in data_split.column_names for col in required_cols):\n",
    "        print(f\"Error: Dataset split '{split}' missing one or more required columns: {required_cols}\")\n",
    "        return\n",
    "\n",
    "    # Infer number of marks (dim_process) - Assuming it's constant\n",
    "    # Take it from the first row, ensure it's an integer\n",
    "    try:\n",
    "        dim_process = int(data_split[0][\"dim_process\"])\n",
    "    except (ValueError, TypeError):\n",
    "        print(\"Error: 'dim_process' feature is not a valid integer in the first row.\")\n",
    "        return\n",
    "\n",
    "    if max_time is None:\n",
    "        max_time = get_max_time(dataset, split)\n",
    "\n",
    "    # Define time bins\n",
    "    bins = np.linspace(0, max_time, n_bins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_width = bins[1] - bins[0]\n",
    "    if bin_width == 0:\n",
    "        print(\"Warning: Cannot calculate rate, time bin width is zero.\")\n",
    "        return\n",
    "\n",
    "    # Count events per mark in each bin\n",
    "    counts_per_mark = np.zeros((dim_process, n_bins))\n",
    "\n",
    "    for trajectory in data_split:\n",
    "        times = np.array(trajectory.get(\"time_since_start\", []))\n",
    "        types = np.array(trajectory.get(\"type_event\", []))\n",
    "\n",
    "        if len(times) != len(types) or len(times) == 0:\n",
    "            continue  # Skip if data is inconsistent or empty\n",
    "\n",
    "        # Ensure types are integers\n",
    "        try:\n",
    "            types = types.astype(int)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Warning: Could not convert 'type_event' to integers for a trajectory. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for mark in range(dim_process):\n",
    "            try:\n",
    "                # Filter times corresponding to the current mark\n",
    "                mark_times = times[types == mark]\n",
    "                # Ensure mark_times contains numeric data before histogramming\n",
    "                numeric_mark_times = [t for t in mark_times if isinstance(t, (int, float))]\n",
    "                if numeric_mark_times:\n",
    "                    hist, _ = np.histogram(numeric_mark_times, bins=bins)\n",
    "                    counts_per_mark[mark, :] += hist\n",
    "            except IndexError:\n",
    "                print(f\"Warning: Index error processing mark {mark}. Check data consistency (e.g., mark values vs dim_process).\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process mark {mark} for a trajectory: {e}\")\n",
    "\n",
    "    # Calculate rates\n",
    "    rates_per_mark = counts_per_mark / (num_trajectories * bin_width)\n",
    "\n",
    "    # Plotting\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, dim_process))\n",
    "    for mark in range(dim_process):\n",
    "        ax.plot(bin_centers, rates_per_mark[mark, :], marker=\".\", linestyle=\"-\", label=f\"Mark {mark}\", color=colors[mark])\n",
    "\n",
    "    ax.set_title(f\"Event Rate per Mark ({split} split)\")\n",
    "    ax.set_xlabel(\"Time (from time_since_start)\")\n",
    "    ax.set_ylabel(\"Avg. Events / Trajectory / Time Unit\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    ax.set_xlim(0, max_time)\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "    if fig_created:  # Only show if we created the figure\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_raster_sample(dataset, split=\"train\", n_samples=10, max_time=None, ax=None, seed=None):\n",
    "    \"\"\"\n",
    "    Creates a raster plot for a sample of trajectories from a Hugging Face Dataset.\n",
    "    Assumes 'time_since_start' and 'type_event' columns exist.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): Dictionary containing Hugging Face Dataset objects per split.\n",
    "        split (str): The dataset split to use.\n",
    "        n_samples (int): Number of trajectories to sample and plot.\n",
    "        max_time (float, optional): Maximum time for the plot range. If None, calculated from data.\n",
    "        ax (matplotlib.axes.Axes, optional): Matplotlib axes to plot on. If None, creates a new figure.\n",
    "        seed (int, optional): Random seed for reproducibility of sampling.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig_created = True\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    else:\n",
    "        fig_created = False\n",
    "\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"No data found for split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    num_trajectories = len(data_split)\n",
    "    if num_trajectories == 0:\n",
    "        print(f\"No trajectories in split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [\"dim_process\", \"time_since_start\", \"type_event\"]\n",
    "    if not all(col in data_split.column_names for col in required_cols):\n",
    "        print(f\"Error: Dataset split '{split}' missing one or more required columns: {required_cols}\")\n",
    "        return\n",
    "\n",
    "    if num_trajectories < n_samples:\n",
    "        print(f\"Warning: Requested {n_samples} samples, but only {num_trajectories} trajectories available.\")\n",
    "        n_samples = num_trajectories\n",
    "\n",
    "    if n_samples == 0:\n",
    "        return\n",
    "\n",
    "    # Sample indices\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    sampled_indices = random.sample(range(num_trajectories), n_samples)\n",
    "\n",
    "    # Select the sampled trajectories (more efficient than iterating and filtering)\n",
    "    # Note: This creates a new dataset view, doesn't load all into memory at once\n",
    "    sampled_data = data_split.select(sampled_indices)\n",
    "\n",
    "    # Infer number of marks from the first sampled trajectory\n",
    "    try:\n",
    "        dim_process = int(sampled_data[0][\"dim_process\"])\n",
    "    except (ValueError, TypeError):\n",
    "        print(\"Error: 'dim_process' feature is not a valid integer in the first sampled row.\")\n",
    "        return\n",
    "\n",
    "    if max_time is None:\n",
    "        # Calculate max_time only from the sampled trajectories for efficiency\n",
    "        max_t_sample = 0\n",
    "        for traj in sampled_data:\n",
    "            times = traj.get(\"time_since_start\")\n",
    "            if times and hasattr(times, \"__len__\") and len(times) > 0:\n",
    "                try:\n",
    "                    current_max = np.max(times)\n",
    "                    if isinstance(current_max, (int, float)):\n",
    "                        max_t_sample = max(max_t_sample, current_max)\n",
    "                except (TypeError, ValueError):\n",
    "                    pass  # Ignore errors here, focus is on getting a reasonable max\n",
    "        max_time = max_t_sample if max_t_sample > 0 else 1.0\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, dim_process))\n",
    "\n",
    "    # Plot events using scatter\n",
    "    event_times = []\n",
    "    y_positions = []\n",
    "    event_colors = []\n",
    "    for i, trajectory in enumerate(sampled_data):  # Iterate over the selected sample\n",
    "        times = trajectory.get(\"time_since_start\", [])\n",
    "        types = trajectory.get(\"type_event\", [])\n",
    "\n",
    "        if (\n",
    "            not isinstance(times, (list, np.ndarray))\n",
    "            or not isinstance(types, (list, np.ndarray))\n",
    "            or len(times) != len(types)\n",
    "            or len(times) == 0\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Ensure types are integers for indexing colors\n",
    "            types = np.array(types).astype(int)\n",
    "            times = np.array(times)  # Ensure times are numpy array for potential filtering\n",
    "\n",
    "            # Filter out non-numeric times just in case\n",
    "            valid_indices = [idx for idx, t in enumerate(times) if isinstance(t, (int, float))]\n",
    "            times = times[valid_indices]\n",
    "            types = types[valid_indices]\n",
    "\n",
    "            for t, type_ in zip(times, types):\n",
    "                if 0 <= type_ < dim_process:  # Check if type is valid\n",
    "                    event_times.append(t)\n",
    "                    y_positions.append(i)  # Use sample index for y-position\n",
    "                    event_colors.append(colors[type_])\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid event type {type_} encountered in sampled trajectory {i}. Skipping event.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing sampled trajectory {i}: {e}\")\n",
    "\n",
    "    # Using scatter for better color control per event\n",
    "    if event_times:  # Only plot if there's data\n",
    "        ax.scatter(event_times, y_positions, c=event_colors, marker=\"|\", s=100)  # s is marker size\n",
    "    else:\n",
    "        print(\"No valid events found in the sample to plot.\")\n",
    "\n",
    "    ax.set_title(f\"Raster Plot ({n_samples} Sampled Trajectories from {split} split)\")\n",
    "    ax.set_xlabel(\"Time (from time_since_start)\")\n",
    "    ax.set_ylabel(\"Sampled Trajectory Index\")\n",
    "    ax.set_yticks(range(n_samples))\n",
    "    # ax.set_yticklabels([f\"Orig Idx {sampled_indices[i]}\" for i in range(n_samples)]) # Optional\n",
    "    ax.set_ylim(-0.5, n_samples - 0.5)\n",
    "    ax.set_xlim(0, max_time)\n",
    "    ax.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Add a legend for marks\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker=\"|\", color=colors[mark], linestyle=\"None\", markersize=10, label=f\"Mark {mark}\")\n",
    "        for mark in range(dim_process)\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, title=\"Event Marks\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    if fig_created:  # Only show if we created the figure\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_event_count_heatmap(dataset, split=\"train\", n_bins=50, max_time=None, normalize=True, ax=None):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of event counts per mark over time using a Hugging Face Dataset.\n",
    "    Assumes 'time_since_start', 'type_event', 'dim_process' columns exist.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): Dictionary containing Hugging Face Dataset objects per split.\n",
    "        split (str): The dataset split to use.\n",
    "        n_bins (int): Number of time bins.\n",
    "        max_time (float, optional): Maximum time for the plot range. If None, calculated from data.\n",
    "        normalize (bool): If True, normalize counts by the number of trajectories.\n",
    "        ax (matplotlib.axes.Axes, optional): Matplotlib axes to plot on. If None, creates a new figure.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig_created = True\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    else:\n",
    "        fig_created = False\n",
    "\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"No data found for split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    num_trajectories = len(data_split)\n",
    "    if num_trajectories == 0:\n",
    "        print(f\"No trajectories in split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [\"dim_process\", \"time_since_start\", \"type_event\"]\n",
    "    if not all(col in data_split.column_names for col in required_cols):\n",
    "        print(f\"Error: Dataset split '{split}' missing one or more required columns: {required_cols}\")\n",
    "        return\n",
    "\n",
    "    # Infer number of marks\n",
    "    try:\n",
    "        dim_process = int(data_split[0][\"dim_process\"])\n",
    "    except (ValueError, TypeError):\n",
    "        print(\"Error: 'dim_process' feature is not a valid integer in the first row.\")\n",
    "        return\n",
    "\n",
    "    if max_time is None:\n",
    "        max_time = get_max_time(dataset, split)\n",
    "\n",
    "    # Define time bins\n",
    "    bins = np.linspace(0, max_time, n_bins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2  # For labeling ticks\n",
    "\n",
    "    # Initialize count matrix (marks x time_bins)\n",
    "    heatmap_counts = np.zeros((dim_process, n_bins))\n",
    "\n",
    "    # Populate the heatmap counts\n",
    "    for trajectory in data_split:\n",
    "        times = np.array(trajectory.get(\"time_since_start\", []))\n",
    "        types = np.array(trajectory.get(\"type_event\", []))\n",
    "\n",
    "        if len(times) != len(types) or len(times) == 0:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            types = types.astype(int)\n",
    "            # Filter out non-numeric times\n",
    "            valid_indices = [idx for idx, t in enumerate(times) if isinstance(t, (int, float))]\n",
    "            times = times[valid_indices]\n",
    "            types = types[valid_indices]\n",
    "\n",
    "            for mark in range(dim_process):\n",
    "                mark_times = times[types == mark]\n",
    "                if len(mark_times) > 0:  # Only histogram if there are times for this mark\n",
    "                    hist, _ = np.histogram(mark_times, bins=bins)\n",
    "                    heatmap_counts[mark, :] += hist\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process trajectory for heatmap: {e}\")\n",
    "\n",
    "    # Normalize counts (optional)\n",
    "    if normalize and num_trajectories > 0:\n",
    "        heatmap_data = heatmap_counts / num_trajectories\n",
    "        cbar_label = \"Avg Events / Trajectory / Bin\"\n",
    "    else:\n",
    "        heatmap_data = heatmap_counts\n",
    "        cbar_label = \"Total Events / Bin\"\n",
    "\n",
    "    # Plotting the heatmap\n",
    "    sns.heatmap(heatmap_data, ax=ax, cmap=\"viridis\", cbar_kws={\"label\": cbar_label})\n",
    "\n",
    "    # Set x-axis ticks and labels (show fewer labels for clarity)\n",
    "    tick_positions = np.linspace(0, n_bins - 1, num=min(n_bins, 10), dtype=int)  # Show ~10 ticks\n",
    "    ax.set_xticks(tick_positions + 0.5)  # Center ticks\n",
    "    ax.set_xticklabels([f\"{bin_centers[i]:.2f}\" for i in tick_positions], rotation=45, ha=\"right\")\n",
    "\n",
    "    ax.set_yticks(np.arange(dim_process) + 0.5)\n",
    "    ax.set_yticklabels([f\"Mark {i}\" for i in range(dim_process)], rotation=0)\n",
    "\n",
    "    ax.set_xlabel(\"Time Bins (from time_since_start)\")\n",
    "    ax.set_ylabel(\"Event Mark\")\n",
    "    title = f\"Event Count Heatmap ({split} split)\"\n",
    "    if normalize:\n",
    "        title = f\"Average {title}\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "    if fig_created:  # Only show if we created the figure\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_inter_event_times(dataset, split=\"train\", n_bins=50, plot_type=\"all_consecutive\"):\n",
    "    \"\"\"\n",
    "    Plots histograms of inter-event times using a Hugging Face Dataset.\n",
    "    Assumes 'time_since_start' and 'type_event' columns exist.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): Dictionary containing Hugging Face Dataset objects per split.\n",
    "        split (str): The dataset split to use.\n",
    "        n_bins (int): Number of bins for the histogram.\n",
    "        plot_type (str): Type of inter-event time to plot:\n",
    "                         'all_consecutive': Time between any two consecutive events.\n",
    "                         'mark_to_next': Time from an event of a specific mark to the *next* event (any mark).\n",
    "                         'time_since_last': Uses the 'time_since_last_event' feature directly.\n",
    "    \"\"\"\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"No data found for split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    num_trajectories = len(data_split)\n",
    "    if num_trajectories == 0:\n",
    "        print(f\"No trajectories in split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    all_diffs = []\n",
    "\n",
    "    if plot_type == \"all_consecutive\":\n",
    "        if \"time_since_start\" not in data_split.column_names:\n",
    "            print(\"Error: 'time_since_start' column required for plot_type='all_consecutive'.\")\n",
    "            return\n",
    "\n",
    "        for trajectory in data_split:\n",
    "            times = trajectory.get(\"time_since_start\", [])\n",
    "            # Ensure times are numeric and sortable\n",
    "            if times and isinstance(times, (list, np.ndarray)):\n",
    "                try:\n",
    "                    numeric_times = np.sort([t for t in times if isinstance(t, (int, float))])\n",
    "                    if len(numeric_times) > 1:\n",
    "                        diffs = np.diff(numeric_times)\n",
    "                        all_diffs.extend(diffs)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not calculate time differences for a trajectory: {e}\")\n",
    "\n",
    "        if not all_diffs:\n",
    "            print(\"No consecutive events found to calculate differences.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(all_diffs, bins=n_bins, edgecolor=\"k\", alpha=0.7)\n",
    "        plt.title(f\"Histogram of Time Between Consecutive Events ({split} split)\")\n",
    "        plt.xlabel(\"Time Difference (dt from time_since_start)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.yscale(\"log\")  # Often useful for time differences\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    elif plot_type == \"mark_to_next\":\n",
    "        required_cols = [\"dim_process\", \"time_since_start\", \"type_event\"]\n",
    "        if not all(col in data_split.column_names for col in required_cols):\n",
    "            print(f\"Error: Dataset split '{split}' missing one or more required columns for plot_type='mark_to_next': {required_cols}\")\n",
    "            return\n",
    "\n",
    "        # Infer number of marks\n",
    "        try:\n",
    "            dim_process = int(data_split[0][\"dim_process\"])\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Error: 'dim_process' feature is not a valid integer in the first row.\")\n",
    "            return\n",
    "\n",
    "        diffs_by_mark = defaultdict(list)\n",
    "\n",
    "        for trajectory in data_split:\n",
    "            times = np.array(trajectory.get(\"time_since_start\", []))\n",
    "            types = np.array(trajectory.get(\"type_event\", []))\n",
    "\n",
    "            if len(times) != len(types) or len(times) < 2:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Sort by time\n",
    "                valid_indices = [idx for idx, t in enumerate(times) if isinstance(t, (int, float))]\n",
    "                if len(valid_indices) < 2:\n",
    "                    continue  # Need at least 2 valid times\n",
    "\n",
    "                times = times[valid_indices]\n",
    "                types = types[valid_indices].astype(int)  # Assume types correspond to valid times\n",
    "\n",
    "                sort_indices = np.argsort(times)\n",
    "                times = times[sort_indices]\n",
    "                types = types[sort_indices]\n",
    "\n",
    "                for i in range(len(times) - 1):\n",
    "                    current_mark = types[i]\n",
    "                    time_diff = times[i + 1] - times[i]\n",
    "                    if 0 <= current_mark < dim_process:  # Check mark validity\n",
    "                        diffs_by_mark[current_mark].append(time_diff)\n",
    "                    else:\n",
    "                        print(f\"Warning: Invalid mark {current_mark} encountered.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process trajectory for mark_to_next diffs: {e}\")\n",
    "\n",
    "        if not diffs_by_mark:\n",
    "            print(\"No valid events found to calculate mark-to-next differences.\")\n",
    "            return\n",
    "\n",
    "        # Plotting - create subplots for each mark\n",
    "        n_marks = dim_process\n",
    "        n_cols = 3\n",
    "        n_rows = (n_marks + n_cols - 1) // n_cols\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows), squeeze=False)\n",
    "        axes = axes.flatten()  # Flatten to 1D array for easy iteration\n",
    "\n",
    "        for mark in range(n_marks):\n",
    "            ax = axes[mark]\n",
    "            mark_diffs = diffs_by_mark.get(mark, [])\n",
    "            if mark_diffs:\n",
    "                ax.hist(mark_diffs, bins=n_bins, edgecolor=\"k\", alpha=0.7)\n",
    "                ax.set_title(f\"Mark {mark} to Next Event\")\n",
    "                ax.set_xlabel(\"Time Difference (dt)\")\n",
    "                ax.set_ylabel(\"Frequency\")\n",
    "                ax.set_yscale(\"log\")\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "                ax.set_title(f\"Mark {mark} to Next Event\")\n",
    "\n",
    "        # Hide unused subplots\n",
    "        for i in range(n_marks, len(axes)):\n",
    "            fig.delaxes(axes[i])\n",
    "\n",
    "        fig.suptitle(f\"Time from Event Mark to Next Event ({split} split)\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to prevent title overlap\n",
    "        plt.show()\n",
    "\n",
    "    elif plot_type == \"time_since_last\":\n",
    "        # Plot histogram directly from the 'time_since_last_event' feature\n",
    "        if \"time_since_last_event\" not in data_split.column_names:\n",
    "            print(\"Error: 'time_since_last_event' column required for plot_type='time_since_last'.\")\n",
    "            return\n",
    "\n",
    "        for trajectory in data_split:\n",
    "            tsl = trajectory.get(\"time_since_last_event\", [])\n",
    "            if tsl and isinstance(tsl, (list, np.ndarray)):\n",
    "                # Filter out potential non-numeric values or NaNs\n",
    "                numeric_tsl = [t for t in tsl if isinstance(t, (int, float)) and np.isfinite(t)]\n",
    "                all_diffs.extend(numeric_tsl)\n",
    "\n",
    "        if not all_diffs:\n",
    "            print(\"No valid 'time_since_last_event' data found.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(all_diffs, bins=n_bins, edgecolor=\"k\", alpha=0.7)\n",
    "        plt.title(f\"Histogram of Time Since Last Event ({split} split)\")\n",
    "        plt.xlabel(\"Time Since Last Event (feature value)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Unknown plot_type '{plot_type}'. Choose 'all_consecutive', 'mark_to_next', or 'time_since_last'.\")\n",
    "\n",
    "\n",
    "# plot_branching_ratio_matrix remains the same as it depends on an external matrix,\n",
    "# not directly on the dataset structure for iteration.\n",
    "def plot_branching_ratio_matrix(alpha_matrix, ax=None):\n",
    "    \"\"\"\n",
    "    Plots the estimated branching ratio (influence) matrix as a heatmap.\n",
    "    NOTE: This function requires the matrix 'alpha_matrix' to be pre-computed\n",
    "          through Hawkes process parameter estimation.\n",
    "\n",
    "    Args:\n",
    "        alpha_matrix (np.ndarray): A square matrix where alpha[i, j] represents\n",
    "                                   the influence of mark j on mark i.\n",
    "        ax (matplotlib.axes.Axes, optional): Matplotlib axes to plot on. If None, creates a new figure.\n",
    "    \"\"\"\n",
    "    if not isinstance(alpha_matrix, np.ndarray) or alpha_matrix.ndim != 2 or alpha_matrix.shape[0] != alpha_matrix.shape[1]:\n",
    "        print(\"Error: alpha_matrix must be a square numpy array.\")\n",
    "        return\n",
    "\n",
    "    if ax is None:\n",
    "        fig_created = True\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    else:\n",
    "        fig_created = False\n",
    "\n",
    "    dim_process = alpha_matrix.shape[0]\n",
    "    sns.heatmap(\n",
    "        alpha_matrix,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"viridis\",\n",
    "        ax=ax,\n",
    "        cbar_kws={\"label\": \"Influence Magnitude (α_ij)\"},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"black\",\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Influencing Mark (j)\")\n",
    "    ax.set_ylabel(\"Influenced Mark (i)\")\n",
    "    ax.set_title(\"Estimated Branching Ratio / Influence Matrix (α)\")\n",
    "    ax.set_xticklabels(range(dim_process))\n",
    "    ax.set_yticklabels(range(dim_process), rotation=0)\n",
    "\n",
    "    if fig_created:  # Only show if we created the figure\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assuming 'my_hf_dataset' is loaded and is a dictionary like:\n",
    "# my_hf_dataset = {'train': Dataset(...), 'test': Dataset(...)}\n",
    "#\n",
    "# plot_aggregate_rate(my_hf_dataset, split='train')\n",
    "# plot_rate_per_mark(my_hf_dataset, split='train')\n",
    "# plot_raster_sample(my_hf_dataset, split='train', n_samples=15)\n",
    "# plot_event_count_heatmap(my_hf_dataset, split='train', n_bins=60)\n",
    "# plot_inter_event_times(my_hf_dataset, split='train', plot_type='all_consecutive')\n",
    "# plot_inter_event_times(my_hf_dataset, split='train', plot_type='mark_to_next')\n",
    "# plot_inter_event_times(my_hf_dataset, split='train', plot_type='time_since_last') # Use existing feature\n",
    "#\n",
    "# # For branching ratios (requires external estimation):\n",
    "# # estimated_alpha = estimate_hawkes_params(my_hf_dataset['train']) # Your estimation code\n",
    "# # plot_branching_ratio_matrix(estimated_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(taxi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aggregate_rate(taxi_data)\n",
    "\n",
    "plot_rate_per_mark(taxi_data)\n",
    "\n",
    "plot_raster_sample(taxi_data)\n",
    "\n",
    "plot_event_count_heatmap(taxi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_stats_per_event_type(taxi_data, splits=['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(retweet_data, splits=[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_stats_per_event_type(retweet_data, splits=['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aggregate_rate(retweet_data)\n",
    "\n",
    "plot_rate_per_mark(retweet_data)\n",
    "\n",
    "plot_raster_sample(retweet_data)\n",
    "\n",
    "plot_event_count_heatmap(retweet_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(amazon_data, splits=[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_stats_per_event_type(amazon_data, splits=['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aggregate_rate(amazon_data)\n",
    "\n",
    "plot_rate_per_mark(amazon_data)\n",
    "\n",
    "plot_raster_sample(amazon_data)\n",
    "\n",
    "plot_event_count_heatmap(amazon_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taobao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(taobao_data, splits=[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_stats_per_event_type(taobao_data, splits=['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aggregate_rate(taobao_data)\n",
    "\n",
    "plot_rate_per_mark(taobao_data)\n",
    "\n",
    "plot_raster_sample(taobao_data)\n",
    "\n",
    "plot_event_count_heatmap(taobao_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(stackoverflow_data, splits=[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aggregate_rate(stackoverflow_data)\n",
    "\n",
    "plot_rate_per_mark(stackoverflow_data)\n",
    "\n",
    "plot_raster_sample(stackoverflow_data)\n",
    "\n",
    "plot_event_count_heatmap(stackoverflow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_stats_per_event_type(stackoverflow_data, splits=['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from tick.hawkes import HawkesKernelTimeFunc, SimuHawkes\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# == Helper Function: Convert Tick Timestamps\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def tick_timestamps_to_single_timeseries(tick_timestamps):\n",
    "    \"\"\"\n",
    "    Converts the list of arrays format from tick library (one array per event type)\n",
    "    into a single sorted time series with corresponding event types.\n",
    "\n",
    "    Args:\n",
    "        tick_timestamps: list[np.array]\n",
    "            List where each element is a numpy array of event times for a specific type.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.array, np.array]:\n",
    "            - Sorted event times.\n",
    "            - Corresponding event types.\n",
    "    \"\"\"\n",
    "    # Handle cases where some event types might have no events\n",
    "    event_times_list = []\n",
    "    event_types_list = []\n",
    "    for event_type, events in enumerate(tick_timestamps):\n",
    "        # Check if events is a list/array and has content\n",
    "        if events is not None and hasattr(events, \"__len__\") and len(events) > 0:\n",
    "            event_times_list.append(np.asarray(events))\n",
    "            event_types_list.append(np.full(len(events), event_type, dtype=int))\n",
    "\n",
    "    if not event_times_list:  # No events simulated\n",
    "        return np.array([]), np.array([], dtype=int)\n",
    "\n",
    "    # Concatenate all event times and types\n",
    "    event_times = np.concatenate(event_times_list)\n",
    "    event_types = np.concatenate(event_types_list)\n",
    "\n",
    "    # Sort events chronologically\n",
    "    sorted_indices = np.argsort(event_times)\n",
    "\n",
    "    return event_times[sorted_indices], event_types[sorted_indices]\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# == Kernel Function Classes\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class HawkesExpKernel:\n",
    "    \"\"\"\n",
    "    Evaluates an exponential kernel function: a_0 * exp(-a_1 * t).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a_0: float, a_1: float) -> None:\n",
    "        if a_1 <= 0:\n",
    "            raise ValueError(\"Decay rate a_1 must be positive.\")\n",
    "        self.a_0 = a_0\n",
    "        self.a_1 = a_1\n",
    "\n",
    "    def __call__(self, grid_size: int, max_time: float = None, eps: float = 1e-3):\n",
    "        if abs(self.a_0) <= eps:  # Handle cases where amplitude is very small or zero\n",
    "            max_time_calc = 1.0  # Default to a small interval\n",
    "        elif max_time is None:\n",
    "            max_time_calc = -np.log(eps / abs(self.a_0)) / self.a_1\n",
    "        else:\n",
    "            max_time_calc = max_time\n",
    "        max_time_calc = max(max_time_calc, eps)  # Ensure max_time is positive\n",
    "        t_grid = np.linspace(0, max_time_calc, grid_size)\n",
    "        kernel_values = self.a_0 * np.exp(-self.a_1 * t_grid)\n",
    "        return t_grid, kernel_values\n",
    "\n",
    "\n",
    "class HawkesExpSquaredKernel:\n",
    "    \"\"\"\n",
    "    Evaluates an exponential-squared kernel function: a_0 * t * exp(-a_1 * t^2).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a_0: float, a_1: float) -> None:\n",
    "        if a_1 <= 0:\n",
    "            raise ValueError(\"Decay rate a_1 must be positive.\")\n",
    "        self.a_0 = a_0\n",
    "        self.a_1 = a_1\n",
    "\n",
    "    def __call__(self, grid_size: int, max_time: float = None, eps: float = 1e-3):\n",
    "        if max_time is None:\n",
    "            peak_time = np.sqrt(1.0 / (2.0 * self.a_1)) if self.a_1 > 0 else 0\n",
    "            try:\n",
    "                if abs(self.a_0) > 1e-9 and self.a_1 > 1e-9:\n",
    "                    # Heuristic based on exponential decay part\n",
    "                    log_term = -np.log(eps / abs(self.a_0 * max(peak_time, 1.0))) / self.a_1  # Rough estimate\n",
    "                    max_time_calc = np.sqrt(max(log_term, 1.0))\n",
    "                else:\n",
    "                    max_time_calc = 5.0 * peak_time  # Fallback heuristic\n",
    "            except (OverflowError, ValueError, RuntimeWarning):\n",
    "                max_time_calc = 5.0 * peak_time  # Fallback heuristic\n",
    "            max_time_calc = max(max_time_calc, 5.0 * peak_time, eps)  # Ensure covers peak and is positive\n",
    "        else:\n",
    "            max_time_calc = max_time\n",
    "\n",
    "        t_grid = np.linspace(0, max_time_calc, grid_size)\n",
    "        kernel_values = self.a_0 * t_grid * np.exp(-self.a_1 * t_grid**2)\n",
    "        return t_grid, kernel_values\n",
    "\n",
    "\n",
    "class HawkesExpShiftedKernel:\n",
    "    \"\"\"\n",
    "    Evaluates a shifted exponential (Gaussian-like) kernel function:\n",
    "    a_0 * exp(-(t - a_1)^2 / (2 * a_2^2)).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a_0: float, a_1: float, a_2: float) -> None:\n",
    "        if a_2 <= 0:\n",
    "            raise ValueError(\"Spread parameter a_2 must be positive.\")\n",
    "        self.a_0 = a_0\n",
    "        self.a_1 = a_1  # Center\n",
    "        self.a_2 = a_2  # Std dev\n",
    "\n",
    "    def __call__(self, grid_size: int, max_time: float = None, eps: float = 1e-3):\n",
    "        if max_time is None:\n",
    "            if abs(self.a_0) <= eps:  # Handle low amplitude\n",
    "                max_time_calc = self.a_1 + 3 * self.a_2  # Go 3 standard deviations out\n",
    "            else:\n",
    "                try:\n",
    "                    # Calculate time t where Gaussian drops to eps\n",
    "                    log_term = -2 * self.a_2**2 * np.log(eps / abs(self.a_0))\n",
    "                    if log_term < 0:  # log argument was > 1 (eps > |a_0|)\n",
    "                        max_time_calc = self.a_1 + 3 * self.a_2  # Fallback\n",
    "                    else:\n",
    "                        max_time_calc = self.a_1 + np.sqrt(log_term)\n",
    "                except (ValueError, OverflowError, RuntimeWarning):\n",
    "                    max_time_calc = self.a_1 + 3 * self.a_2  # Fallback\n",
    "            max_time_calc = max(max_time_calc, self.a_1 + 3 * self.a_2, eps)  # Ensure covers peak+decay, positive\n",
    "        else:\n",
    "            max_time_calc = max_time\n",
    "\n",
    "        t_grid = np.linspace(0, max_time_calc, grid_size)\n",
    "        kernel_values = self.a_0 * np.exp(-((t_grid - self.a_1) ** 2) / (2 * self.a_2**2))\n",
    "        kernel_values[t_grid < 0] = 0  # Ensure kernel is zero for t < 0\n",
    "        return t_grid, kernel_values\n",
    "\n",
    "\n",
    "class HawkesExpSinKernel:\n",
    "    \"\"\"\n",
    "    Evaluates an exponentially decaying sinusoidal kernel function:\n",
    "    a_0 * sin(a_2 * t) * exp(-a_1 * t).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a_0: float, a_1: float, a_2: float) -> None:\n",
    "        if a_1 <= 0:\n",
    "            raise ValueError(\"Decay rate a_1 must be positive.\")\n",
    "        self.a_0 = a_0\n",
    "        self.a_1 = a_1  # Decay\n",
    "        self.a_2 = a_2  # Frequency\n",
    "\n",
    "    def __call__(self, grid_size: int, max_time: float = None, eps: float = 1e-3):\n",
    "        if max_time is None:\n",
    "            if abs(self.a_0) <= eps:\n",
    "                max_time_calc = 1.0  # Default small interval\n",
    "            else:\n",
    "                max_time_calc = -np.log(eps / abs(self.a_0)) / self.a_1\n",
    "        else:\n",
    "            max_time_calc = max_time\n",
    "        max_time_calc = max(max_time_calc, eps)  # Ensure max_time is positive\n",
    "        t_grid = np.linspace(0, max_time_calc, grid_size)\n",
    "        kernel_values = self.a_0 * np.sin(self.a_2 * t_grid) * np.exp(-self.a_1 * t_grid)\n",
    "        return t_grid, kernel_values\n",
    "\n",
    "\n",
    "class HawkesPowerLawKernel:\n",
    "    \"\"\"\n",
    "    Evaluates a power-law kernel function: a_0 * (t + c)^(-a_1).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a_0: float, a_1: float, c: float) -> None:\n",
    "        if a_1 <= 0:\n",
    "            print(f\"Warning: Power-law exponent a_1={a_1} is not positive. Kernel may not decay.\")\n",
    "        if c <= 0:\n",
    "            raise ValueError(\"Shift constant c must be positive.\")\n",
    "        self.a_0 = a_0\n",
    "        self.a_1 = a_1  # Exponent\n",
    "        self.c = c  # Shift\n",
    "\n",
    "    def __call__(self, grid_size: int, max_time: float = None, eps: float = 1e-3):\n",
    "        if max_time is None:\n",
    "            if abs(self.a_0) <= eps or self.a_1 <= 0:  # Handle low amplitude or non-decaying\n",
    "                max_time_calc = 100.0  # Default large interval\n",
    "            else:\n",
    "                try:\n",
    "                    base = abs(self.a_0) / eps\n",
    "                    exponent = 1.0 / self.a_1\n",
    "                    # Prevent overflow by checking large intermediate results\n",
    "                    if exponent * np.log(base) > np.log(np.finfo(float).max / 2):\n",
    "                        max_time_calc = 1e6  # Use a large fallback max_time\n",
    "                    else:\n",
    "                        max_time_calc = (base) ** exponent - self.c\n",
    "                except (ValueError, OverflowError, ZeroDivisionError, RuntimeWarning):\n",
    "                    max_time_calc = 100.0  # Fallback\n",
    "            max_time_calc = max(max_time_calc, eps)  # Ensure max_time is positive\n",
    "        else:\n",
    "            max_time_calc = max_time\n",
    "\n",
    "        t_grid = np.linspace(0, max_time_calc, grid_size)\n",
    "        # Add small epsilon to prevent division by zero if t=0 and c is tiny\n",
    "        kernel_values = self.a_0 * (t_grid + self.c + 1e-12) ** (-self.a_1)\n",
    "        return t_grid, kernel_values\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# == Simulation Function (Using Tick)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def run_hawkes_simulation(baselines, kernel_grids, kernel_evaluations, num_paths, n_events_per_path, track_intensity, seed=0):\n",
    "    \"\"\"\n",
    "    Runs a Hawkes simulation using the tick library.\n",
    "\n",
    "    Args:\n",
    "        baselines (np.array): Baseline intensities for each event type (marks). Shape (num_marks,).\n",
    "        kernel_grids (list[np.array]): List of time grids for each kernel (num_marks items).\n",
    "        kernel_evaluations (list[np.array]): List of kernel values corresponding to grids (num_marks items).\n",
    "                                             Assumes diagonal kernels (kernel i affects only type i).\n",
    "        num_paths (int): Number of independent simulation paths.\n",
    "        n_events_per_path (int): Maximum number of events to simulate per path.\n",
    "        track_intensity (bool): Whether to track the intensity function over time.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - event_times (np.array): Simulated event times [num_paths, n_events_per_path]. Padded with NaNs.\n",
    "            - event_types (np.array): Simulated event types [num_paths, n_events_per_path]. Padded with -1.\n",
    "            - intensity_times (list[torch.Tensor] | None): List of time points for tracked intensities (if tracked).\n",
    "            - intensities (list[torch.Tensor] | None): List of tracked intensity values (if tracked).\n",
    "    \"\"\"\n",
    "    num_marks = len(baselines)\n",
    "    if not (len(kernel_grids) == num_marks and len(kernel_evaluations) == num_marks):\n",
    "        raise ValueError(\"Baselines, kernel_grids, and kernel_evaluations must have the same length (num_marks).\")\n",
    "\n",
    "    # Initialize Hawkes simulator from tick\n",
    "    hawkes = SimuHawkes(baseline=baselines.tolist(), max_jumps=n_events_per_path, seed=seed, verbose=False)\n",
    "    hawkes.threshold_negative_intensity(allow=True)  # Allow inhibitory effects\n",
    "\n",
    "    # Set the diagonal kernels\n",
    "    for i in range(num_marks):\n",
    "        t_vals = np.asarray(kernel_grids[i])\n",
    "        y_vals = np.asarray(kernel_evaluations[i])\n",
    "\n",
    "        # Ensure grid is valid (non-empty, sorted, starts near 0)\n",
    "        if len(t_vals) == 0:\n",
    "            print(f\"Warning: Kernel grid for mark {i} is empty. Skipping kernel setting.\")\n",
    "            continue\n",
    "\n",
    "        t_vals = np.maximum(t_vals, 0)  # Ensure time is non-negative\n",
    "        sort_idx = np.argsort(t_vals)\n",
    "        t_vals = t_vals[sort_idx]\n",
    "        y_vals = y_vals[sort_idx]\n",
    "\n",
    "        # tick requires the first time point to be exactly 0 for HawkesKernelTimeFunc\n",
    "        if not np.isclose(t_vals[0], 0):\n",
    "            y_at_zero = y_vals[0]  # Use first value as approximation for t=0\n",
    "            t_vals = np.insert(t_vals, 0, 0.0)\n",
    "            y_vals = np.insert(y_vals, 0, y_at_zero)\n",
    "            # Remove duplicates if inserting 0 created one\n",
    "            unique_indices = np.unique(t_vals, return_index=True)[1]\n",
    "            t_vals = t_vals[unique_indices]\n",
    "            y_vals = y_vals[unique_indices]\n",
    "\n",
    "        # Ensure there are at least two points for the kernel function\n",
    "        if len(t_vals) < 2:\n",
    "            print(f\"Warning: Kernel grid for mark {i} has < 2 points after processing. Using constant kernel.\")\n",
    "            # Create a minimal valid kernel (e.g., constant value at 0)\n",
    "            t_vals = np.array([0.0, max(t_vals[0] + 1e-3, 1e-3)])  # Add a small time step\n",
    "            y_vals = np.array([y_vals[0], y_vals[0]])  # Constant value\n",
    "\n",
    "        try:\n",
    "            kernel = HawkesKernelTimeFunc(t_values=t_vals, y_values=y_vals)\n",
    "            hawkes.set_kernel(i, i, kernel)  # Set kernel from type i to type i\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting kernel for mark {i}: {e}\")\n",
    "            print(f\"  t_values: {t_vals}\")\n",
    "            print(f\"  y_values: {y_vals}\")\n",
    "            continue  # Skip this kernel if invalid\n",
    "\n",
    "    # Configure intensity tracking if requested\n",
    "    if track_intensity:\n",
    "        min_max_time = float(\"inf\")\n",
    "        for grid in kernel_grids:\n",
    "            if len(grid) > 1:\n",
    "                min_max_time = min(min_max_time, grid[-1])\n",
    "        intensity_track_step = max(min_max_time / 20, 1e-4) if min_max_time > 1e-6 else 0.01\n",
    "        hawkes.track_intensity(intensity_track_step)\n",
    "\n",
    "    # Store results\n",
    "    all_event_times = []\n",
    "    all_event_types = []\n",
    "    all_intensities = []\n",
    "    all_intensity_times = []\n",
    "\n",
    "    print(f\"Starting Hawkes simulation: {num_paths} paths, {n_events_per_path} events/path, {num_marks} marks.\")\n",
    "\n",
    "    for i in range(num_paths):\n",
    "        try:\n",
    "            hawkes.reset()\n",
    "            hawkes.simulate()\n",
    "\n",
    "            path_times, path_types = tick_timestamps_to_single_timeseries(hawkes.timestamps)\n",
    "\n",
    "            first_event_time_orig = path_times[0] if len(path_times) > 0 else 0\n",
    "\n",
    "            if len(path_times) == 0:\n",
    "                # print(f\"Warning: Path {i} produced 0 events.\")\n",
    "                padded_times = np.full(n_events_per_path, np.nan)\n",
    "                padded_types = np.full(n_events_per_path, -1, dtype=int)\n",
    "            else:\n",
    "                path_times = path_times - first_event_time_orig  # Normalize start time to 0\n",
    "\n",
    "                current_len = len(path_times)\n",
    "                if current_len < n_events_per_path:\n",
    "                    pad_width = n_events_per_path - current_len\n",
    "                    padded_times = np.pad(path_times, (0, pad_width), \"constant\", constant_values=np.nan)\n",
    "                    padded_types = np.pad(path_types, (0, pad_width), \"constant\", constant_values=-1)\n",
    "                else:\n",
    "                    padded_times = path_times[:n_events_per_path]\n",
    "                    padded_types = path_types[:n_events_per_path]\n",
    "\n",
    "            all_event_times.append(padded_times)\n",
    "            all_event_types.append(padded_types)\n",
    "\n",
    "            if track_intensity:\n",
    "                tracked_intensity_np = np.array(hawkes.tracked_intensity)\n",
    "                tracked_intensity_times_np = np.array(hawkes.intensity_tracked_times)\n",
    "\n",
    "                if len(tracked_intensity_times_np) > 0:\n",
    "                    # Align intensity time relative to the (original) first event time\n",
    "                    tracked_intensity_times_np = tracked_intensity_times_np - first_event_time_orig\n",
    "\n",
    "                all_intensities.append(torch.from_numpy(tracked_intensity_np))\n",
    "                all_intensity_times.append(torch.from_numpy(tracked_intensity_times_np))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during simulation path {i}: {e}\")\n",
    "            all_event_times.append(np.full(n_events_per_path, np.nan))\n",
    "            all_event_types.append(np.full(n_events_per_path, -1, dtype=int))\n",
    "            if track_intensity:\n",
    "                all_intensities.append(torch.empty(0))\n",
    "                all_intensity_times.append(torch.empty(0))\n",
    "            # Depending on the error, you might want to break or log more details\n",
    "            # break # Uncomment to stop simulation on first error\n",
    "\n",
    "    print(\"Simulation finished.\")\n",
    "\n",
    "    final_event_times = np.stack(all_event_times)\n",
    "    final_event_types = np.stack(all_event_types)\n",
    "\n",
    "    return (\n",
    "        final_event_times,\n",
    "        final_event_types,\n",
    "        all_intensity_times if track_intensity else None,\n",
    "        all_intensities if track_intensity else None,\n",
    "    )\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# == Parameter Definitions (Based on PDF Stats)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Dataset Statistics (Estimates from PDF) ---\n",
    "dataset_stats = {\n",
    "    \"volcano\": {\"num_marks\": 1, \"baseline\": 0.1, \"avg_len\": 7, \"max_time\": 150},\n",
    "    \"taobao\": {\"num_marks\": 1, \"baseline\": 0.05, \"avg_len\": 50, \"max_time\": 20000},  # Using 1 mark for simplicity\n",
    "    \"retweet\": {\"num_marks\": 1, \"baseline\": 5.0, \"avg_len\": 20, \"max_time\": 2},\n",
    "    \"stackoverflow\": {\"num_marks\": 1, \"baseline\": 0.1, \"avg_len\": 10, \"max_time\": 100},  # Using 1 mark for simplicity\n",
    "    \"taxi\": {\"num_marks\": 2, \"baseline\": 10.0, \"avg_len\": 20, \"max_time\": 1},\n",
    "    \"amazon\": {\"num_marks\": 1, \"baseline\": 0.02, \"avg_len\": 10, \"max_time\": 600},  # Using 1 mark for simplicity\n",
    "    \"earthquake\": {\"num_marks\": 1, \"baseline\": 0.01, \"avg_len\": 100, \"max_time\": 10000},\n",
    "}\n",
    "\n",
    "# --- Kernel Parameter Calculation ---\n",
    "kernel_params = {}\n",
    "grid_size = 100  # Number of points for kernel evaluation grid\n",
    "\n",
    "for name, stats in dataset_stats.items():\n",
    "    kernel_params[name] = {}\n",
    "    mu = stats[\"baseline\"]\n",
    "    Tmax = stats[\"max_time\"]\n",
    "    num_marks = stats[\"num_marks\"]\n",
    "    avg_len = stats[\"avg_len\"]\n",
    "\n",
    "    # --- Exponential Kernel ---\n",
    "    a1_exp = 3.0 / Tmax if Tmax > 1e-6 else 1.0\n",
    "    a1_exp = max(a1_exp, 1e-6)\n",
    "    a0_exp = 0.5 * a1_exp  # Norm = 0.5\n",
    "    kernel_params[name][\"Exp\"] = {\"a_0\": a0_exp, \"a_1\": a1_exp}\n",
    "\n",
    "    # --- ExpSquared Kernel ---\n",
    "    a1_expsq = 50.0 / Tmax**2 if Tmax > 1e-3 else 1.0\n",
    "    a1_expsq = max(a1_expsq, 1e-6)\n",
    "    a0_expsq = 0.5 * (2 * a1_expsq)  # Norm = 0.5\n",
    "    kernel_params[name][\"ExpSquared\"] = {\"a_0\": a0_expsq, \"a_1\": a1_expsq}\n",
    "\n",
    "    # --- ExpShifted Kernel (Gaussian) ---\n",
    "    a1_shift = Tmax / 5.0 if Tmax > 1e-6 else 0.1\n",
    "    a2_shift = Tmax / 10.0 if Tmax > 1e-6 else 0.05\n",
    "    a2_shift = max(a2_shift, 1e-3)\n",
    "    norm_gauss = 0.5\n",
    "    a0_shift = norm_gauss / (a2_shift * np.sqrt(2 * np.pi)) if a2_shift > 1e-9 else 0.1\n",
    "    kernel_params[name][\"ExpShifted\"] = {\"a_0\": a0_shift, \"a_1\": a1_shift, \"a_2\": a2_shift}\n",
    "\n",
    "    # --- ExpSin Kernel ---\n",
    "    a1_expsin = 3.0 / Tmax if Tmax > 1e-6 else 1.0\n",
    "    a1_expsin = max(a1_expsin, 1e-6)\n",
    "    a2_expsin = 4 * np.pi / Tmax if Tmax > 1e-6 else np.pi\n",
    "    a0_expsin = 0.3 * a1_expsin  # Implicit norm ~0.3\n",
    "    kernel_params[name][\"ExpSin\"] = {\"a_0\": a0_expsin, \"a_1\": a1_expsin, \"a_2\": a2_expsin}\n",
    "\n",
    "    # --- PowerLaw Kernel ---\n",
    "    a1_pow = 1.1\n",
    "    avg_inter_event = Tmax / avg_len if avg_len > 0 and Tmax > 0 else 1.0\n",
    "    c_pow = max(0.1 * avg_inter_event, 0.01)\n",
    "    norm_pow = 0.5\n",
    "    if c_pow > 1e-9 and a1_pow > 1.0:\n",
    "        a0_pow = norm_pow * (a1_pow - 1) * (c_pow ** (a1_pow - 1))\n",
    "    else:\n",
    "        a0_pow = 0.1  # Fallback\n",
    "    if name == \"earthquake\":  # Adjust for very long timescale\n",
    "        a1_pow = 1.05\n",
    "        c_pow = max(0.01 * avg_inter_event, 0.1)\n",
    "        if c_pow > 1e-9 and a1_pow > 1.0:\n",
    "            a0_pow = norm_pow * (a1_pow - 1) * (c_pow ** (a1_pow - 1))\n",
    "        else:\n",
    "            a0_pow = 0.05  # Fallback\n",
    "    kernel_params[name][\"PowerLaw\"] = {\"a_0\": a0_pow, \"a_1\": a1_pow, \"c\": c_pow}\n",
    "\n",
    "# --- Map Kernel Names to Classes ---\n",
    "kernel_classes = {\n",
    "    \"Exp\": HawkesExpKernel,\n",
    "    \"ExpSquared\": HawkesExpSquaredKernel,\n",
    "    \"ExpShifted\": HawkesExpShiftedKernel,\n",
    "    \"ExpSin\": HawkesExpSinKernel,\n",
    "    \"PowerLaw\": HawkesPowerLawKernel,\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# == Simulation Execution\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Simulation Setup ---\n",
    "num_paths = 50  # Number of sequences to generate per dataset/kernel\n",
    "track_intensity = False  # Set to True to track intensity (can be slow)\n",
    "simulation_seed = 42  # For reproducibility\n",
    "\n",
    "# Store generated data: results[dataset_name][kernel_name] = (times, types, intensity_times, intensities)\n",
    "synthetic_data = {}\n",
    "\n",
    "# --- Run Simulations ---\n",
    "print(\"Starting data generation...\")\n",
    "overall_start_time = time.time()\n",
    "\n",
    "for dataset_name, stats in dataset_stats.items():\n",
    "    print(f\"\\n--- Processing Dataset: {dataset_name} ---\")\n",
    "    synthetic_data[dataset_name] = {}\n",
    "    num_marks = stats[\"num_marks\"]\n",
    "    n_events = int(stats[\"avg_len\"])  # Target number of events\n",
    "    # Distribute baseline evenly among marks (adjust if specific per-mark baselines needed)\n",
    "    baselines = np.full(num_marks, stats[\"baseline\"] / num_marks) if num_marks > 0 else np.array([])\n",
    "    baselines = np.maximum(baselines, 1e-9)  # Ensure baseline is positive\n",
    "\n",
    "    for kernel_name, KernelClass in kernel_classes.items():\n",
    "        print(f\"  Kernel: {kernel_name}\")\n",
    "        params = kernel_params[dataset_name][kernel_name]\n",
    "\n",
    "        # Create kernel instance\n",
    "        try:\n",
    "            kernel_instance = KernelClass(**params)\n",
    "        except Exception as e:\n",
    "            print(f\"    Error initializing kernel {kernel_name} for {dataset_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Evaluate kernel on grid\n",
    "        try:\n",
    "            # Use dataset's max_time as a hint for the grid range\n",
    "            t_grid, kernel_vals = kernel_instance(grid_size=grid_size, max_time=stats[\"max_time\"])\n",
    "        except Exception as e:\n",
    "            print(f\"    Error evaluating kernel {kernel_name} for {dataset_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Prepare inputs for simulation (diagonal kernels)\n",
    "        kernel_grids = [t_grid] * num_marks\n",
    "        kernel_evaluations = [kernel_vals] * num_marks\n",
    "\n",
    "        # Run simulation\n",
    "        sim_results = run_hawkes_simulation(\n",
    "            baselines=baselines,\n",
    "            kernel_grids=kernel_grids,\n",
    "            kernel_evaluations=kernel_evaluations,\n",
    "            num_paths=num_paths,\n",
    "            n_events_per_path=n_events,\n",
    "            track_intensity=track_intensity,\n",
    "            seed=simulation_seed,\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        synthetic_data[dataset_name][kernel_name] = sim_results\n",
    "        # Add a small delay if needed, e.g., time.sleep(0.1)\n",
    "\n",
    "overall_end_time = time.time()\n",
    "print(\"\\n--- Simulation Complete ---\")\n",
    "print(f\"Total time: {overall_end_time - overall_start_time:.2f} seconds\")\n",
    "\n",
    "# ==============================================================================\n",
    "# == Example Usage: Accessing Data and Plotting\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Example: Accessing generated data ---\n",
    "try:\n",
    "    amazon_exp_times, amazon_exp_types, _, _ = synthetic_data[\"amazon\"][\"Exp\"]\n",
    "    print(f\"\\nExample: Amazon Exp Kernel - Event times shape: {amazon_exp_times.shape}\")\n",
    "    # Print first 10 events of the first path (excluding NaNs)\n",
    "    first_path_valid_times = amazon_exp_times[0, ~np.isnan(amazon_exp_times[0, :])]\n",
    "    print(f\"First path, first 10 events: {first_path_valid_times[:10]}\")\n",
    "except KeyError:\n",
    "    print(\"\\nCould not retrieve example data for Amazon Exp Kernel.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError accessing example data: {e}\")\n",
    "\n",
    "\n",
    "# --- Optional: Plot a sample kernel and a sample path ---\n",
    "def plot_example(dataset=\"retweet\", kernel=\"Exp\"):\n",
    "    print(f\"\\nPlotting example for: Dataset={dataset}, Kernel={kernel}\")\n",
    "    if dataset not in synthetic_data or kernel not in synthetic_data[dataset]:\n",
    "        print(f\"--> Data for {dataset} with {kernel} kernel not available.\")\n",
    "        return\n",
    "    if dataset not in dataset_stats or kernel not in kernel_params.get(dataset, {}):\n",
    "        print(f\"--> Parameter definition for {dataset}/{kernel} not found.\")\n",
    "        return\n",
    "\n",
    "    plt.style.use(\"seaborn-v0_8-whitegrid\")  # Use a nice style\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Kernel\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    try:\n",
    "        params = kernel_params[dataset][kernel]\n",
    "        KernelClass = kernel_classes[kernel]\n",
    "        kernel_instance = KernelClass(**params)\n",
    "        # Use a finer grid for plotting the kernel shape\n",
    "        t_grid_plot, kernel_vals_plot = kernel_instance(grid_size=200, max_time=dataset_stats[dataset][\"max_time\"])\n",
    "\n",
    "        ax1.plot(t_grid_plot, kernel_vals_plot, label=f\"{kernel} Kernel\", color=\"blue\", linewidth=2)\n",
    "        ax1.set_title(f\"{dataset} - {kernel} Kernel\")\n",
    "        ax1.set_xlabel(\"Time Lag (t)\")\n",
    "        ax1.set_ylabel(\"Kernel Value phi(t)\")\n",
    "        ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "        ax1.legend()\n",
    "        # Set y-limits to be reasonable, avoiding extreme values if kernel explodes\n",
    "        min_val = np.min(kernel_vals_plot[np.isfinite(kernel_vals_plot)])\n",
    "        max_val = np.max(kernel_vals_plot[np.isfinite(kernel_vals_plot)])\n",
    "        padding = (max_val - min_val) * 0.1\n",
    "        ax1.set_ylim(min_val - padding, max_val + padding)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"--> Error plotting kernel: {e}\")\n",
    "        ax1.text(0.5, 0.5, \"Error plotting kernel\", ha=\"center\", va=\"center\")\n",
    "        ax1.set_title(f\"{dataset} - {kernel} Kernel (Error)\")\n",
    "\n",
    "    # Plot Raster for first path\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    try:\n",
    "        event_times, event_types, _, _ = synthetic_data[dataset][kernel]\n",
    "        path_index = 0\n",
    "        times_path = event_times[path_index, :]\n",
    "        types_path = event_types[path_index, :]\n",
    "\n",
    "        # Filter out padding NaNs/-1 types\n",
    "        valid_indices = ~np.isnan(times_path) & (types_path >= 0)\n",
    "        times_path = times_path[valid_indices]\n",
    "        types_path = types_path[valid_indices]\n",
    "\n",
    "        num_events_in_path = len(times_path)\n",
    "        if num_events_in_path > 0:\n",
    "            num_marks_actual = dataset_stats[dataset][\"num_marks\"]\n",
    "            # Define colors for marks\n",
    "            colors = plt.cm.viridis(np.linspace(0, 1, max(num_marks_actual, 2)))  # Need at least 2 colors for cmap\n",
    "\n",
    "            # Create data suitable for eventplot (list of arrays, one per mark)\n",
    "            event_data = []\n",
    "            mark_indices = []\n",
    "            for i in range(num_marks_actual):\n",
    "                mark_times = times_path[types_path == i]\n",
    "                if len(mark_times) > 0:\n",
    "                    event_data.append(mark_times)\n",
    "                    mark_indices.append(i)  # Keep track of which marks have events\n",
    "\n",
    "            if event_data:  # Only plot if there are events\n",
    "                # Use colors corresponding to the marks that actually have events\n",
    "                plot_colors = [colors[i] for i in mark_indices]\n",
    "                ax2.eventplot(event_data, linelengths=0.8, colors=plot_colors, lineoffsets=mark_indices)\n",
    "                ax2.set_yticks(mark_indices)\n",
    "                ax2.set_yticklabels([f\"Mark {i}\" for i in mark_indices])\n",
    "            else:  # Handle case where filtering removed all events (e.g., all were padding)\n",
    "                ax2.text(0.5, 0.5, \"No valid events in sample path\", ha=\"center\", va=\"center\")\n",
    "\n",
    "            ax2.set_title(f\"Sample Path {path_index} ({num_events_in_path} events)\")\n",
    "            ax2.set_xlabel(\"Time\")\n",
    "            ax2.set_ylabel(\"Event Type (Mark)\")\n",
    "            ax2.grid(True, linestyle=\"--\", alpha=0.7, axis=\"x\")  # Grid only on x-axis for clarity\n",
    "            # Set x-limit based on the last event time\n",
    "            if num_events_in_path > 0:\n",
    "                ax2.set_xlim(0, times_path[-1] * 1.05)  # Add 5% padding\n",
    "\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, \"No events in sample path\", ha=\"center\", va=\"center\")\n",
    "            ax2.set_title(f\"Sample Path {path_index} (0 events)\")\n",
    "            ax2.set_yticks([])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"--> Error plotting raster: {e}\")\n",
    "        ax2.text(0.5, 0.5, \"Error plotting raster\", ha=\"center\", va=\"center\")\n",
    "        ax2.set_title(f\"Sample Path {path_index} (Error)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Plot Examples ---\n",
    "# You can uncomment these lines one by one or change the parameters\n",
    "# to see different results after the simulation runs.\n",
    "# plot_example(dataset='retweet', kernel='Exp')\n",
    "# plot_example(dataset='earthquake', kernel='PowerLaw')\n",
    "# plot_example(dataset='taxi', kernel='Exp')\n",
    "# plot_example(dataset='volcano', kernel='ExpShifted')\n",
    "# plot_example(dataset='amazon', kernel='ExpSquared')\n",
    "\n",
    "# Example: Plotting after the script finishes (assuming execution in interactive environment)\n",
    "if __name__ == \"__main__\":\n",
    "    # Plot a few examples automatically after running the script\n",
    "    plot_example(dataset=\"retweet\", kernel=\"Exp\")\n",
    "    plot_example(dataset=\"earthquake\", kernel=\"PowerLaw\")\n",
    "    plot_example(dataset=\"taxi\", kernel=\"ExpSquared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
