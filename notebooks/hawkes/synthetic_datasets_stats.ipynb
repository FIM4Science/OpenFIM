{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib to avoid Type 1 fonts - MUST BE RUN FIRST\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Set global font configuration to avoid Type 1 fonts\n",
    "matplotlib.rcParams.update(\n",
    "    {\n",
    "        \"font.family\": \"sans-serif\",\n",
    "        \"font.sans-serif\": [\"Arial\", \"DejaVu Sans\", \"Liberation Sans\", \"Bitstream Vera Sans\", \"sans-serif\"],\n",
    "        \"pdf.fonttype\": 42,  # Use TrueType fonts instead of Type 1\n",
    "        \"ps.fonttype\": 42,  # Use TrueType fonts instead of Type 1\n",
    "        \"axes.unicode_minus\": False,  # Avoid issues with minus signs\n",
    "        \"figure.dpi\": 300,  # High DPI for better quality\n",
    "        \"savefig.dpi\": 300,\n",
    "        \"savefig.bbox\": \"tight\",\n",
    "        \"savefig.pad_inches\": 0.1,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"âœ… Font configuration updated to avoid Type 1 fonts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from fim.data.dataloaders import DataLoaderFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for different dimensions\n",
    "configs_by_dimension = {\n",
    "    \"1D\": {\n",
    "        \"name\": \"HawkesDataLoader\",\n",
    "        \"path\": {\n",
    "            \"train\": (\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_1D_2k_paths_const_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_1D_2k_paths_const_base_exp_kernel_no_interactions/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_1D_2k_paths_Gamma_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_1D_2k_paths_const_base_rayleigh_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_1D_2k_paths_const_base_rayleigh_kernel_sparse/train\",\n",
    "            ),\n",
    "        },\n",
    "        \"loader_kwargs\": {\n",
    "            \"batch_size\": 1,\n",
    "            \"num_workers\": 0,\n",
    "            \"test_batch_size\": 1,\n",
    "            \"variable_num_of_paths\": True,\n",
    "            \"min_path_count\": 2,\n",
    "            \"max_path_count\": 2,\n",
    "            \"max_number_of_minibatch_sizes\": 1,\n",
    "            \"variable_sequence_lens\": {\n",
    "                \"train\": True,\n",
    "                \"validation\": False,\n",
    "            },\n",
    "            \"min_sequence_len\": 15,\n",
    "            \"max_sequence_len\": 100,\n",
    "            \"full_len_ratio\": 0.1,\n",
    "            \"num_inference_paths\": 1,\n",
    "            \"num_inference_times\": 2000,\n",
    "        },\n",
    "        \"dataset_kwargs\": {\n",
    "            \"files_to_load\": {\n",
    "                \"base_intensity_functions\": \"base_intensity_functions.pt\",\n",
    "                \"event_times\": \"event_times.pt\",\n",
    "                \"event_types\": \"event_types.pt\",\n",
    "                \"kernel_functions\": \"kernel_functions.pt\",\n",
    "            },\n",
    "            \"field_name_for_dimension_grouping\": [\"base_intensity_functions\", \"kernel_functions\"],\n",
    "        },\n",
    "    },\n",
    "    \"5D\": {\n",
    "        \"name\": \"HawkesDataLoader\",\n",
    "        \"path\": {\n",
    "            \"train\": (\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_5D_2k_paths_const_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_5D_2k_paths_const_base_exp_kernel_no_interactions/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_5D_2k_paths_Gamma_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_5D_2k_paths_const_base_rayleigh_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_5D_2k_paths_const_base_rayleigh_kernel_sparse/train\",\n",
    "            ),\n",
    "        },\n",
    "        \"loader_kwargs\": {\n",
    "            \"batch_size\": 1,\n",
    "            \"num_workers\": 0,\n",
    "            \"test_batch_size\": 1,\n",
    "            \"variable_num_of_paths\": True,\n",
    "            \"min_path_count\": 2,\n",
    "            \"max_path_count\": 2,\n",
    "            \"max_number_of_minibatch_sizes\": 1,\n",
    "            \"variable_sequence_lens\": {\n",
    "                \"train\": True,\n",
    "                \"validation\": False,\n",
    "            },\n",
    "            \"min_sequence_len\": 15,\n",
    "            \"max_sequence_len\": 100,\n",
    "            \"full_len_ratio\": 0.1,\n",
    "            \"num_inference_paths\": 1,\n",
    "            \"num_inference_times\": 2000,\n",
    "        },\n",
    "        \"dataset_kwargs\": {\n",
    "            \"files_to_load\": {\n",
    "                \"base_intensity_functions\": \"base_intensity_functions.pt\",\n",
    "                \"event_times\": \"event_times.pt\",\n",
    "                \"event_types\": \"event_types.pt\",\n",
    "                \"kernel_functions\": \"kernel_functions.pt\",\n",
    "            },\n",
    "            \"field_name_for_dimension_grouping\": [\"base_intensity_functions\", \"kernel_functions\"],\n",
    "        },\n",
    "    },\n",
    "    \"10D\": {\n",
    "        \"name\": \"HawkesDataLoader\",\n",
    "        \"path\": {\n",
    "            \"train\": (\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_10D_2k_paths_const_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_10D_2k_paths_const_base_exp_kernel_no_interactions/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_10D_2k_paths_poisson/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_10D_2k_paths_Gamma_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_10D_2k_paths_sin_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_10D_2k_paths_const_base_exp_kernel_sparse/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_10D_2k_paths_Gamma_base_exp_kernel_sparse/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_10D_2k_paths_sin_base_exp_kernel_sparse/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_10D_2k_paths_const_base_rayleigh_kernel_sparse/train\",\n",
    "            ),\n",
    "        },\n",
    "        \"loader_kwargs\": {\n",
    "            \"batch_size\": 1,\n",
    "            \"num_workers\": 0,\n",
    "            \"test_batch_size\": 1,\n",
    "            \"variable_num_of_paths\": True,\n",
    "            \"min_path_count\": 2,\n",
    "            \"max_path_count\": 2,\n",
    "            \"max_number_of_minibatch_sizes\": 1,\n",
    "            \"variable_sequence_lens\": {\n",
    "                \"train\": True,\n",
    "                \"validation\": False,\n",
    "            },\n",
    "            \"min_sequence_len\": 15,\n",
    "            \"max_sequence_len\": 100,\n",
    "            \"full_len_ratio\": 0.1,\n",
    "            \"num_inference_paths\": 1,\n",
    "            \"num_inference_times\": 2000,\n",
    "        },\n",
    "        \"dataset_kwargs\": {\n",
    "            \"files_to_load\": {\n",
    "                \"base_intensity_functions\": \"base_intensity_functions.pt\",\n",
    "                \"event_times\": \"event_times.pt\",\n",
    "                \"event_types\": \"event_types.pt\",\n",
    "                \"kernel_functions\": \"kernel_functions.pt\",\n",
    "            },\n",
    "            \"field_name_for_dimension_grouping\": [\"base_intensity_functions\", \"kernel_functions\"],\n",
    "        },\n",
    "    },\n",
    "    \"15D\": {\n",
    "        \"name\": \"HawkesDataLoader\",\n",
    "        \"path\": {\n",
    "            \"train\": (\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_15D_2k_paths_const_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_15D_2k_paths_const_base_exp_kernel_no_interactions/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_15D_2k_paths_poisson/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_15D_2k_paths_Gamma_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_15D_2k_paths_sin_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_15D_2k_paths_const_base_exp_kernel_sparse/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_15D_2k_paths_Gamma_base_exp_kernel_sparse/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_15D_2k_paths_sin_base_exp_kernel_sparse/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/1k_15D_2k_paths_const_base_rayleigh_kernel/train\",\n",
    "            ),\n",
    "        },\n",
    "        \"loader_kwargs\": {\n",
    "            \"batch_size\": 1,\n",
    "            \"num_workers\": 0,\n",
    "            \"test_batch_size\": 1,\n",
    "            \"variable_num_of_paths\": True,\n",
    "            \"min_path_count\": 2,\n",
    "            \"max_path_count\": 2,\n",
    "            \"max_number_of_minibatch_sizes\": 1,\n",
    "            \"variable_sequence_lens\": {\n",
    "                \"train\": True,\n",
    "                \"validation\": False,\n",
    "            },\n",
    "            \"min_sequence_len\": 15,\n",
    "            \"max_sequence_len\": 100,\n",
    "            \"full_len_ratio\": 0.1,\n",
    "            \"num_inference_paths\": 1,\n",
    "            \"num_inference_times\": 2000,\n",
    "        },\n",
    "        \"dataset_kwargs\": {\n",
    "            \"files_to_load\": {\n",
    "                \"base_intensity_functions\": \"base_intensity_functions.pt\",\n",
    "                \"event_times\": \"event_times.pt\",\n",
    "                \"event_types\": \"event_types.pt\",\n",
    "                \"kernel_functions\": \"kernel_functions.pt\",\n",
    "            },\n",
    "            \"field_name_for_dimension_grouping\": [\"base_intensity_functions\", \"kernel_functions\"],\n",
    "        },\n",
    "    },\n",
    "    \"22D\": {\n",
    "        \"name\": \"HawkesDataLoader\",\n",
    "        \"path\": {\n",
    "            \"train\": (\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/5k_22D_2k_paths_const_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/5k_22D_2k_paths_const_base_exp_kernel_no_interactions/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/5k_22D_2k_paths_poisson/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/5k_22D_2k_paths_Gamma_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/5k_22D_2k_paths_sin_base_exp_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/5k_22D_2k_paths_const_base_exp_kernel_sparse/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/5k_22D_2k_paths_const_base_rayleigh_kernel/train\",\n",
    "                \"/cephfs_projects/foundation_models/data/PP/train/5k_22D_2k_paths_const_base_rayleigh_kernel_sparse/train\",\n",
    "            ),\n",
    "        },\n",
    "        \"loader_kwargs\": {\n",
    "            \"batch_size\": 1,\n",
    "            \"num_workers\": 0,\n",
    "            \"test_batch_size\": 1,\n",
    "            \"variable_num_of_paths\": True,\n",
    "            \"min_path_count\": 2,\n",
    "            \"max_path_count\": 2,\n",
    "            \"max_number_of_minibatch_sizes\": 1,\n",
    "            \"variable_sequence_lens\": {\n",
    "                \"train\": True,\n",
    "                \"validation\": False,\n",
    "            },\n",
    "            \"min_sequence_len\": 15,\n",
    "            \"max_sequence_len\": 100,\n",
    "            \"full_len_ratio\": 0.1,\n",
    "            \"num_inference_paths\": 1,\n",
    "            \"num_inference_times\": 2000,\n",
    "        },\n",
    "        \"dataset_kwargs\": {\n",
    "            \"files_to_load\": {\n",
    "                \"base_intensity_functions\": \"base_intensity_functions.pt\",\n",
    "                \"event_times\": \"event_times.pt\",\n",
    "                \"event_types\": \"event_types.pt\",\n",
    "                \"kernel_functions\": \"kernel_functions.pt\",\n",
    "            },\n",
    "            \"field_name_for_dimension_grouping\": [\"base_intensity_functions\", \"kernel_functions\"],\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def normalize_obs_grid(obs_grid: Tensor, seq_lengths: Tensor) -> tuple[Tensor, Tensor]:\n",
    "    batch_indices = torch.arange(obs_grid.size(0), device=obs_grid.device).view(-1, 1).expand(-1, obs_grid.size(1))\n",
    "    path_indices = torch.arange(obs_grid.size(1), device=obs_grid.device).view(1, -1).expand(obs_grid.size(0), -1)\n",
    "\n",
    "    # Clamp seq_lengths to not exceed the actual sequence dimension size\n",
    "    max_seq_len = obs_grid.size(2)\n",
    "    clamped_seq_lengths = torch.clamp(seq_lengths, max=max_seq_len)\n",
    "\n",
    "    max_times = obs_grid[batch_indices, path_indices, clamped_seq_lengths - 1]\n",
    "    norm_constants = max_times.amax(dim=[1, 2])\n",
    "    obs_grid_normalized = obs_grid / norm_constants.view(-1, 1, 1, 1)\n",
    "    return obs_grid_normalized\n",
    "\n",
    "\n",
    "def convert_to_easytpp_format(sample):\n",
    "    # Extract tensors from the dictionary\n",
    "    event_times = sample[\"event_times\"][0]  # [P, L, 1]\n",
    "    delta_times = sample[\"delta_times\"][0]  # [P, L, 1]\n",
    "    event_types = sample[\"event_types\"][0]  # [P, L, 1]\n",
    "    seq_lengths = sample[\"seq_lengths\"][0]  # [P]\n",
    "    dim_process = sample[\"dim_process\"]\n",
    "\n",
    "    # Get dimensions\n",
    "    num_sequences = event_times.shape[0]\n",
    "\n",
    "    # Initialize list to store the converted data\n",
    "    easytpp_data = []\n",
    "\n",
    "    # Process each sequence\n",
    "    for i in range(num_sequences):\n",
    "        seq_len = seq_lengths[i].item()\n",
    "\n",
    "        # Extract valid events for this sequence\n",
    "        times = event_times[i, :seq_len, 0].cpu().numpy().tolist()\n",
    "        deltas = delta_times[i, :seq_len, 0].cpu().numpy().tolist()\n",
    "        types = event_types[i, :seq_len, 0].cpu().numpy().tolist()\n",
    "\n",
    "        # Create a single sequence entry\n",
    "        sequence_entry = {\n",
    "            \"time_since_start\": times,\n",
    "            \"time_since_last_event\": deltas,\n",
    "            \"type_event\": types,\n",
    "            \"seq_idx\": i,\n",
    "            \"seq_len\": seq_len,\n",
    "            \"dim_process\": dim_process,\n",
    "        }\n",
    "\n",
    "        easytpp_data.append(sequence_entry)\n",
    "\n",
    "    # Create a Hugging Face Dataset\n",
    "    # dataset = Dataset.from_list(easytpp_data)\n",
    "    return easytpp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(dataset, split=\"test\"):\n",
    "    dataset = dataset[split]\n",
    "    stats = {\n",
    "        \"num_sequences\": len(dataset),\n",
    "        \"max_sequence_length\": 0,\n",
    "        \"min_sequence_length\": 1000000,\n",
    "        \"max_event_time\": 0,\n",
    "        \"min_event_time\": 1000000,\n",
    "    }\n",
    "    seq_lengths = [len(seq[\"time_since_last_event\"]) for seq in dataset]\n",
    "    times_since_last_event = [time for seq in dataset for time in seq[\"time_since_last_event\"]]\n",
    "    min_delta_event_time = min(times_since_last_event)\n",
    "    max_delta_event_time = max(times_since_last_event)\n",
    "    min_seq_length = min(seq_lengths)\n",
    "    max_seq_length = max(seq_lengths)\n",
    "    avg_seq_length = sum(seq_lengths) / len(seq_lengths)\n",
    "    avg_delta_event_time = sum(times_since_last_event) / len(times_since_last_event)\n",
    "    stats[\"max_sequence_length\"] = max_seq_length\n",
    "    stats[\"min_sequence_length\"] = min_seq_length\n",
    "    stats[\"num_sequences\"] = len(seq_lengths)\n",
    "    stats[\"avg_sequence_length\"] = avg_seq_length\n",
    "    stats[\"max_event_time\"] = max_delta_event_time\n",
    "    stats[\"min_event_time\"] = min_delta_event_time\n",
    "    stats[\"avg_event_time\"] = avg_delta_event_time\n",
    "    encoutered_marks = set()\n",
    "    for seq in dataset:\n",
    "        for mark in seq[\"type_event\"]:\n",
    "            if mark not in encoutered_marks:\n",
    "                encoutered_marks.add(mark)\n",
    "    stats[\"num_marks\"] = len(encoutered_marks)\n",
    "    pprint(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Configure seaborn to work with our font settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\", font_scale=0.8)\n",
    "\n",
    "\n",
    "# --- Helper Function ---\n",
    "\n",
    "\n",
    "def plot_stats(dataset, splits=[\"train\", \"validation\", \"test\"]):\n",
    "    # Set style for better-looking plots\n",
    "    plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Extract the data for each split\n",
    "    split_data = {split: dataset[split] for split in splits if split in dataset}\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(len(split_data), 2, figsize=(15, 6 * len(split_data)))\n",
    "    if len(split_data) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, split in enumerate(split_data.keys()):\n",
    "        data = split_data[split]\n",
    "        seq_len = [len(seq[\"time_since_last_event\"]) for seq in data]\n",
    "        time_since_last_event = [time for seq in data for time in seq[\"time_since_last_event\"]]\n",
    "\n",
    "        # Histogram of the seq_len\n",
    "        axes[i, 0].hist(seq_len, bins=30, edgecolor=\"black\", alpha=0.7, color=\"steelblue\")\n",
    "        axes[i, 0].set_title(f\"Sequence Length Distribution ({split})\", fontsize=14, fontweight=\"bold\")\n",
    "        axes[i, 0].set_xlabel(\"Sequence Length\", fontsize=12)\n",
    "        axes[i, 0].set_ylabel(\"Frequency\", fontsize=12)\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Histogram of the time_since_last_event\n",
    "        axes[i, 1].hist(time_since_last_event, bins=50, edgecolor=\"black\", alpha=0.7, color=\"darkseagreen\")\n",
    "        axes[i, 1].set_title(f\"Time Since Last Event Distribution ({split})\", fontsize=14, fontweight=\"bold\")\n",
    "        axes[i, 1].set_xlabel(\"Time Since Last Event\", fontsize=12)\n",
    "        axes[i, 1].set_ylabel(\"Frequency\", fontsize=12)\n",
    "        axes[i, 1].set_yscale(\"log\")\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_max_time(dataset, split=\"train\"):\n",
    "    \"\"\"\n",
    "    Calculates the maximum event time ('time_since_start') across all trajectories\n",
    "    in a split of a Hugging Face Dataset object.\n",
    "    \"\"\"\n",
    "    max_t = 0\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"Warning: Split '{split}' not found in dataset.\")\n",
    "        return 1.0  # Default max time\n",
    "\n",
    "    # Check if 'time_since_start' column exists\n",
    "    if \"time_since_start\" not in data_split.column_names:\n",
    "        print(f\"Error: 'time_since_start' column not found in dataset split '{split}'. Cannot determine max time.\")\n",
    "        return 1.0  # Default max time\n",
    "\n",
    "    for trajectory in data_split:\n",
    "        times = trajectory.get(\"time_since_start\")\n",
    "        # Ensure times is a non-empty list or array\n",
    "        if times and hasattr(times, \"__len__\") and len(times) > 0:\n",
    "            # Check for potential nested lists or other issues if max fails\n",
    "            try:\n",
    "                current_max = np.max(times)\n",
    "                if isinstance(current_max, (int, float)):  # Ensure it's a number\n",
    "                    max_t = max(max_t, current_max)\n",
    "            except (TypeError, ValueError) as e:\n",
    "                print(f\"Warning: Could not compute max time for a trajectory: {e}. Skipping.\")\n",
    "                print(f\"Problematic 'time_since_start' value: {times}\")\n",
    "\n",
    "    return max_t if max_t > 0 else 1.0  # Avoid max_time being 0\n",
    "\n",
    "\n",
    "# --- Professional color palette (colorblind-friendly) ---\n",
    "PROFESSIONAL_COLORS = [\n",
    "    \"#2E86AB\",  # Ocean Blue\n",
    "    \"#A23B72\",  # Burgundy\n",
    "    \"#F18F01\",  # Amber\n",
    "    \"#C73E1D\",  # Red\n",
    "    \"#6A994E\",  # Forest Green\n",
    "    \"#7209B7\",  # Purple\n",
    "    \"#FFB500\",  # Golden Yellow\n",
    "    \"#F72585\",  # Hot Pink\n",
    "    \"#4CC9F0\",  # Sky Blue\n",
    "    \"#8ECAE6\",  # Light Blue\n",
    "    \"#FFB3BA\",  # Light Pink\n",
    "    \"#BAFFC9\",  # Light Green\n",
    "    \"#BAE1FF\",  # Pale Blue\n",
    "    \"#FFFFBA\",  # Light Yellow\n",
    "    \"#E6E6FA\",  # Lavender\n",
    "]\n",
    "\n",
    "\n",
    "def get_professional_palette(n_colors):\n",
    "    \"\"\"Get a professional, colorblind-friendly palette with n_colors.\"\"\"\n",
    "    if n_colors <= len(PROFESSIONAL_COLORS):\n",
    "        return PROFESSIONAL_COLORS[:n_colors]\n",
    "    else:\n",
    "        # Repeat colors if we need more than available\n",
    "        return (PROFESSIONAL_COLORS * ((n_colors // len(PROFESSIONAL_COLORS)) + 1))[:n_colors]\n",
    "\n",
    "\n",
    "# --- Visualization Functions ---\n",
    "\n",
    "\n",
    "def plot_aggregate_rate(dataset, split=\"train\", n_bins=50, max_time=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plots the aggregate event rate over time using a Hugging Face Dataset.\n",
    "    Assumes 'time_since_start' contains absolute event times.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): Dictionary containing Hugging Face Dataset objects per split.\n",
    "        split (str): The dataset split to use (e.g., 'train').\n",
    "        n_bins (int): Number of time bins.\n",
    "        max_time (float, optional): Maximum time for the plot range. If None, calculated from data.\n",
    "        ax (matplotlib.axes.Axes, optional): Matplotlib axes to plot on. If None, creates a new figure.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig_created = True\n",
    "        plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    else:\n",
    "        fig_created = False\n",
    "\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"No data found for split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    num_trajectories = len(data_split)\n",
    "    if num_trajectories == 0:\n",
    "        print(f\"No trajectories in split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    if \"time_since_start\" not in data_split.column_names:\n",
    "        print(f\"Error: 'time_since_start' column not found in dataset split '{split}'. Cannot plot aggregate rate.\")\n",
    "        return\n",
    "\n",
    "    if max_time is None:\n",
    "        max_time = get_max_time(dataset, split)\n",
    "\n",
    "    # Define time bins\n",
    "    bins = np.linspace(0, max_time, n_bins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_width = bins[1] - bins[0]\n",
    "    if bin_width == 0:  # Avoid division by zero if max_time is 0 or n_bins is 1\n",
    "        print(\"Warning: Cannot calculate rate, time bin width is zero.\")\n",
    "        return\n",
    "\n",
    "    # Count events in each bin\n",
    "    counts = np.zeros(n_bins)\n",
    "\n",
    "    for trajectory in data_split:\n",
    "        times = trajectory.get(\"time_since_start\", [])\n",
    "        # Ensure times is a list/array of numbers\n",
    "        if times and isinstance(times, (list, np.ndarray)):\n",
    "            try:\n",
    "                # Filter out non-numeric types if necessary, though HF datasets usually handle this\n",
    "                numeric_times = [t for t in times if isinstance(t, (int, float))]\n",
    "                if numeric_times:\n",
    "                    hist, _ = np.histogram(numeric_times, bins=bins)\n",
    "                    counts += hist\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process times for a trajectory: {e}\")\n",
    "                print(f\"Problematic 'time_since_start': {times}\")\n",
    "\n",
    "    # Calculate rate (average events per trajectory per unit time)\n",
    "    rate = counts / (num_trajectories * bin_width)\n",
    "\n",
    "    # Plotting with professional styling\n",
    "    ax.plot(\n",
    "        bin_centers,\n",
    "        rate,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        color=PROFESSIONAL_COLORS[0],\n",
    "        linewidth=2,\n",
    "        markersize=4,\n",
    "        markerfacecolor=\"white\",\n",
    "        markeredgewidth=1.5,\n",
    "        markeredgecolor=PROFESSIONAL_COLORS[0],\n",
    "    )\n",
    "    ax.set_title(f\"Aggregate Event Rate ({split} split)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Time (from time_since_start)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Avg. Events / Trajectory / Time Unit\", fontsize=10)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    ax.set_xlim(0, max_time)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    if fig_created:  # Only show if we created the figure\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_rate_per_mark(dataset, split=\"train\", n_bins=50, max_time=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plots the event rate per mark over time using a Hugging Face Dataset.\n",
    "    Assumes 'time_since_start' contains absolute event times and 'type_event' contains marks.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): Dictionary containing Hugging Face Dataset objects per split.\n",
    "        split (str): The dataset split to use.\n",
    "        n_bins (int): Number of time bins.\n",
    "        max_time (float, optional): Maximum time for the plot range. If None, calculated from data.\n",
    "        ax (matplotlib.axes.Axes, optional): Matplotlib axes to plot on. If None, creates a new figure.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig_created = True\n",
    "        plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    else:\n",
    "        fig_created = False\n",
    "\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"No data found for split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    num_trajectories = len(data_split)\n",
    "    if num_trajectories == 0:\n",
    "        print(f\"No trajectories in split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [\"dim_process\", \"time_since_start\", \"type_event\"]\n",
    "    if not all(col in data_split.column_names for col in required_cols):\n",
    "        print(f\"Error: Dataset split '{split}' missing one or more required columns: {required_cols}\")\n",
    "        return\n",
    "\n",
    "    # Infer number of marks (dim_process) - Assuming it's constant\n",
    "    # Take it from the first row, ensure it's an integer\n",
    "    try:\n",
    "        dim_process = int(data_split[0][\"dim_process\"])\n",
    "    except (ValueError, TypeError):\n",
    "        print(\"Error: 'dim_process' feature is not a valid integer in the first row.\")\n",
    "        return\n",
    "\n",
    "    if max_time is None:\n",
    "        max_time = get_max_time(dataset, split)\n",
    "\n",
    "    # Define time bins\n",
    "    bins = np.linspace(0, max_time, n_bins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_width = bins[1] - bins[0]\n",
    "    if bin_width == 0:\n",
    "        print(\"Warning: Cannot calculate rate, time bin width is zero.\")\n",
    "        return\n",
    "\n",
    "    # Count events per mark in each bin\n",
    "    counts_per_mark = np.zeros((dim_process, n_bins))\n",
    "\n",
    "    for trajectory in data_split:\n",
    "        times = np.array(trajectory.get(\"time_since_start\", []))\n",
    "        types = np.array(trajectory.get(\"type_event\", []))\n",
    "\n",
    "        if len(times) != len(types) or len(times) == 0:\n",
    "            continue  # Skip if data is inconsistent or empty\n",
    "\n",
    "        # Ensure types are integers\n",
    "        try:\n",
    "            types = types.astype(int)\n",
    "        except (ValueError, TypeError):\n",
    "            print(\"Warning: Could not convert 'type_event' to integers for a trajectory. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for mark in range(dim_process):\n",
    "            try:\n",
    "                # Filter times corresponding to the current mark\n",
    "                mark_times = times[types == mark]\n",
    "                # Ensure mark_times contains numeric data before histogramming\n",
    "                numeric_mark_times = [t for t in mark_times if isinstance(t, (int, float))]\n",
    "                if numeric_mark_times:\n",
    "                    hist, _ = np.histogram(numeric_mark_times, bins=bins)\n",
    "                    counts_per_mark[mark, :] += hist\n",
    "            except IndexError:\n",
    "                print(f\"Warning: Index error processing mark {mark}. Check data consistency (e.g., mark values vs dim_process).\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not process mark {mark} for a trajectory: {e}\")\n",
    "\n",
    "    # Calculate rates\n",
    "    rates_per_mark = counts_per_mark / (num_trajectories * bin_width)\n",
    "\n",
    "    # Plotting with professional styling\n",
    "    colors = get_professional_palette(dim_process)\n",
    "    for mark in range(dim_process):\n",
    "        ax.plot(\n",
    "            bin_centers,\n",
    "            rates_per_mark[mark, :],\n",
    "            marker=\"o\",\n",
    "            linestyle=\"-\",\n",
    "            label=f\"Mark {mark}\",\n",
    "            color=colors[mark],\n",
    "            linewidth=2,\n",
    "            markersize=3,\n",
    "            markerfacecolor=\"white\",\n",
    "            markeredgewidth=1,\n",
    "            markeredgecolor=colors[mark],\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"Event Rate per Mark ({split} split)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Time (from time_since_start)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Avg. Events / Trajectory / Time Unit\", fontsize=10)\n",
    "    ax.legend(fontsize=8, frameon=True, fancybox=True, shadow=True, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    ax.set_xlim(0, max_time)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    if fig_created:  # Only show if we created the figure\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_raster_sample(dataset, split=\"train\", n_samples=8, max_time=None, ax=None, seed=None):\n",
    "    \"\"\"\n",
    "    Creates a raster plot for a sample of trajectories from a Hugging Face Dataset.\n",
    "    Assumes 'time_since_start' and 'type_event' columns exist.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): Dictionary containing Hugging Face Dataset objects per split.\n",
    "        split (str): The dataset split to use.\n",
    "        n_samples (int): Number of trajectories to sample and plot.\n",
    "        max_time (float, optional): Maximum time for the plot range. If None, calculated from data.\n",
    "        ax (matplotlib.axes.Axes, optional): Matplotlib axes to plot on. If None, creates a new figure.\n",
    "        seed (int, optional): Random seed for reproducibility of sampling.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig_created = True\n",
    "        plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    else:\n",
    "        fig_created = False\n",
    "\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"No data found for split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    num_trajectories = len(data_split)\n",
    "    if num_trajectories == 0:\n",
    "        print(f\"No trajectories in split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [\"dim_process\", \"time_since_start\", \"type_event\"]\n",
    "    if not all(col in data_split.column_names for col in required_cols):\n",
    "        print(f\"Error: Dataset split '{split}' missing one or more required columns: {required_cols}\")\n",
    "        return\n",
    "\n",
    "    if num_trajectories < n_samples:\n",
    "        print(f\"Warning: Requested {n_samples} samples, but only {num_trajectories} trajectories available.\")\n",
    "        n_samples = num_trajectories\n",
    "\n",
    "    if n_samples == 0:\n",
    "        return\n",
    "\n",
    "    # Sample indices\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    sampled_indices = random.sample(range(num_trajectories), n_samples)\n",
    "\n",
    "    # Select the sampled trajectories (more efficient than iterating and filtering)\n",
    "    # Note: This creates a new dataset view, doesn't load all into memory at once\n",
    "    sampled_data = data_split.select(sampled_indices)\n",
    "\n",
    "    # Infer number of marks from the first sampled trajectory\n",
    "    try:\n",
    "        dim_process = int(sampled_data[0][\"dim_process\"])\n",
    "    except (ValueError, TypeError):\n",
    "        print(\"Error: 'dim_process' feature is not a valid integer in the first sampled row.\")\n",
    "        return\n",
    "\n",
    "    if max_time is None:\n",
    "        # Calculate max_time only from the sampled trajectories for efficiency\n",
    "        max_t_sample = 0\n",
    "        for traj in sampled_data:\n",
    "            times = traj.get(\"time_since_start\")\n",
    "            if times and hasattr(times, \"__len__\") and len(times) > 0:\n",
    "                try:\n",
    "                    current_max = np.max(times)\n",
    "                    if isinstance(current_max, (int, float)):\n",
    "                        max_t_sample = max(max_t_sample, current_max)\n",
    "                except (TypeError, ValueError):\n",
    "                    pass  # Ignore errors here, focus is on getting a reasonable max\n",
    "        max_time = max_t_sample if max_t_sample > 0 else 1.0\n",
    "\n",
    "    colors = get_professional_palette(dim_process)\n",
    "\n",
    "    # Plot events using scatter with enhanced styling\n",
    "    event_times = []\n",
    "    y_positions = []\n",
    "    event_colors = []\n",
    "    for i, trajectory in enumerate(sampled_data):  # Iterate over the selected sample\n",
    "        times = trajectory.get(\"time_since_start\", [])\n",
    "        types = trajectory.get(\"type_event\", [])\n",
    "\n",
    "        if (\n",
    "            not isinstance(times, (list, np.ndarray))\n",
    "            or not isinstance(types, (list, np.ndarray))\n",
    "            or len(times) != len(types)\n",
    "            or len(times) == 0\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Ensure types are integers for indexing colors\n",
    "            types = np.array(types).astype(int)\n",
    "            times = np.array(times)  # Ensure times are numpy array for potential filtering\n",
    "\n",
    "            # Filter out non-numeric times just in case\n",
    "            valid_indices = [idx for idx, t in enumerate(times) if isinstance(t, (int, float))]\n",
    "            times = times[valid_indices]\n",
    "            types = types[valid_indices]\n",
    "\n",
    "            for t, type_ in zip(times, types):\n",
    "                if 0 <= type_ < dim_process:  # Check if type is valid\n",
    "                    event_times.append(t)\n",
    "                    y_positions.append(i)  # Use sample index for y-position\n",
    "                    event_colors.append(colors[type_])\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid event type {type_} encountered in sampled trajectory {i}. Skipping event.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing sampled trajectory {i}: {e}\")\n",
    "\n",
    "    # Using scatter for better color control per event\n",
    "    if event_times:  # Only plot if there's data\n",
    "        ax.scatter(event_times, y_positions, c=event_colors, marker=\"|\", s=80, linewidths=2)\n",
    "    else:\n",
    "        print(\"No valid events found in the sample to plot.\")\n",
    "\n",
    "    ax.set_title(f\"Raster Plot ({n_samples} Sampled Trajectories from {split} split)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Time (from time_since_start)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Sampled Trajectory Index\", fontsize=10)\n",
    "    ax.set_yticks(range(n_samples))\n",
    "    ax.set_ylim(-0.5, n_samples - 0.5)\n",
    "    ax.set_xlim(0, max_time)\n",
    "    ax.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.4)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # Add a legend for marks with professional colors\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker=\"|\", color=colors[mark], linestyle=\"None\", markersize=10, label=f\"Mark {mark}\", markeredgewidth=2)\n",
    "        for mark in range(dim_process)\n",
    "    ]\n",
    "    ax.legend(\n",
    "        handles=legend_elements,\n",
    "        title=\"Event Marks\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc=\"upper left\",\n",
    "        fontsize=8,\n",
    "        title_fontsize=10,\n",
    "        frameon=True,\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "    )\n",
    "\n",
    "    if fig_created:  # Only show if we created the figure\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_event_count_heatmap(dataset, split=\"train\", n_bins=30, max_time=None, normalize=True, ax=None):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of event counts per mark over time using a Hugging Face Dataset.\n",
    "    Assumes 'time_since_start', 'type_event', 'dim_process' columns exist.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): Dictionary containing Hugging Face Dataset objects per split.\n",
    "        split (str): The dataset split to use.\n",
    "        n_bins (int): Number of time bins.\n",
    "        max_time (float, optional): Maximum time for the plot range. If None, calculated from data.\n",
    "        normalize (bool): If True, normalize counts by the number of trajectories.\n",
    "        ax (matplotlib.axes.Axes, optional): Matplotlib axes to plot on. If None, creates a new figure.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig_created = True\n",
    "        plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    else:\n",
    "        fig_created = False\n",
    "\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"No data found for split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    num_trajectories = len(data_split)\n",
    "    if num_trajectories == 0:\n",
    "        print(f\"No trajectories in split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [\"dim_process\", \"time_since_start\", \"type_event\"]\n",
    "    if not all(col in data_split.column_names for col in required_cols):\n",
    "        print(f\"Error: Dataset split '{split}' missing one or more required columns: {required_cols}\")\n",
    "        return\n",
    "\n",
    "    # Infer number of marks\n",
    "    try:\n",
    "        dim_process = int(data_split[0][\"dim_process\"])\n",
    "    except (ValueError, TypeError):\n",
    "        print(\"Error: 'dim_process' feature is not a valid integer in the first row.\")\n",
    "        return\n",
    "\n",
    "    if max_time is None:\n",
    "        max_time = get_max_time(dataset, split)\n",
    "\n",
    "    # Define time bins\n",
    "    bins = np.linspace(0, max_time, n_bins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2  # For labeling ticks\n",
    "\n",
    "    # Initialize count matrix (marks x time_bins)\n",
    "    heatmap_counts = np.zeros((dim_process, n_bins))\n",
    "\n",
    "    # Populate the heatmap counts\n",
    "    for trajectory in data_split:\n",
    "        times = np.array(trajectory.get(\"time_since_start\", []))\n",
    "        types = np.array(trajectory.get(\"type_event\", []))\n",
    "\n",
    "        if len(times) != len(types) or len(times) == 0:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            types = types.astype(int)\n",
    "            # Filter out non-numeric times\n",
    "            valid_indices = [idx for idx, t in enumerate(times) if isinstance(t, (int, float))]\n",
    "            times = times[valid_indices]\n",
    "            types = types[valid_indices]\n",
    "\n",
    "            for mark in range(dim_process):\n",
    "                mark_times = times[types == mark]\n",
    "                if len(mark_times) > 0:  # Only histogram if there are times for this mark\n",
    "                    hist, _ = np.histogram(mark_times, bins=bins)\n",
    "                    heatmap_counts[mark, :] += hist\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process trajectory for heatmap: {e}\")\n",
    "\n",
    "    # Normalize counts (optional)\n",
    "    if normalize and num_trajectories > 0:\n",
    "        heatmap_data = heatmap_counts / num_trajectories\n",
    "        cbar_label = \"Avg Events / Trajectory / Bin\"\n",
    "    else:\n",
    "        heatmap_data = heatmap_counts\n",
    "        cbar_label = \"Total Events / Bin\"\n",
    "\n",
    "    # Plotting the heatmap with professional styling\n",
    "    cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "    sns.heatmap(heatmap_data, ax=ax, cmap=cmap, cbar_kws={\"label\": cbar_label, \"shrink\": 0.8}, linewidths=0.5, linecolor=\"white\")\n",
    "\n",
    "    # Set x-axis ticks and labels (show fewer labels for clarity)\n",
    "    tick_positions = np.linspace(0, n_bins - 1, num=min(n_bins, 8), dtype=int)  # Show ~8 ticks\n",
    "    ax.set_xticks(tick_positions + 0.5)  # Center ticks\n",
    "    ax.set_xticklabels([f\"{bin_centers[i]:.1f}\" for i in tick_positions], rotation=45, ha=\"right\")\n",
    "\n",
    "    ax.set_yticks(np.arange(dim_process) + 0.5)\n",
    "    ax.set_yticklabels([f\"Mark {i}\" for i in range(dim_process)], rotation=0)\n",
    "\n",
    "    ax.set_xlabel(\"Time Bins (from time_since_start)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Event Mark\", fontsize=10)\n",
    "    title = f\"Event Count Heatmap ({split} split)\"\n",
    "    if normalize:\n",
    "        title = f\"Average {title}\"\n",
    "    ax.set_title(title, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    if fig_created:  # Only show if we created the figure\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def create_appendix_plots(dataset, split=\"train\", save_path=None):\n",
    "    \"\"\"\n",
    "    Creates a comprehensive set of plots for the appendix optimized for A4 page.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): Dictionary containing Hugging Face Dataset objects per split.\n",
    "        split (str): The dataset split to use.\n",
    "        save_path (str, optional): Path to save the plots. If None, plots are displayed.\n",
    "    \"\"\"\n",
    "    # Ensure figures directory exists\n",
    "    import os\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    # Set style for publication-quality plots (fonts already configured globally)\n",
    "    plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "    plt.rcParams.update(\n",
    "        {\n",
    "            \"font.size\": 8,\n",
    "            \"axes.titlesize\": 10,\n",
    "            \"axes.labelsize\": 9,\n",
    "            \"xtick.labelsize\": 7,\n",
    "            \"ytick.labelsize\": 7,\n",
    "            \"legend.fontsize\": 7,\n",
    "            \"figure.titlesize\": 12,\n",
    "            \"lines.linewidth\": 1.5,\n",
    "            \"lines.markersize\": 4,\n",
    "            \"grid.alpha\": 0.4,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"No data found for split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    # Get dataset dimensions\n",
    "    try:\n",
    "        dim_process = int(data_split[0][\"dim_process\"])\n",
    "        seq_lengths = [len(seq[\"time_since_last_event\"]) for seq in data_split]\n",
    "        num_sequences = len(data_split)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting dataset info: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Creating appendix plots for dataset with {dim_process}D processes, {num_sequences} sequences\")\n",
    "\n",
    "    # Create figure optimized for single column layout\n",
    "    plt.figure(figsize=(8.3, 20))\n",
    "\n",
    "    # 1. Aggregate Rate (top)\n",
    "    ax1 = plt.subplot(6, 1, 1)\n",
    "    plot_aggregate_rate(dataset, split=split, ax=ax1)\n",
    "\n",
    "    # 2. Rate per Mark\n",
    "    ax2 = plt.subplot(6, 1, 2)\n",
    "    plot_rate_per_mark(dataset, split=split, ax=ax2)\n",
    "\n",
    "    # 3. Raster Plot\n",
    "    ax3 = plt.subplot(6, 1, 3)\n",
    "    plot_raster_sample(dataset, split=split, n_samples=8, ax=ax3, seed=42)\n",
    "\n",
    "    # 4. Event Count Heatmap\n",
    "    ax4 = plt.subplot(6, 1, 4)\n",
    "    plot_event_count_heatmap(dataset, split=split, ax=ax4)\n",
    "\n",
    "    # 5. Sequence Length Distribution\n",
    "    ax5 = plt.subplot(6, 1, 5)\n",
    "    ax5.hist(seq_lengths, bins=20, edgecolor=\"black\", alpha=0.8, color=PROFESSIONAL_COLORS[1])\n",
    "    ax5.set_title(\"Sequence Length Distribution\", fontsize=10, fontweight=\"bold\")\n",
    "    ax5.set_xlabel(\"Sequence Length\", fontsize=9)\n",
    "    ax5.set_ylabel(\"Frequency\", fontsize=9)\n",
    "    ax5.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    ax5.spines[\"top\"].set_visible(False)\n",
    "    ax5.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # 6. Dataset Info and Event Type Distribution (bottom)\n",
    "    ax6 = plt.subplot(6, 1, 6)\n",
    "\n",
    "    # Event Type Distribution as bar chart\n",
    "    event_counts = [0] * dim_process\n",
    "    for seq in data_split:\n",
    "        types = seq.get(\"type_event\", [])\n",
    "        for t in types:\n",
    "            if isinstance(t, int) and 0 <= t < dim_process:\n",
    "                event_counts[t] += 1\n",
    "\n",
    "    colors = get_professional_palette(dim_process)\n",
    "    ax6.bar(range(dim_process), event_counts, color=colors, edgecolor=\"black\", alpha=0.8, linewidth=1)\n",
    "    ax6.set_title(\"Event Type Distribution\", fontsize=10, fontweight=\"bold\")\n",
    "    ax6.set_xlabel(\"Event Type\", fontsize=9)\n",
    "    ax6.set_ylabel(\"Total Count\", fontsize=9)\n",
    "    ax6.set_xticks(range(dim_process))\n",
    "    ax6.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "    ax6.spines[\"top\"].set_visible(False)\n",
    "    ax6.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.suptitle(f\"Synthetic Hawkes Process Dataset Analysis ({dim_process}D, {split} split)\", fontsize=14, fontweight=\"bold\", y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.96])\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}_appendix_{dim_process}D_{split}.png\", dpi=300, bbox_inches=\"tight\", facecolor=\"white\", edgecolor=\"none\")\n",
    "        plt.savefig(f\"{save_path}_appendix_{dim_process}D_{split}.pdf\", bbox_inches=\"tight\", facecolor=\"white\", edgecolor=\"none\")\n",
    "        print(f\"Plots saved to {save_path}_appendix_{dim_process}D_{split}.[png/pdf]\")\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comprehensive_stats(dataset, split=\"train\"):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive statistics for a dataset split.\n",
    "    Returns a dictionary with all relevant statistics.\n",
    "    \"\"\"\n",
    "    data_split = dataset[split]\n",
    "\n",
    "    # Basic counts\n",
    "    num_sequences = len(data_split)\n",
    "\n",
    "    # Sequence lengths\n",
    "    seq_lengths = [len(seq[\"time_since_last_event\"]) for seq in data_split]\n",
    "\n",
    "    # Event times and deltas\n",
    "    times_since_last_event = [time for seq in data_split for time in seq[\"time_since_last_event\"]]\n",
    "    times_since_start = [time for seq in data_split for time in seq[\"time_since_start\"]]\n",
    "\n",
    "    # Event types\n",
    "    all_event_types = [event_type for seq in data_split for event_type in seq[\"type_event\"]]\n",
    "    unique_event_types = set(all_event_types)\n",
    "\n",
    "    # Get dimension from first sequence\n",
    "    dim_process = data_split[0][\"dim_process\"] if data_split else 0\n",
    "\n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        \"dimension\": dim_process,\n",
    "        \"num_sequences\": num_sequences,\n",
    "        \"total_events\": len(times_since_last_event),\n",
    "        \"num_event_types\": len(unique_event_types),\n",
    "        # Sequence length statistics\n",
    "        \"min_seq_length\": min(seq_lengths) if seq_lengths else 0,\n",
    "        \"max_seq_length\": max(seq_lengths) if seq_lengths else 0,\n",
    "        \"mean_seq_length\": sum(seq_lengths) / len(seq_lengths) if seq_lengths else 0,\n",
    "        \"std_seq_length\": np.std(seq_lengths) if seq_lengths else 0,\n",
    "        # Event time statistics\n",
    "        \"min_delta_time\": min(times_since_last_event) if times_since_last_event else 0,\n",
    "        \"max_delta_time\": max(times_since_last_event) if times_since_last_event else 0,\n",
    "        \"mean_delta_time\": sum(times_since_last_event) / len(times_since_last_event) if times_since_last_event else 0,\n",
    "        \"std_delta_time\": np.std(times_since_last_event) if times_since_last_event else 0,\n",
    "        # Absolute time statistics\n",
    "        \"min_abs_time\": min(times_since_start) if times_since_start else 0,\n",
    "        \"max_abs_time\": max(times_since_start) if times_since_start else 0,\n",
    "        \"mean_abs_time\": sum(times_since_start) / len(times_since_start) if times_since_start else 0,\n",
    "        \"std_abs_time\": np.std(times_since_start) if times_since_start else 0,\n",
    "        # Event type distribution\n",
    "        \"event_type_counts\": {str(i): all_event_types.count(i) for i in range(dim_process)},\n",
    "    }\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated heatmap function with continuous styling\n",
    "def plot_event_count_heatmap_continuous(dataset, split=\"train\", n_bins=30, max_time=None, normalize=True, ax=None):\n",
    "    \"\"\"\n",
    "    Plots a continuous heatmap of event counts per mark over time using a Hugging Face Dataset.\n",
    "    Assumes 'time_since_start', 'type_event', 'dim_process' columns exist.\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): Dictionary containing Hugging Face Dataset objects per split.\n",
    "        split (str): The dataset split to use.\n",
    "        n_bins (int): Number of time bins.\n",
    "        max_time (float, optional): Maximum time for the plot range. If None, calculated from data.\n",
    "        normalize (bool): If True, normalize counts by the number of trajectories.\n",
    "        ax (matplotlib.axes.Axes, optional): Matplotlib axes to plot on. If None, creates a new figure.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig_created = True\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    else:\n",
    "        fig_created = False\n",
    "\n",
    "    data_split = dataset.get(split)\n",
    "    if data_split is None:\n",
    "        print(f\"No data found for split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    num_trajectories = len(data_split)\n",
    "    if num_trajectories == 0:\n",
    "        print(f\"No trajectories in split '{split}'.\")\n",
    "        return\n",
    "\n",
    "    # Check for required columns\n",
    "    required_cols = [\"dim_process\", \"time_since_start\", \"type_event\"]\n",
    "    if not all(col in data_split.column_names for col in required_cols):\n",
    "        print(f\"Error: Dataset split '{split}' missing one or more required columns: {required_cols}\")\n",
    "        return\n",
    "\n",
    "    # Infer number of marks\n",
    "    try:\n",
    "        dim_process = int(data_split[0][\"dim_process\"])\n",
    "    except (ValueError, TypeError):\n",
    "        print(\"Error: 'dim_process' feature is not a valid integer in the first row.\")\n",
    "        return\n",
    "\n",
    "    if max_time is None:\n",
    "        max_time = get_max_time(dataset, split)\n",
    "\n",
    "    # Define time bins\n",
    "    bins = np.linspace(0, max_time, n_bins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2  # For labeling ticks\n",
    "\n",
    "    # Initialize count matrix (marks x time_bins)\n",
    "    heatmap_counts = np.zeros((dim_process, n_bins))\n",
    "\n",
    "    # Populate the heatmap counts\n",
    "    for trajectory in data_split:\n",
    "        times = np.array(trajectory.get(\"time_since_start\", []))\n",
    "        types = np.array(trajectory.get(\"type_event\", []))\n",
    "\n",
    "        if len(times) != len(types) or len(times) == 0:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            types = types.astype(int)\n",
    "            # Filter out non-numeric times\n",
    "            valid_indices = [idx for idx, t in enumerate(times) if isinstance(t, (int, float))]\n",
    "            times = times[valid_indices]\n",
    "            types = types[valid_indices]\n",
    "\n",
    "            for mark in range(dim_process):\n",
    "                mark_times = times[types == mark]\n",
    "                if len(mark_times) > 0:  # Only histogram if there are times for this mark\n",
    "                    hist, _ = np.histogram(mark_times, bins=bins)\n",
    "                    heatmap_counts[mark, :] += hist\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process trajectory for heatmap: {e}\")\n",
    "\n",
    "    # Normalize counts (optional)\n",
    "    if normalize and num_trajectories > 0:\n",
    "        heatmap_data = heatmap_counts / num_trajectories\n",
    "        cbar_label = \"Avg Events / Trajectory / Bin\"\n",
    "    else:\n",
    "        heatmap_data = heatmap_counts\n",
    "        cbar_label = \"Total Events / Bin\"\n",
    "\n",
    "    # Plotting the heatmap with continuous styling\n",
    "    cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        ax=ax,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={\"label\": cbar_label, \"shrink\": 0.8},\n",
    "        linewidths=0,  # Remove grid lines for continuous appearance\n",
    "        square=False,  # Allow rectangular cells\n",
    "        cbar=True,  # Ensure colorbar is shown\n",
    "        vmin=0,  # Set minimum value for color scale\n",
    "        vmax=None,\n",
    "    )  # Let it auto-scale maximum\n",
    "\n",
    "    # Set x-axis ticks and labels (show fewer labels for clarity)\n",
    "    tick_positions = np.linspace(0, n_bins - 1, num=min(n_bins, 8), dtype=int)  # Show ~8 ticks\n",
    "    ax.set_xticks(tick_positions + 0.5)  # Center ticks\n",
    "    ax.set_xticklabels([f\"{bin_centers[i]:.1f}\" for i in tick_positions], rotation=45, ha=\"right\")\n",
    "\n",
    "    ax.set_yticks(np.arange(dim_process) + 0.5)\n",
    "    ax.set_yticklabels([f\"Mark {i}\" for i in range(dim_process)], rotation=0)\n",
    "\n",
    "    ax.set_xlabel(\"Time Bins (from time_since_start)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Event Mark\", fontsize=10)\n",
    "    title = f\"Event Count Heatmap ({split} split)\"\n",
    "    if normalize:\n",
    "        title = f\"Average {title}\"\n",
    "    ax.set_title(title, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    if fig_created:  # Only show if we created the figure\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive analysis for all dimensions\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"figures/hawkes_dataset\", exist_ok=True)\n",
    "os.makedirs(\"tables\", exist_ok=True)\n",
    "\n",
    "# Initialize results storage\n",
    "all_stats = {}\n",
    "all_plots_created = {}\n",
    "\n",
    "print(\"Starting comprehensive analysis for all dimensions...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for dim_name, config in configs_by_dimension.items():\n",
    "    print(f\"\\nProcessing {dim_name} dataset...\")\n",
    "\n",
    "    try:\n",
    "        # Create dataloader for this dimension\n",
    "        dataloader = DataLoaderFactory.create(**config)\n",
    "        data_point = next(iter(dataloader.train_it))\n",
    "\n",
    "        # Process data (same as before but for each dimension)\n",
    "        num_samples = 1_000\n",
    "        i = 0\n",
    "        data = []\n",
    "\n",
    "        for sample in dataloader.train_it:\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "            event_times = sample[\"context_event_times\"]\n",
    "            seq_lengths = sample[\"context_seq_lengths\"]\n",
    "            process_dim = sample[\"base_intensity_functions\"].shape[1]\n",
    "\n",
    "            # Normalize the event times\n",
    "            normalized_event_times = normalize_obs_grid(event_times, seq_lengths)\n",
    "\n",
    "            # Calculate delta times\n",
    "            delta_times = normalized_event_times[:, :, 1:] - normalized_event_times[:, :, :-1]\n",
    "            delta_times = torch.cat([torch.zeros_like(delta_times[:, :, :1]), delta_times], dim=2)\n",
    "\n",
    "            # Create processed sample\n",
    "            processed_sample = {\n",
    "                \"event_times\": normalized_event_times,\n",
    "                \"delta_times\": delta_times,\n",
    "                \"event_types\": sample[\"context_event_types\"],\n",
    "                \"seq_lengths\": seq_lengths,\n",
    "                \"dim_process\": process_dim,\n",
    "            }\n",
    "            data.append(processed_sample)\n",
    "\n",
    "        # Convert to EasyTPP format\n",
    "        converted_datasets = []\n",
    "        for sample in data:\n",
    "            converted_datasets.extend(convert_to_easytpp_format(sample))\n",
    "        dataset = Dataset.from_list(converted_datasets)\n",
    "        dataset = DatasetDict({\"train\": dataset})\n",
    "\n",
    "        # Calculate statistics\n",
    "        print(f\"Calculating statistics for {dim_name}...\")\n",
    "        stats = calculate_comprehensive_stats(dataset, split=\"train\")\n",
    "        all_stats[dim_name] = stats\n",
    "\n",
    "        # Create plots\n",
    "        print(f\"Creating plots for {dim_name}...\")\n",
    "        plot_path = f\"figures/hawkes_dataset/{dim_name}\"\n",
    "        create_appendix_plots(dataset, split=\"train\", save_path=plot_path)\n",
    "        all_plots_created[dim_name] = plot_path\n",
    "\n",
    "        print(f\"âœ“ {dim_name} completed successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error processing {dim_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Analysis completed!\")\n",
    "print(f\"Processed {len(all_stats)} dimensions\")\n",
    "print(f\"Created plots for {len(all_plots_created)} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the original heatmap function to use the continuous version\n",
    "# plot_event_count_heatmap = plot_event_count_heatmap_continuous\n",
    "\n",
    "print(\"âœ… Heatmap function updated to use continuous styling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latex_table(all_stats, save_path=\"tables/hawkes_dataset_stats.tex\"):\n",
    "    \"\"\"\n",
    "    Create a comprehensive LaTeX table with statistics for all dimensions.\n",
    "    \"\"\"\n",
    "    # Prepare data for DataFrame\n",
    "    table_data = []\n",
    "\n",
    "    for dim_name, stats in all_stats.items():\n",
    "        row = {\n",
    "            \"Dimension\": dim_name,\n",
    "            \"Process Dim\": stats[\"dimension\"],\n",
    "            \"Sequences\": stats[\"num_sequences\"],\n",
    "            \"Total Events\": stats[\"total_events\"],\n",
    "            \"Event Types\": stats[\"num_event_types\"],\n",
    "            \"Seq Length (min)\": f\"{stats['min_seq_length']:.0f}\",\n",
    "            \"Seq Length (max)\": f\"{stats['max_seq_length']:.0f}\",\n",
    "            \"Seq Length (mean)\": f\"{stats['mean_seq_length']:.1f}\",\n",
    "            \"Seq Length (std)\": f\"{stats['std_seq_length']:.1f}\",\n",
    "            \"Delta Time (min)\": f\"{stats['min_delta_time']:.3f}\",\n",
    "            \"Delta Time (max)\": f\"{stats['max_delta_time']:.3f}\",\n",
    "            \"Delta Time (mean)\": f\"{stats['mean_delta_time']:.3f}\",\n",
    "            \"Delta Time (std)\": f\"{stats['std_delta_time']:.3f}\",\n",
    "            \"Abs Time (min)\": f\"{stats['min_abs_time']:.3f}\",\n",
    "            \"Abs Time (max)\": f\"{stats['max_abs_time']:.3f}\",\n",
    "            \"Abs Time (mean)\": f\"{stats['mean_abs_time']:.3f}\",\n",
    "            \"Abs Time (std)\": f\"{stats['std_abs_time']:.3f}\",\n",
    "        }\n",
    "        table_data.append(row)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    # Generate LaTeX table\n",
    "    latex_table = df.to_latex(\n",
    "        index=False,\n",
    "        escape=False,\n",
    "        column_format=\"l\" + \"c\" * (len(df.columns) - 1),\n",
    "        caption=\"Comprehensive Statistics for Synthetic Hawkes Process Datasets\",\n",
    "        label=\"tab:hawkes_dataset_stats\",\n",
    "        position=\"htbp\",\n",
    "    )\n",
    "\n",
    "    # Add custom formatting\n",
    "    latex_table = latex_table.replace(\"\\\\begin{table}[htbp]\", \"\\\\begin{table}[htbp]\\n\\\\centering\")\n",
    "    latex_table = latex_table.replace(\n",
    "        \"\\\\toprule\",\n",
    "        \"\\\\toprule\\n\\\\multicolumn{1}{c}{\\\\textbf{Dimension}} & \\\\multicolumn{1}{c}{\\\\textbf{Process Dim}} & \\\\multicolumn{1}{c}{\\\\textbf{Sequences}} & \\\\multicolumn{1}{c}{\\\\textbf{Total Events}} & \\\\multicolumn{1}{c}{\\\\textbf{Event Types}} & \\\\multicolumn{4}{c}{\\\\textbf{Sequence Length}} & \\\\multicolumn{4}{c}{\\\\textbf{Delta Time}} & \\\\multicolumn{4}{c}{\\\\textbf{Absolute Time}} \\\\\\\\\\n\\\\cmidrule(lr){6-9} \\\\cmidrule(lr){10-13} \\\\cmidrule(lr){14-17}\\n& & & & & \\\\textbf{Min} & \\\\textbf{Max} & \\\\textbf{Mean} & \\\\textbf{Std} & \\\\textbf{Min} & \\\\textbf{Max} & \\\\textbf{Mean} & \\\\textbf{Std} & \\\\textbf{Min} & \\\\textbf{Max} & \\\\textbf{Mean} & \\\\textbf{Std} \\\\\\\\\",\n",
    "    )\n",
    "\n",
    "    # Save to file\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(latex_table)\n",
    "\n",
    "    print(f\"LaTeX table saved to: {save_path}\")\n",
    "    return latex_table\n",
    "\n",
    "\n",
    "# Create the LaTeX table\n",
    "if all_stats:\n",
    "    latex_table = create_latex_table(all_stats)\n",
    "    print(\"\\nLaTeX Table Preview:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(latex_table)\n",
    "else:\n",
    "    print(\"No statistics available to create table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of created outputs\n",
    "print(\"SUMMARY OF GENERATED OUTPUTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nðŸ“Š PLOTS CREATED:\")\n",
    "for dim_name, plot_path in all_plots_created.items():\n",
    "    print(f\"  {dim_name}: {plot_path}_appendix_{dim_name}_train.png\")\n",
    "    print(f\"  {dim_name}: {plot_path}_appendix_{dim_name}_train.pdf\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ STATISTICS CALCULATED:\")\n",
    "for dim_name, stats in all_stats.items():\n",
    "    print(f\"  {dim_name}: {stats['num_sequences']} sequences, {stats['total_events']} total events\")\n",
    "\n",
    "print(\"\\nðŸ“‹ LATEX TABLE:\")\n",
    "print(\"  Saved to: tables/hawkes_dataset_stats.tex\")\n",
    "\n",
    "print(\"\\nðŸ“ OUTPUT DIRECTORIES:\")\n",
    "print(\"  figures/hawkes_dataset/ - Contains all plot files\")\n",
    "print(\"  tables/ - Contains LaTeX table file\")\n",
    "\n",
    "print(\"\\nâœ… All outputs successfully generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
