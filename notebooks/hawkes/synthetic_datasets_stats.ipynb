{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fim.data.dataloaders import DataLoaderFactory\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "    \"name\": \"HawkesDataLoader\",\n",
    "    \"path_collections\": {\n",
    "        \"train\": (\n",
    "            \"/home/berghaus/FoundationModels/FIM/data/synthetic_data/hawkes/1k_5_st_hawkes_mixed_2000_paths_250_events/train\",\n",
    "        ),\n",
    "        \"validation\": (\n",
    "            \"/home/berghaus/FoundationModels/FIM/data/synthetic_data/hawkes/1k_5_st_hawkes_mixed_2000_paths_250_events/val\",\n",
    "        )\n",
    "    },\n",
    "    \"loader_kwargs\": {\n",
    "        \"batch_size\": 1,\n",
    "        \"num_workers\": 8,\n",
    "        \"test_batch_size\": 1,\n",
    "        \"variable_num_of_paths\": True,\n",
    "        \"min_path_count\": 100,\n",
    "        \"max_path_count\": 1000,\n",
    "        \"max_number_of_minibatch_sizes\": 10,\n",
    "        \"variable_sequence_lens\": True,\n",
    "        \"min_sequence_len\": 10,\n",
    "        \"max_sequence_len\": 250,\n",
    "        \"num_kernel_evaluation_points\": 10,\n",
    "        \"is_bulk_model\": False\n",
    "    },\n",
    "    \"dataset_kwargs\": {\n",
    "        \"files_to_load\": {\n",
    "            \"base_intensities\": \"base_intensities.pt\",\n",
    "            \"event_times\": \"event_times.pt\",\n",
    "            \"event_types\": \"event_types.pt\",\n",
    "            \"kernel_evaluations\": \"kernel_evaluations.pt\",\n",
    "            \"kernel_grids\": \"kernel_grids.pt\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "\n",
    "def normalize_obs_grid(obs_grid: Tensor, seq_lengths: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        batch_indices = torch.arange(obs_grid.size(0), device=obs_grid.device).view(-1, 1).expand(-1, obs_grid.size(1))\n",
    "        path_indices = torch.arange(obs_grid.size(1), device=obs_grid.device).view(1, -1).expand(obs_grid.size(0), -1)\n",
    "        max_times = obs_grid[batch_indices, path_indices, seq_lengths-1]          \n",
    "        norm_constants = max_times.amax(dim=[1,2])\n",
    "        obs_grid_normalized = obs_grid / norm_constants.view(-1, 1, 1, 1)\n",
    "        return obs_grid_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoaderFactory.create(**dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "num_samples = 10\n",
    "i = 0\n",
    "\n",
    "data = []\n",
    "for sample in dataloader.train_it:\n",
    "    if i >= num_samples:\n",
    "        break\n",
    "    i += 1\n",
    "    sample[\"event_times\"] = normalize_obs_grid(sample[\"event_times\"], sample[\"seq_lengths\"])\n",
    "    sample[\"delta_times\"] = sample[\"event_times\"][:, :, 1:] - sample[\"event_times\"][:, :, :-1]\n",
    "    # Add a delta time of 0 for the first event\n",
    "    sample[\"delta_times\"] = torch.cat([torch.zeros_like(sample[\"delta_times\"][:, :, :1]), sample[\"delta_times\"]], dim=2)\n",
    "    data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['base_intensities', 'event_times', 'event_types', 'kernel_evaluations', 'kernel_grids', 'seq_lengths', 'delta_times'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def convert_to_easytpp_format(sample):\n",
    "    # Extract tensors from the dictionary\n",
    "    event_times = sample[\"event_times\"][0]  # [P, L, 1]\n",
    "    delta_times = sample[\"delta_times\"][0]  # [P, L, 1]\n",
    "    event_types = sample[\"event_types\"][0]  # [P, L, 1]\n",
    "    seq_lengths = sample[\"seq_lengths\"][0]  # [P]\n",
    "\n",
    "    # Get dimensions\n",
    "    num_sequences = event_times.shape[0]\n",
    "\n",
    "    # Initialize list to store the converted data\n",
    "    easytpp_data = []\n",
    "\n",
    "    # Process each sequence\n",
    "    for i in range(num_sequences):\n",
    "        seq_len = seq_lengths[i].item()\n",
    "\n",
    "        # Extract valid events for this sequence\n",
    "        times = event_times[i, :seq_len, 0].cpu().numpy().tolist()\n",
    "        deltas = delta_times[i, :seq_len, 0].cpu().numpy().tolist()\n",
    "        types = event_types[i, :seq_len, 0].cpu().numpy().tolist()\n",
    "\n",
    "        # Create a single sequence entry\n",
    "        sequence_entry = {\n",
    "            \"time_since_start\": times,\n",
    "            \"time_since_last_event\": deltas,\n",
    "            \"type_event\": types,\n",
    "            \"seq_idx\": i,\n",
    "            \"seq_len\": seq_len,\n",
    "            \"dim_process\": 1\n",
    "        }\n",
    "        \n",
    "        easytpp_data.append(sequence_entry)\n",
    "\n",
    "    # Create a Hugging Face Dataset\n",
    "    dataset = Dataset.from_list(easytpp_data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each sample to EasyTPP format\n",
    "converted_datasets = [convert_to_easytpp_format(sample) for sample in data]\n",
    "\n",
    "# Create the final dataset with the train split\n",
    "data = [{\"test\": converted_dataset} for converted_dataset in converted_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(dataset, split='test'):\n",
    "   dataset = dataset[split]\n",
    "   stats = {\"num_sequences\": len(dataset), \"max_sequence_length\": 0, \"min_sequence_length\": 1000000, \"max_event_time\": 0, \"min_event_time\": 1000000}\n",
    "   seq_lengths = [len(seq['time_since_last_event']) for seq in dataset]\n",
    "   times_since_last_event = [time for seq in dataset for time in seq['time_since_last_event']]\n",
    "   min_delta_event_time = min(times_since_last_event)\n",
    "   max_delta_event_time = max(times_since_last_event)\n",
    "   min_seq_length = min(seq_lengths)\n",
    "   max_seq_length = max(seq_lengths)\n",
    "   avg_seq_length = sum(seq_lengths) / len(seq_lengths)\n",
    "   avg_delta_event_time = sum(times_since_last_event) / len(times_since_last_event)\n",
    "   stats[\"max_sequence_length\"] = max_seq_length\n",
    "   stats[\"min_sequence_length\"] = min_seq_length\n",
    "   stats[\"num_sequences\"] = len(seq_lengths)\n",
    "   stats[\"avg_sequence_length\"] = avg_seq_length\n",
    "   stats[\"max_event_time\"] = max_delta_event_time\n",
    "   stats[\"min_event_time\"] = min_delta_event_time\n",
    "   stats[\"avg_event_time\"] = avg_delta_event_time\n",
    "   encoutered_marks = set()\n",
    "   for seq in dataset:\n",
    "       for mark in seq['type_event']:\n",
    "           if mark not in encoutered_marks:\n",
    "               encoutered_marks.add(mark)\n",
    "   stats[\"num_marks\"] = len(encoutered_marks)\n",
    "   pprint(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_event_time': 0.014049674406222562,\n",
      " 'avg_sequence_length': 34.63,\n",
      " 'max_event_time': 0.16662979125976562,\n",
      " 'max_sequence_length': 85,\n",
      " 'max_total_time': 1.0,\n",
      " 'min_event_time': 0.0,\n",
      " 'min_sequence_length': 20,\n",
      " 'num_marks': 5,\n",
      " 'num_sequences': 100}\n"
     ]
    }
   ],
   "source": [
    "print_stats(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
