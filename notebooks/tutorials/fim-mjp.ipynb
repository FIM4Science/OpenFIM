{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8292fef340b747789c9f6de06a78ab37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mjp.py:   0%|          | 0.00/7.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbcd841d36b4c558dd56550f42986ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "6_st_DFR_V=0.zip:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f441c233f4014b5fa633016a124438fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c93732ccb254c788a839e03d246e826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "FIMMJP(\n",
       "  (pos_encodings): SineTimeEncoding(\n",
       "    (linear_embedding): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (periodic_embedding): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=57, bias=True)\n",
       "      (1): SinActivation()\n",
       "    )\n",
       "  )\n",
       "  (ts_encoder): TransformerEncoder(\n",
       "    (layers): MaskedSequential(\n",
       "      (0): TransformerBlock(\n",
       "        (attention_head): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): MLP(\n",
       "          (layers): Sequential(\n",
       "            (linear_0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (activation_0): ReLU()\n",
       "            (linear_1): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (activation_1): ReLU()\n",
       "            (output): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention_head): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): MLP(\n",
       "          (layers): Sequential(\n",
       "            (linear_0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (activation_0): ReLU()\n",
       "            (linear_1): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (activation_1): ReLU()\n",
       "            (output): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention_head): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): MLP(\n",
       "          (layers): Sequential(\n",
       "            (linear_0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (activation_0): ReLU()\n",
       "            (linear_1): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (activation_1): ReLU()\n",
       "            (output): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention_head): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): MLP(\n",
       "          (layers): Sequential(\n",
       "            (linear_0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (activation_0): ReLU()\n",
       "            (linear_1): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (activation_1): ReLU()\n",
       "            (output): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (path_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (intensity_matrix_decoder): MLP(\n",
       "    (layers): Sequential(\n",
       "      (linear_0): Linear(in_features=65, out_features=64, bias=True)\n",
       "      (activation_0): ReLU()\n",
       "      (dropout_0): Dropout(p=0.1, inplace=False)\n",
       "      (linear_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation_1): ReLU()\n",
       "      (output): Linear(in_features=64, out_features=60, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (initial_distribution_decoder): MLP(\n",
       "    (layers): Sequential(\n",
       "      (linear_0): Linear(in_features=65, out_features=64, bias=True)\n",
       "      (activation_0): ReLU()\n",
       "      (dropout_0): Dropout(p=0.1, inplace=False)\n",
       "      (linear_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation_1): ReLU()\n",
       "      (output): Linear(in_features=64, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (gaussian_nll): GaussianNLLLoss()\n",
       "  (init_cross_entropy): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = load_dataset(\"cvejoski/mjp\", download_mode=\"force_redownload\", trust_remote_code=True)\n",
    "data.set_format(\"torch\")\n",
    "fimmjp = AutoModel.from_pretrained(\"cvejoski/FIMMJP\", trust_remote_code=True, download_mode=\"force_redownload\")\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "   device = \"cuda:0\"\n",
    "elif  torch.backends.mps.is_available():\n",
    "   device = \"mps\"\n",
    "\n",
    "fimmjp = fimmjp.to(device)\n",
    "fimmjp.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {k:v.to(device) for k, v in data[\"train\"][:1].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = fimmjp(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
