{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86315917d0aa4f4c8adcbf2b0d490f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mjp.py:   0%|          | 0.00/7.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e055c6c58224a46abbcc15d398a5a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "6_st_DFR_V=0.zip:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf01076ab0c4551938ef128ac419f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "FIMMJP(\n",
       "  (pos_encodings): SineTimeEncoding(\n",
       "    (linear_embedding): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (periodic_embedding): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=57, bias=True)\n",
       "      (1): SinActivation()\n",
       "    )\n",
       "  )\n",
       "  (ts_encoder): TransformerEncoder(\n",
       "    (layers): MaskedSequential(\n",
       "      (0): TransformerBlock(\n",
       "        (attention_head): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): MLP(\n",
       "          (layers): Sequential(\n",
       "            (linear_0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (activation_0): ReLU()\n",
       "            (linear_1): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (activation_1): ReLU()\n",
       "            (output): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention_head): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): MLP(\n",
       "          (layers): Sequential(\n",
       "            (linear_0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (activation_0): ReLU()\n",
       "            (linear_1): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (activation_1): ReLU()\n",
       "            (output): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention_head): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): MLP(\n",
       "          (layers): Sequential(\n",
       "            (linear_0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (activation_0): ReLU()\n",
       "            (linear_1): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (activation_1): ReLU()\n",
       "            (output): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention_head): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): MLP(\n",
       "          (layers): Sequential(\n",
       "            (linear_0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (activation_0): ReLU()\n",
       "            (linear_1): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (activation_1): ReLU()\n",
       "            (output): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (path_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (intensity_matrix_decoder): MLP(\n",
       "    (layers): Sequential(\n",
       "      (linear_0): Linear(in_features=65, out_features=64, bias=True)\n",
       "      (activation_0): ReLU()\n",
       "      (dropout_0): Dropout(p=0.1, inplace=False)\n",
       "      (linear_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation_1): ReLU()\n",
       "      (output): Linear(in_features=64, out_features=60, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (initial_distribution_decoder): MLP(\n",
       "    (layers): Sequential(\n",
       "      (linear_0): Linear(in_features=65, out_features=64, bias=True)\n",
       "      (activation_0): ReLU()\n",
       "      (dropout_0): Dropout(p=0.1, inplace=False)\n",
       "      (linear_1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (activation_1): ReLU()\n",
       "      (output): Linear(in_features=64, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (gaussian_nll): GaussianNLLLoss()\n",
       "  (init_cross_entropy): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"cvejoski/mjp\", download_mode=\"force_redownload\", trust_remote_code=True)\n",
    "data.set_format(\"torch\")\n",
    "fimmjp = AutoModel.from_pretrained(\"cvejoski/FIMMJP\", trust_remote_code=True)\n",
    "fimmjp = fimmjp.to(\"cuda:0\")\n",
    "fimmjp.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {k:v.to(\"cuda:0\") for k, v in data[\"train\"][:1].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = fimmjp(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'im': tensor([[[0.0000e+00, 3.5877e+12, 1.7241e+03, 7.5924e-08, 1.1935e+05,\n",
       "           5.3584e-04],\n",
       "          [1.8114e-02, 0.0000e+00, 6.0541e-06, 4.9181e-16, 2.7034e+03,\n",
       "           1.4860e+00],\n",
       "          [7.4180e+10, 1.2032e-09, 0.0000e+00, 1.0689e+05, 9.5596e+00,\n",
       "           2.9231e+00],\n",
       "          [1.9610e+02, 1.4257e+09, 2.3333e-09, 0.0000e+00, 4.7958e-01,\n",
       "           8.0688e-08],\n",
       "          [1.4212e+09, 6.0376e+00, 1.4104e-12, 4.6095e+03, 0.0000e+00,\n",
       "           1.6523e+04],\n",
       "          [1.3282e+12, 8.9372e-14, 2.5462e+04, 1.9306e+01, 5.8214e+05,\n",
       "           0.0000e+00]]], device='cuda:0'),\n",
       " 'log_var_im': tensor([[[  0.0000,  14.9927,  -0.7790, -13.0483, -25.4156,   0.8221],\n",
       "          [ 10.2368,   0.0000,  -2.8299, -14.4526,  -5.9720,  -0.2031],\n",
       "          [-24.5299, -16.2988,   0.0000, -17.4131,  -4.3452, -15.0658],\n",
       "          [ 13.2008,  -6.2568,   8.3548,   0.0000,  23.4692,  11.1400],\n",
       "          [-16.7470,  -6.6993,  30.4441,  13.6615,   0.0000,  17.1055],\n",
       "          [ -3.7591,  -4.0660,   7.4323, -19.6868, -14.8919,   0.0000]]],\n",
       "        device='cuda:0'),\n",
       " 'init_cond': tensor([[-17.5206,  -3.5781,   2.9082,  15.3123, -36.3456,   6.6769]],\n",
       "        device='cuda:0'),\n",
       " 'losses': {'loss': tensor(2.1105e+26, device='cuda:0'),\n",
       "  'loss_gauss': tensor(1.5285e+26, device='cuda:0'),\n",
       "  'loss_initial': tensor(32.8331, device='cuda:0'),\n",
       "  'loss_missing_link': tensor(5.8197e+25, device='cuda:0'),\n",
       "  'rmse_loss': tensor(6.9860e+11, device='cuda:0'),\n",
       "  'beta_gauss_nll': tensor(1., device='cuda:0'),\n",
       "  'beta_init_cross_entropy': tensor(1., device='cuda:0'),\n",
       "  'beta_missing_link': tensor(1., device='cuda:0'),\n",
       "  'number_of_paths': tensor(10000, device='cuda:0')}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fim312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
