{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on How to use the FIM-MJP\n",
    "\n",
    "In this notebook we will show how to use the model proposesd in the paper [Foundation Inference Models for Markov Jump Processes](https://arxiv.org/abs/2406.06419) (Berghaus et. al. NeurIPS2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the Model \n",
    "\n",
    "### Input to the model\n",
    "\n",
    "The model takes as an input dictionary containing at least three items and one additional argument. The input dictionary should contain the following items:\n",
    "\n",
    "   1. The _observation grid_ with size `[num_paths, grid_size]` which are the locations in time when a observation was recorded. The key in the dictonary is `observation_grid` and the data type is `float`.\n",
    "   2. The _observation values_ with size `[num_paths, grid_size]` are the actuall observed values (state) of the process. The key in the dictionary is `observation_values` and the data type is `int`.\n",
    "   3. The _sequence length_ with size `[num_paths]` which is the length of the observed sequence. The key in the dictionary is `seq_length` and the data type is `int`.\n",
    "   4. The _dimension of the process_ which is an `integer` between 2 and 6. The maximum number of states that are supported by our model is 6. The argument name is `n_states`.\n",
    "\n",
    "Optionally, the dictionary can contain the following items:\n",
    "\n",
    "   - The _time normalization factor_ with size `[num_paths]` which is the factor by which the time is normalized. The key in the dictionary is `time_normalization_factors` and the data type is `float`. In case this item is not provided, the model will normalize the time by the maximum time in the observation grid.\n",
    "   - Items for calculating the loss:\n",
    "      - _intensity matrix_ with size `[num_paths, n_states, n_states]` which is the intensity matrix of the process. The key in the dictionary is `intensity_matrices` and the data type is `float`.\n",
    "      - _initial distribution_ with size `[num_paths, n_states]` which is the initial distribution of the process. The key in the dictionary is `initial_distributions` and the data type is `int`.\n",
    "      - _adjacency matrix_ with size `[num_paths, n_states, n_states]` which is the adjacency matrix of the process. The key in the dictionary is `adjacency_matrices` and the data type is `int`.\n",
    "\n",
    "### Output of the model\n",
    "\n",
    "The model returns a dictionary containing the following items:\n",
    "\n",
    "   - The _intensity matrix_ with size `[num_paths, n_states, n_states]` which is the intensity matrix of the process. The key in the dictionary is `intensity_matrices` and the data type is `float`.\n",
    "   - The _initial distribution_ with size `[num_paths, n_states]` which is the initial distribution of the process. The key in the dictionary is `initial_distributions` and the data type is `int`.\n",
    "   - The _adjacency matrix_ with size `[num_paths, n_states, n_states]` which is the adjacency matrix of the process. The key in the dictionary is `adjacency_matrices` and the data type is `int`.\n",
    "   - The _losses_ which is the loss of the model. The key in the dictionary is `loss` and the data type is `float`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModel\n",
    "from fim.models import FIMMJP\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from fim.trainers.utils import get_accel_type\n",
    "import pandas as pd\n",
    "device = get_accel_type()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We also provide a synthetic dataset for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193001ef0cf542b9a52f54533929791d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mjp.py:   0%|          | 0.00/7.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9085258b7f4f2ab3e67713cadf88a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "6_st_DFR_V=1.zip:   0%|          | 0.00/253k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52f63983c9747cea713f2cecc1e2d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the Discrete Flashing Ratchet (DFR) dataset from Huggingface\n",
    "data = load_dataset(\"cvejoski/mjp\", download_mode=\"force_redownload\", trust_remote_code=True, name=\"DFR_V=1\")\n",
    "data.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIMMJP(\n",
       "  (pos_encodings): DeltaTimeEncoding()\n",
       "  (ts_encoder): RNNEncoder(\n",
       "    (rnn): LSTM(8, 256, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (path_attention): MultiHeadLearnableQueryAttention(\n",
       "    (W_k): Linear(in_features=512, out_features=128, bias=False)\n",
       "    (W_v): Linear(in_features=512, out_features=128, bias=False)\n",
       "  )\n",
       "  (intensity_matrix_decoder): MLP(\n",
       "    (layers): Sequential(\n",
       "      (linear_0): Linear(in_features=2049, out_features=128, bias=True)\n",
       "      (activation_0): SELU()\n",
       "      (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation_1): SELU()\n",
       "      (output): Linear(in_features=128, out_features=60, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (initial_distribution_decoder): MLP(\n",
       "    (layers): Sequential(\n",
       "      (linear_0): Linear(in_features=2049, out_features=128, bias=True)\n",
       "      (activation_0): SELU()\n",
       "      (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation_1): SELU()\n",
       "      (output): Linear(in_features=128, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (gaussian_nll): GaussianNLLLoss()\n",
       "  (init_cross_entropy): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the FIMMJP model from Huggingface\n",
    "fimmjp = AutoModel.from_pretrained(\"cvejoski/FIMMJP\", trust_remote_code=True)\n",
    "# fimmjp = FIMMJP.load_model(\"/home/cvejoski/Projects/FoundationModels/FIM/results/FIM_MJP_Homogeneous_no_annealing_rnn_256_path_attention_one_head_model_dim_var_path_same-experiment-seed-0_11-15-1410/checkpoints/epoch-3009\")\n",
    "\n",
    "fimmjp = fimmjp.to(device)\n",
    "fimmjp.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy data to device\n",
    "batch = {k:v.to(device) for k, v in data[\"train\"][:1].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a batch\n",
    "n_paths_eval = [1, 30, 100, 300, 500, 1000, 5000]\n",
    "total_n_paths = batch[\"observation_grid\"].shape[1]\n",
    "statistics = total_n_paths // 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "   for n_paths in n_paths_eval:\n",
    "      for _ in range(statistics):\n",
    "         paths_idx = torch.randperm(total_n_paths)[:n_paths]\n",
    "         mini_batch = batch.copy()\n",
    "         mini_batch[\"observation_grid\"] = batch[\"observation_grid\"][:, paths_idx]\n",
    "         mini_batch[\"observation_values\"] = batch[\"observation_values\"][:, paths_idx]\n",
    "         mini_batch[\"seq_lengths\"] = batch[\"seq_lengths\"][:, paths_idx]\n",
    "         output = fimmjp(mini_batch, n_states=6)\n",
    "         result[n_paths].append(output[\"losses\"][\"rmse_loss\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = {n_paths: torch.tensor(losses).mean().item() for n_paths, losses in result.items()}\n",
    "stds = {n_paths: torch.tensor(losses).std().item() for n_paths, losses in result.items()}\n",
    "\n",
    "df_result = pd.DataFrame({\n",
    "   '# Paths during Evaluatioin': list(means.keys()),\n",
    "   'RMSE': [f\"{mean:.3f} Â± {std:.3f}\" for mean, std in zip(means.values(), stds.values())]\n",
    "})\n",
    "\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fim312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
