{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_dataset_builder, get_dataset_config_names\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(load_dataset_builder('monash_tsf', 'tourism_monthly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourism_monthly_ds = load_dataset('monash_tsf', 'tourism_monthly')\n",
    "train_dataset = tourism_monthly_ds[\"train\"]\n",
    "test_dataset = tourism_monthly_ds[\"test\"]\n",
    "\n",
    "train_example = tourism_monthly_ds['train'][0]\n",
    "validation_example = tourism_monthly_ds['validation'][0]\n",
    "test_example = tourism_monthly_ds['test'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(train_example['start'])\n",
    "print(train_example['target'])\n",
    "print(len(train_example['target']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_example['start'])\n",
    "print(validation_example['target'])\n",
    "print(len(validation_example['target']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_example['start'])\n",
    "print(test_example['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(train_example['target'],  label='Train Example', color='blue')\n",
    "plt.plot(validation_example['target'], label='Validation Example', color='green', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Train and Validation Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial, lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "@lru_cache(10_000)\n",
    "def convert_to_pandas_period(date, freq):\n",
    "    return pd.Period(date, freq=freq)\n",
    "\n",
    "def transform_start_field(batch, freq):\n",
    "    batch['start'] = [convert_to_pandas_period(date, freq) for date in batch['start']]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '1M'\n",
    "prediction_length = 24\n",
    "train_dataset.set_transform(partial(transform_start_field, freq=freq))\n",
    "test_dataset.set_transform(partial(transform_start_field, freq=freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.time_feature import get_lags_for_frequency\n",
    "\n",
    "lags_sequence = get_lags_for_frequency(freq)\n",
    "print(lags_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "\n",
    "time_features = time_features_from_frequency_str(freq)\n",
    "print(time_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\n",
    "\n",
    "config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=prediction_length,\n",
    "    # context length:\n",
    "    context_length=prediction_length * 2,\n",
    "    # lags coming from helper given the freq:\n",
    "    lags_sequence=lags_sequence,\n",
    "    # we'll add 2 time features (\"month of year\" and \"age\", see further):\n",
    "    num_time_features=len(time_features) + 1,\n",
    "    # we have a single static categorical feature, namely time series ID:\n",
    "    num_static_categorical_features=1,\n",
    "    # it has 366 possible values:\n",
    "    cardinality=[len(train_dataset)],\n",
    "    # the model will learn an embedding of size 2 for each of the 366 possible values:\n",
    "    embedding_dimension=[2],\n",
    "\n",
    "    # transformer params:\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    d_model=32,\n",
    ")\n",
    "\n",
    "model = TimeSeriesTransformerForPrediction(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.time_feature import (\n",
    "    time_features_from_frequency_str,\n",
    "    TimeFeature,\n",
    "    get_lags_for_frequency,\n",
    ")\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (\n",
    "    AddAgeFeature,\n",
    "    AddObservedValuesIndicator,\n",
    "    AddTimeFeatures,\n",
    "    AsNumpyArray,\n",
    "    Chain,\n",
    "    ExpectedNumInstanceSampler,\n",
    "    InstanceSplitter,\n",
    "    RemoveFields,\n",
    "    SelectFields,\n",
    "    SetField,\n",
    "    TestSplitSampler,\n",
    "    Transformation,\n",
    "    ValidationSplitSampler,\n",
    "    VstackFeatures,\n",
    "    RenameFields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\n",
    "    remove_field_names = []\n",
    "    if config.num_static_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\n",
    "    if config.num_dynamic_real_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\n",
    "    if config.num_static_categorical_features == 0:\n",
    "        remove_field_names.append(FieldName.FEAT_STATIC_CAT)\n",
    "\n",
    "    # a bit like torchvision.transforms.Compose\n",
    "    return Chain(\n",
    "        # step 1: remove static/dynamic fields if not specified\n",
    "        [RemoveFields(field_names=remove_field_names)]\n",
    "        # step 2: convert the data to NumPy (potentially not needed)\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_CAT,\n",
    "                    expected_ndim=1,\n",
    "                    dtype=int,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + (\n",
    "            [\n",
    "                AsNumpyArray(\n",
    "                    field=FieldName.FEAT_STATIC_REAL,\n",
    "                    expected_ndim=1,\n",
    "                )\n",
    "            ]\n",
    "            if config.num_static_real_features > 0\n",
    "            else []\n",
    "        )\n",
    "        + [\n",
    "            AsNumpyArray(\n",
    "                field=FieldName.TARGET,\n",
    "                # we expect an extra dim for the multivariate case:\n",
    "                expected_ndim=1 if config.input_size == 1 else 2,\n",
    "            ),\n",
    "            # step 3: handle the NaN's by filling in the target with zero\n",
    "            # and return the mask (which is in the observed values)\n",
    "            # true for observed values, false for nan's\n",
    "            # the decoder uses this mask (no loss is incurred for unobserved values)\n",
    "            # see loss_weights inside the xxxForPrediction model\n",
    "            AddObservedValuesIndicator(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.OBSERVED_VALUES,\n",
    "            ),\n",
    "            # step 4: add temporal features based on freq of the dataset\n",
    "            # month of year in the case when freq=\"M\"\n",
    "            # these serve as positional encodings\n",
    "            AddTimeFeatures(\n",
    "                start_field=FieldName.START,\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                time_features=time_features_from_frequency_str(freq),\n",
    "                pred_length=config.prediction_length,\n",
    "            ),\n",
    "            # step 5: add another temporal feature (just a single number)\n",
    "            # tells the model where in its life the value of the time series is,\n",
    "            # sort of a running counter\n",
    "            AddAgeFeature(\n",
    "                target_field=FieldName.TARGET,\n",
    "                output_field=FieldName.FEAT_AGE,\n",
    "                pred_length=config.prediction_length,\n",
    "                log_scale=False,\n",
    "            ),\n",
    "            # step 6: vertically stack all the temporal features into the key FEAT_TIME\n",
    "            VstackFeatures(\n",
    "                output_field=FieldName.FEAT_TIME,\n",
    "                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\n",
    "                + (\n",
    "                    [FieldName.FEAT_DYNAMIC_REAL]\n",
    "                    if config.num_dynamic_real_features > 0\n",
    "                    else []\n",
    "                ),\n",
    "            ),\n",
    "            # step 7: rename to match HuggingFace names\n",
    "            RenameFields(\n",
    "                mapping={\n",
    "                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\n",
    "                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\n",
    "                    FieldName.FEAT_TIME: \"time_features\",\n",
    "                    FieldName.TARGET: \"values\",\n",
    "                    FieldName.OBSERVED_VALUES: \"observed_mask\",\n",
    "                }\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.transform.sampler import InstanceSampler\n",
    "from typing import Optional\n",
    "\n",
    "def create_instance_splitter(\n",
    "    config: PretrainedConfig,\n",
    "    mode: str,\n",
    "    train_sampler: Optional[InstanceSampler] = None,\n",
    "    validation_sampler: Optional[InstanceSampler] = None,\n",
    ") -> Transformation:\n",
    "    assert mode in [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "    instance_sampler = {\n",
    "        \"train\": train_sampler\n",
    "        or ExpectedNumInstanceSampler(\n",
    "            num_instances=1.0, min_future=config.prediction_length\n",
    "        ),\n",
    "        \"validation\": validation_sampler\n",
    "        or ValidationSplitSampler(min_future=config.prediction_length),\n",
    "        \"test\": TestSplitSampler(),\n",
    "    }[mode]\n",
    "\n",
    "    return InstanceSplitter(\n",
    "        target_field=\"values\",\n",
    "        is_pad_field=FieldName.IS_PAD,\n",
    "        start_field=FieldName.START,\n",
    "        forecast_start_field=FieldName.FORECAST_START,\n",
    "        instance_sampler=instance_sampler,\n",
    "        past_length=config.context_length + max(config.lags_sequence),\n",
    "        future_length=config.prediction_length,\n",
    "        time_series_fields=[\"time_features\", \"observed_mask\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "from gluonts.itertools import Cached, Cyclic\n",
    "from gluonts.dataset.loader import as_stacked_batches\n",
    "\n",
    "\n",
    "def create_train_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    num_batches_per_epoch: int,\n",
    "    shuffle_buffer_length: Optional[int] = None,\n",
    "    cache_data: bool = True,\n",
    "    **kwargs,\n",
    ") -> Iterable:\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\n",
    "        \"future_values\",\n",
    "        \"future_observed_mask\",\n",
    "    ]\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data, is_train=True)\n",
    "    if cache_data:\n",
    "        transformed_data = Cached(transformed_data)\n",
    "\n",
    "    # we initialize a Training instance\n",
    "    instance_splitter = create_instance_splitter(config, \"train\")\n",
    "\n",
    "    # the instance splitter will sample a window of\n",
    "    # context length + lags + prediction length (from the 366 possible transformed time series)\n",
    "    # randomly from within the target time series and return an iterator.\n",
    "    stream = Cyclic(transformed_data).stream()\n",
    "    training_instances = instance_splitter.apply(stream)\n",
    "\n",
    "    return as_stacked_batches(\n",
    "        training_instances,\n",
    "        batch_size=batch_size,\n",
    "        shuffle_buffer_length=shuffle_buffer_length,\n",
    "        field_names=TRAINING_INPUT_NAMES,\n",
    "        output_type=torch.tensor,\n",
    "        num_batches_per_epoch=num_batches_per_epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_backtest_dataloader(\n",
    "    config: PretrainedConfig,\n",
    "    freq,\n",
    "    data,\n",
    "    batch_size: int,\n",
    "    **kwargs,\n",
    "):\n",
    "    PREDICTION_INPUT_NAMES = [\n",
    "        \"past_time_features\",\n",
    "        \"past_values\",\n",
    "        \"past_observed_mask\",\n",
    "        \"future_time_features\",\n",
    "    ]\n",
    "    if config.num_static_categorical_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\n",
    "\n",
    "    if config.num_static_real_features > 0:\n",
    "        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\n",
    "\n",
    "    transformation = create_transformation(freq, config)\n",
    "    transformed_data = transformation.apply(data)\n",
    "\n",
    "    # we create a Validation Instance splitter which will sample the very last\n",
    "    # context window seen during training only for the encoder.\n",
    "    instance_sampler = create_instance_splitter(config, \"validation\")\n",
    "\n",
    "    # we apply the transformations in train mode\n",
    "    testing_instances = instance_sampler.apply(transformed_data, is_train=True)\n",
    "\n",
    "    return as_stacked_batches(\n",
    "        testing_instances,\n",
    "        batch_size=batch_size,\n",
    "        output_type=torch.tensor,\n",
    "        field_names=PREDICTION_INPUT_NAMES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_train_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=train_dataset,\n",
    "    batch_size=256,\n",
    "    num_batches_per_epoch=100,\n",
    ")\n",
    "\n",
    "test_dataloader = create_backtest_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=test_dataset,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "test_batch = next(iter(test_dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape, v.type())\n",
    "print(\"=====================================\")\n",
    "for k, v in test_batch.items():\n",
    "    print(k, v.shape, v.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fim.data.dataloaders import BaseDataLoader\n",
    "from datasets import DownloadMode\n",
    "\n",
    "\n",
    "dl = BaseDataLoader(\"monash_tsf\", \"cif_2016\",  batch_size=32, test_batch_size=8, output_fields=[\"target\", \"time_feat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': tensor([ 855.1448,  847.2921,  824.7317,  866.6454,  928.3333,  927.6493,\n",
       "          901.0723,  940.8434,  954.7402,  960.6534,  982.3750,  899.6649,\n",
       "          993.9540, 1009.3860,  982.5870,  928.3856,  970.4632,  945.7121,\n",
       "         1101.3882, 1036.4437,  958.8881, 1018.3873, 1033.0359, 1056.0311,\n",
       "         1028.7294, 1071.8850, 1068.1157, 1036.9075, 1086.4535, 1085.1133,\n",
       "         1119.3572, 1123.7422, 1077.8127, 1127.4800, 1176.4647, 1019.8162,\n",
       "         1122.4321, 1106.1438, 1137.0187, 1098.7883, 1138.2531, 1175.6012,\n",
       "         1138.2096, 1203.3689, 1186.0001, 1221.8325, 1185.2561, 1198.4602,\n",
       "         1124.8439, 1176.3593, 1203.8153, 1270.6649, 1213.8478, 1276.1298,\n",
       "         1291.0450, 1281.3455, 1288.1537, 1306.7916, 1323.0547, 1300.7440,\n",
       "         1294.2686, 1278.7461, 1267.8088, 1313.7328, 1353.0507, 1352.4359,\n",
       "         1370.4838, 1311.1353, 1329.4156, 1285.0197, 1433.7803, 1457.5480,\n",
       "         1333.9028, 1364.5618, 1373.6469, 1415.9513, 1393.5914, 1376.0908,\n",
       "         1464.8846, 1410.0138, 1450.5886, 1376.8431, 1459.9176, 1409.3929,\n",
       "         1429.0784, 1489.3418, 1499.3964, 1530.4929, 1464.5566, 1455.0508,\n",
       "         1352.1611, 1434.4976, 1540.6188, 1529.9006, 1520.1356, 1537.2119]),\n",
       " 'time_feat': tensor([[   1, 1900],\n",
       "         [   2, 1900],\n",
       "         [   3, 1900],\n",
       "         [   4, 1900],\n",
       "         [   5, 1900],\n",
       "         [   6, 1900],\n",
       "         [   7, 1900],\n",
       "         [   8, 1900],\n",
       "         [   9, 1900],\n",
       "         [  10, 1900],\n",
       "         [  11, 1900],\n",
       "         [  12, 1900],\n",
       "         [   1, 1901],\n",
       "         [   2, 1901],\n",
       "         [   3, 1901],\n",
       "         [   4, 1901],\n",
       "         [   5, 1901],\n",
       "         [   6, 1901],\n",
       "         [   7, 1901],\n",
       "         [   8, 1901],\n",
       "         [   9, 1901],\n",
       "         [  10, 1901],\n",
       "         [  11, 1901],\n",
       "         [  12, 1901],\n",
       "         [   1, 1902],\n",
       "         [   2, 1902],\n",
       "         [   3, 1902],\n",
       "         [   4, 1902],\n",
       "         [   5, 1902],\n",
       "         [   6, 1902],\n",
       "         [   7, 1902],\n",
       "         [   8, 1902],\n",
       "         [   9, 1902],\n",
       "         [  10, 1902],\n",
       "         [  11, 1902],\n",
       "         [  12, 1902],\n",
       "         [   1, 1903],\n",
       "         [   2, 1903],\n",
       "         [   3, 1903],\n",
       "         [   4, 1903],\n",
       "         [   5, 1903],\n",
       "         [   6, 1903],\n",
       "         [   7, 1903],\n",
       "         [   8, 1903],\n",
       "         [   9, 1903],\n",
       "         [  10, 1903],\n",
       "         [  11, 1903],\n",
       "         [  12, 1903],\n",
       "         [   1, 1904],\n",
       "         [   2, 1904],\n",
       "         [   3, 1904],\n",
       "         [   4, 1904],\n",
       "         [   5, 1904],\n",
       "         [   6, 1904],\n",
       "         [   7, 1904],\n",
       "         [   8, 1904],\n",
       "         [   9, 1904],\n",
       "         [  10, 1904],\n",
       "         [  11, 1904],\n",
       "         [  12, 1904],\n",
       "         [   1, 1905],\n",
       "         [   2, 1905],\n",
       "         [   3, 1905],\n",
       "         [   4, 1905],\n",
       "         [   5, 1905],\n",
       "         [   6, 1905],\n",
       "         [   7, 1905],\n",
       "         [   8, 1905],\n",
       "         [   9, 1905],\n",
       "         [  10, 1905],\n",
       "         [  11, 1905],\n",
       "         [  12, 1905],\n",
       "         [   1, 1906],\n",
       "         [   2, 1906],\n",
       "         [   3, 1906],\n",
       "         [   4, 1906],\n",
       "         [   5, 1906],\n",
       "         [   6, 1906],\n",
       "         [   7, 1906],\n",
       "         [   8, 1906],\n",
       "         [   9, 1906],\n",
       "         [  10, 1906],\n",
       "         [  11, 1906],\n",
       "         [  12, 1906],\n",
       "         [   1, 1907],\n",
       "         [   2, 1907],\n",
       "         [   3, 1907],\n",
       "         [   4, 1907],\n",
       "         [   5, 1907],\n",
       "         [   6, 1907],\n",
       "         [   7, 1907],\n",
       "         [   8, 1907],\n",
       "         [   9, 1907],\n",
       "         [  10, 1907],\n",
       "         [  11, 1907],\n",
       "         [  12, 1907]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zeroshot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
