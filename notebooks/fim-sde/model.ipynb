{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from fim.data.dataloaders import (\n",
    "    DataLoaderFactory\n",
    ")\n",
    "from fim.utils.helper import (\n",
    "    GenericConfig, \n",
    "    expand_params, \n",
    "    load_yaml\n",
    ")\n",
    "\n",
    "from fim.data.datasets import FIMSDEDataset,FIMSDEDatabatchTuple\n",
    "from fim.data.config_dataclasses import FIMDatasetConfig\n",
    "from fim.models.blocks import ModelFactory\n",
    "from fim.models.config_dataclasses import FIMSDEConfig\n",
    "from fim.data.config_dataclasses import FIMDatasetConfig\n",
    "\n",
    "from fim.models.blocks.base import (\n",
    "    Mlp,\n",
    "    TimeEncoding,\n",
    "    TransformerModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Hypercube Size: 1024\n",
      "Max Dimension: 3\n",
      "Max Num Steps: 128\n",
      "Max Hypercube Size: 1024\n",
      "Max Dimension: 3\n",
      "Max Num Steps: 128\n",
      "Max Hypercube Size: 1024\n",
      "Max Dimension: 3\n",
      "Max Num Steps: 128\n"
     ]
    }
   ],
   "source": [
    "# Define model and data\n",
    "parameters_yaml = r\"C:\\Users\\cesar\\Desktop\\Projects\\FoundationModels\\FIM\\configs\\train\\fim-sde\\fim-train-patrick.yaml\"\n",
    "\n",
    "config = load_yaml(parameters_yaml,return_object=True)\n",
    "torch.manual_seed(int(config.experiment.seed))\n",
    "torch.cuda.manual_seed(int(config.experiment.seed))\n",
    "np.random.seed(int(config.experiment.seed))\n",
    "torch.cuda.empty_cache()\n",
    "device_map = config.experiment.device_map\n",
    "\n",
    "config = config.to_dict()\n",
    "dataloader = DataLoaderFactory.create(**config[\"dataset\"])\n",
    "if hasattr(dataloader,\"update_kwargs\"):\n",
    "    # fim model requieres that config is updated after loading the data\n",
    "    dataloader.update_kwargs(config)\n",
    "databatch:FIMSDEDatabatchTuple = next(dataloader.train_it.__iter__())\n",
    "model = ModelFactory.create(config,device_map=device_map,resume=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n  # phi_0^t\\n  temporal_embedding_size: &temporal_embedding_size 19\\n\\n  # phi_0^s\\n  spatial_embedding_size: &spatial_embedding_size 19\\n  spatial_embedding_hidden_layers: &spatial_embedding_hidden_layers null #  if null, this will just be dense layer\\n\\n  # psi_1\\n  sequence_encoding_transformer_hidden_size: &sequence_encoding_transformer_hidden_size 28 \\n  sequence_encoding_transformer_heads: &sequence_encoding_transformer_heads 2\\n  sequence_encoding_transformer_layers: &sequence_encoding_transformer_layers 2\\n\\n  # Omega_1\\n  combining_transformer_hidden_size: &combining_transformer_hidden_size 28 \\n  combining_transformer_heads: &combining_transformer_heads 2\\n  combining_transformer_layers: &combining_transformer_layers 1\\n\\n  # phi_1\\n  trunk_net_size: &trunk_net_size 28 \\n  trunk_net_hidden_layers: &trunk_net_hidden_layers null\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################################################################\n",
    "  # To change\n",
    "\"\"\"\n",
    "  # phi_0^t\n",
    "  temporal_embedding_size: &temporal_embedding_size 19\n",
    "\n",
    "  # phi_0^s\n",
    "  spatial_embedding_size: &spatial_embedding_size 19\n",
    "  spatial_embedding_hidden_layers: &spatial_embedding_hidden_layers null #  if null, this will just be dense layer\n",
    "\n",
    "  # psi_1\n",
    "  sequence_encoding_transformer_hidden_size: &sequence_encoding_transformer_hidden_size 28 \n",
    "  sequence_encoding_transformer_heads: &sequence_encoding_transformer_heads 2\n",
    "  sequence_encoding_transformer_layers: &sequence_encoding_transformer_layers 2\n",
    "\n",
    "  # Omega_1\n",
    "  combining_transformer_hidden_size: &combining_transformer_hidden_size 28 \n",
    "  combining_transformer_heads: &combining_transformer_heads 2\n",
    "  combining_transformer_layers: &combining_transformer_layers 1\n",
    "\n",
    "  # phi_1\n",
    "  trunk_net_size: &trunk_net_size 28 \n",
    "  trunk_net_hidden_layers: &trunk_net_hidden_layers null\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cesar\\anaconda3\\envs\\fim\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# definition of the model parameters\n",
    "model_params:FIMSDEConfig = model.model_params\n",
    "data_params:FIMDatasetConfig = model.data_params\n",
    "\n",
    "# Define different versions\n",
    "x_dimension = data_params.max_dimension\n",
    "x_dimension_full = x_dimension*3 # we encode the difference and its square\n",
    "spatial_plus_time_encoding = model_params.temporal_embedding_size + model_params.spatial_embedding_size\n",
    "psi_1_tokes_dim = model_params.sequence_encoding_tokenizer*model_params.sequence_encoding_transformer_heads\n",
    "\n",
    "# basic embedding\n",
    "phi_0t = TimeEncoding(model_params.temporal_embedding_size)\n",
    "phi_0x = Mlp(\n",
    "    in_features=x_dimension_full,\n",
    "    out_features=model_params.spatial_embedding_size,\n",
    "    hidden_layers=model_params.spatial_embedding_hidden_layers\n",
    ")\n",
    "\n",
    "# trunk network\n",
    "trunk = Mlp(\n",
    "    in_features=x_dimension,\n",
    "    out_features=psi_1_tokes_dim,\n",
    "    hidden_layers=model_params.trunk_net_hidden_layers\n",
    ")\n",
    "\n",
    "#ensures that the embbeding that is sent to the transformer is a multiple of the number of heads\n",
    "phi_xt = nn.Linear(spatial_plus_time_encoding,\n",
    "                   psi_1_tokes_dim)\n",
    "\n",
    "# path transformer (causal encoding of paths)\n",
    "psi_1 = TransformerModel(input_dim=psi_1_tokes_dim, \n",
    "                         nhead=model_params.sequence_encoding_transformer_heads, \n",
    "                         hidden_dim=model_params.sequence_encoding_transformer_hidden_size, \n",
    "                         nlayers=model_params.sequence_encoding_transformer_layers)\n",
    "\n",
    "# time attention\n",
    "omega_1 = nn.MultiheadAttention(\n",
    "    psi_1_tokes_dim, \n",
    "    model_params.combining_transformer_heads,\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "# path attention\n",
    "path_queries = nn.Parameter(torch.randn(1, psi_1_tokes_dim))\n",
    "\n",
    "omega_2 = nn.MultiheadAttention(\n",
    "    psi_1_tokes_dim, \n",
    "    model_params.combining_transformer_heads,\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "# drift_head =\n",
    "# var_drift_head =\n",
    "# diffusion_head = \n",
    "# var_diffusion_head = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_1_tokes_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_params.trunk_net_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_params.combining_transformer_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,P,T,D,_ = databatch.obs_values.shape\n",
    "G = databatch.locations.size(1)\n",
    "\n",
    "# include the square of the difference\n",
    "x_full = torch.concat([databatch.obs_values,databatch.obs_values[:,:,:,:,0].unsqueeze(-1)**2],dim=-1)\n",
    "x_flattened = x_full.view(x_full.shape[0], x_full.shape[1], x_full.shape[2], -1)\n",
    "spatial_encoding = phi_0x(x_flattened) # [B,P,T,spatial_embedding_size]\n",
    "time_encoding = phi_0t(databatch.obs_times) # [B,P,T,temporal_embedding_size]\n",
    "\n",
    "# trunk \n",
    "trunk_encoding = trunk(databatch.locations) #[B,H,trunk_dim]\n",
    "trunk_encoding = trunk_encoding[:,None,:,:].repeat(1,P,1,1)  # [B,P,H,trunk_size]\n",
    "trunk_encoding = trunk_encoding.view(B*P,G,-1)\n",
    "\n",
    "# embbedded input\n",
    "U =  torch.cat([spatial_encoding,time_encoding],dim=-1) #  [B,P,T,spatial_plus_time_encoding]\n",
    "U = phi_xt(U) #  [B,P,T,psi_1_tokes_dim] \n",
    "\n",
    "# TRANSFORMER THAT CREATES A REPRESENTATION FOR THE PATHS\n",
    "U = U.view(B*P,T,psi_1_tokes_dim)\n",
    "H = psi_1(torch.transpose(U,0,1))  # [T,B*P,psi_1_tokes_dim]\n",
    "H = torch.transpose(H,0,1) # [B*P,T,psi_1_tokes_dim]\n",
    "\n",
    "# Attention on Time -> One representation per path\n",
    "hx,_ = omega_1(trunk_encoding,H,H) # [B*P,H,psi_1_tokes_dim]\n",
    "hx = hx.view(B,P,G,-1) # [B,P,G,psi_1_tokes_dim]\n",
    "\n",
    "# Attention on Paths -> One representation per expression\n",
    "hx = hx.transpose(1,2).reshape(G*B,P,-1) # [B*G,P,psi_1_tokes_dim]\n",
    "path_queries_ = path_queries[None,:,:].repeat(G*B,1,1)\n",
    "bx,_ = omega_2(path_queries_,hx,hx)\n",
    "bx = bx.view(B,G,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#omega_1(trunk_encoding,H,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024, 10])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 300, 1024, 10])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600, 1024, 10])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunk_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 600, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape queries to match the attention requirements\n",
    "# H = self.psi1(torch.transpose(H,0,1)) # (seq_lenght,batch_size,encoding0_dim)\n",
    "\n",
    "# tx = tx.reshape(num_hyper, batch_size, self.encoding0_dim)  # Shape: (num_hyper, batch_size, encoding0_dim)\n",
    "\n",
    "# Representation per path\n",
    "# attn_output, _ = multihead_attn(queries[:,None,:].repeat(1,batch_size,1), H, H) # Shape: (1, batch_size, query_dim)\n",
    "#attn_output, _ = self.omega_1(tx, H, H) # Shape: (num_hyper, batch_size, query_dim)\n",
    "#attn_output = torch.transpose(attn_output,1,0) # Shape: (num_hyper, batch_size, query_dim)\n",
    "#attn_output = attn_output.reshape(num_hyper*batch_size,self.encoding0_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H  = H.reshape(batch_size,num_steps,self.encoding0_dim) \n",
    "#H = self.psi1(torch.transpose(H,0,1)) # (seq_lenght,batch_size,encoding0_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Defines all the nn Modules need for the architecture\\nself.encoding0_dim = params.dim_time + params.x0_out_features\\nself.phi_t0 = TimeEncoding(params.dim_time)\\n\\nself.phi_x0 = Mlp(in_features=params.max_dimension,\\n                    out_features=params.x0_out_features,\\n                    hidden_layers=params.x0_hidden_layers,\\n                    output_act=nn.SiLU())\\n\\nself.phi_1 = Mlp(in_features=params.max_dimension,\\n                    out_features=params.max_dimension,\\n                    hidden_layers=params.x0_hidden_layers)\\n\\nself.phi_2 = Mlp(in_features=self.encoding0_dim,\\n                    out_features=params.max_dimension,\\n                    hidden_layers=params.x0_hidden_layers)\\n\\nself.psi1 = TransformerModel(input_dim=self.encoding0_dim, \\n                                nhead=params.n_heads, \\n                                hidden_dim=params.psi1_hidden_dim, \\n                                nlayers=params.psi1_nlayers)\\n\\n#self.queries = nn.Parameter(torch.randn(1, params.encoding0_dim))\\nself.query_1x = QueryGenerator(input_dim=params.max_dimension,\\n                                query_dim=params.encoding0_dim)\\n\\nself.query_1 =  StaticQuery(num_steps=params.max_num_steps,\\n                    query_dim=params.encoding0_dim)\\n\\n# Create the MultiheadAttention module\\nself.omega_1 = nn.MultiheadAttention(self.encoding0_dim, params.n_heads)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Defines all the nn Modules need for the architecture\n",
    "self.encoding0_dim = params.dim_time + params.x0_out_features\n",
    "self.phi_t0 = TimeEncoding(params.dim_time)\n",
    "\n",
    "self.phi_x0 = Mlp(in_features=params.max_dimension,\n",
    "                    out_features=params.x0_out_features,\n",
    "                    hidden_layers=params.x0_hidden_layers,\n",
    "                    output_act=nn.SiLU())\n",
    "\n",
    "self.phi_1 = Mlp(in_features=params.max_dimension,\n",
    "                    out_features=params.max_dimension,\n",
    "                    hidden_layers=params.x0_hidden_layers)\n",
    "\n",
    "self.phi_2 = Mlp(in_features=self.encoding0_dim,\n",
    "                    out_features=params.max_dimension,\n",
    "                    hidden_layers=params.x0_hidden_layers)\n",
    "\n",
    "self.psi1 = TransformerModel(input_dim=self.encoding0_dim, \n",
    "                                nhead=params.n_heads, \n",
    "                                hidden_dim=params.psi1_hidden_dim, \n",
    "                                nlayers=params.psi1_nlayers)\n",
    "\n",
    "#self.queries = nn.Parameter(torch.randn(1, params.encoding0_dim))\n",
    "self.query_1x = QueryGenerator(input_dim=params.max_dimension,\n",
    "                                query_dim=params.encoding0_dim)\n",
    "\n",
    "self.query_1 =  StaticQuery(num_steps=params.max_num_steps,\n",
    "                    query_dim=params.encoding0_dim)\n",
    "\n",
    "# Create the MultiheadAttention module\n",
    "self.omega_1 = nn.MultiheadAttention(self.encoding0_dim, params.n_heads)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
