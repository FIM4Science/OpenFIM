experiment:
  name: DEBUG_fim_ode_noisy_MinMax-experiment-seed-10_08-23-1331
  name_add_date: False # if true, the current date & time will be added to the experiment name
  seed: [10]
  device_map: cpu # auto, cuda, cpu

distributed:
  enabled: false
  sharding_strategy: NO_SHARD # SHARD_GRAD_OP, NO_SHARD, HYBRID_SHARD
  wrap_policy: NO_POLICY # MODEL_SPECIFIC, SIZE_BAZED
  min_num_params: 1e5
  checkpoint_type: full_state # full_state, local_state
  activation_chekpoint: false

model:
  name: FIMODE
  load_in_8bit: false
  use_bf16: false

  loss_configs:
    ode_solver: rk4
    loss_scale_drift: 1.0
    loss_scale_init_cond: 1.0
    loss_scale_unsuperv_loss: 10.0

  normalization_time:
    name: fim.models.blocks.normalization.MinMaxNormalization

  normalization_values:
    name: fim.models.blocks.normalization.MinMaxNormalization

  time_encoding:
    name: fim.models.blocks.positional_encodings.SineTimeEncoding
    model_dim: &dim_time 16

  trunk_net:
    name: fim.models.blocks.base.MLP
    in_features: *dim_time
    out_features: &dim_latent 16
    hidden_layers: !!python/tuple [&hidden_dim_mlp 16]
    hidden_act:
      name: torch.nn.SELU
    output_act:
      name: torch.nn.Identity
    dropout: &dropout 0.1

  branch_net:
    name: fim.models.blocks.base.Transformer
    num_encoder_blocks: 1
    dim_model: *dim_latent
    dim_time: *dim_time
    num_heads: 1
    dropout: *dropout
    residual_mlp:
      name: fim.models.blocks.base.MLP
      in_features: *dim_latent
      out_features: *dim_latent
      hidden_layers: !!python/tuple [*hidden_dim_mlp]
      hidden_act:
        name: torch.nn.SELU
      output_act:
        name: torch.nn.Identity
      dropout: *dropout

  combiner_net:
    name: fim.models.blocks.base.MLP
    in_features: 32 # needs to be the sum of the output dimensions of trunk_net and branch_net => =2* latent_dim
    out_features: *dim_latent
    hidden_layers: !!python/tuple [*hidden_dim_mlp]
    hidden_act:
      name: torch.nn.SELU
    output_act:
      name: torch.nn.Identity
    dropout: *dropout

  vector_field_net:
    name: fim.models.blocks.base.MLP
    in_features: *dim_latent
    out_features: 2
    hidden_layers: !!python/tuple []
    hidden_act:
      name: torch.nn.SELU
    output_act:
      name: torch.nn.Identity
    dropout: *dropout

  init_cond_net:
    name: fim.models.blocks.base.MLP
    in_features: 17 # *dim_latent +1
    out_features: 2
    hidden_layers: !!python/tuple [*hidden_dim_mlp]
    hidden_act:
      name: torch.nn.SELU
    output_act:
      name: torch.nn.Identity
    dropout: *dropout

dataset:
  name: ts_torch_dataloader # base_dataloader
  path: /cephfs_projects/foundation_models/data/torch_2M_ode_chebyshev_max_deg_100_rbf_gp_2_5_and_2_10_length_128_avg_min_8/ #  /cephfs_projects/foundation_models/data/torch_2M_ode_chebyshev_max_deg_100_rbf_gp_2_5_and_2_10_length_128_avg_min_8/
  ds_name: ""
  batch_size: 16
  test_batch_size: 16
  output_fields: !!python/tuple [
      "fine_grid_grid",
      "fine_grid_concept_values",
      "fine_grid_sample_paths",
      "coarse_grid_grid",
      "coarse_grid_noisy_sample_paths", #  "coarse_grid_sample_paths"  or "coarse_grid_noisy_sample_paths"
      "coarse_grid_observation_mask",
    ]
  loader_kwargs:
    num_workers: 2
  # split: "train"
  dataset_name: fim.data.datasets.TimeSeriesDatasetTorch
  dataset_kwargs:
    debugging_data_range: 5

trainer:
  name: Trainer
  debug_iterations: null
  precision: bf16_mixed # null fp16 bf16 bf16_mixed fp16_mixed fp32_policy
  epochs: &epochs 5
  detect_anomaly: false
  save_every: 5
  gradient_accumulation_steps: 1
  best_metric: loss
  logging_format: "RANK_%(rank)s - %(asctime)s - %(name)s - %(levelname)s - %(message)s"
  experiment_dir: ./results/
  schedulers: !!python/tuple
    - name: fim.utils.param_scheduler.PeriodicScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: beta_scheduler
    - name: fim.utils.param_scheduler.ExponentialSchedulerGumbel
      label: temperature_scheduler
      init_temperature: 1
      min_temperature: 0.5
      training_fraction_to_reach_min: 0.7

optimizers: !!python/tuple
  - optimizer_d: # name of the optimizer
      name: torch.optim.Adam # AdamW
      lr: 0.00001
      weight_decay: 0.001
      gradient_norm_clipping: 10
      schedulers: !!python/tuple
        - name: torch.optim.lr_scheduler.CosineAnnealingLR
          T_max: *epochs
          eta_min: 0.0000001
          last_epoch: -1
