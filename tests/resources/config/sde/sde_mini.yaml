experiment:
  name: sde
  name_add_date: False # if true, the current date & time will be added to the experiment name
  seed: 10
  device_map: cpu # auto, cuda, cpu

distributed:
  enabled: false
  sharding_strategy: NO_SHARD # SHARD_GRAD_OP, NO_SHARD, HYBRID_SHARD
  wrap_policy: SIZE_BAZED # NO_POLICY # MODEL_SPECIFIC, SIZE_BAZED
  min_num_params: 1e5
  checkpoint_type: full_state # full_state, local_state
  activation_chekpoint: false

model:
  model_type: fimsde
  ###################################################################################
  # Input data (corrected by dataloader)
  max_dimension: &max_dimension 3
  # max_time_steps: &max_time_steps 128
  # max_location_size: &max_location_size 1024
  # max_num_paths: &max_num_of_paths 300

  # General model config
  non_negative_diffusion_by: null
  model_embedding_size: &model_embedding_size 8

  # General transformer config
  residual_ff_size: &residual_ff_size 4
  transformer_layer_activation: &transformer_layer_activation "gelu"

  # General MLP config
  hidden_layers: &hidden_layers !!python/tuple [4, 4]
  dropout_rate: &dropout_rate 0.1
  hidden_act: &hidden_act
    name: &activation torch.nn.GELU

  # Networks
  layer_norms_in_phi_0: false
  separate_phi_0_encoders: true
  delta_time_only: true

  phi_0t:
    name: fim.models.sde.SineTimeEncoding

  phi_0x:
    name: torch.nn.Linear

  psi_1:
    name: "CombinedPathTransformer"
    num_layers: 2
    layer:
      attn_method: "linear"
      lin_feature_map: "softmax"
      lin_normalize: true
      nhead: 2
      dim_feedforward: *residual_ff_size
      dropout: *dropout_rate
      activation: *activation

  phi_1x:
    name: torch.nn.Linear

  learn_vf_var: false
  operator_specificity: "per_concept" # all, per_concept, per_head

  operator:
    paths_block_attention: false
    num_res_layers: 2
    attention:
      nhead: 2
      dim_feedforward: *residual_ff_size
      dropout: *dropout_rate
      activation: *activation

    projection:
      name: fim.models.blocks.base.MLP
      hidden_layers: *hidden_layers
      hidden_act: *hidden_act
      dropout: *dropout_rate

  # instance normalization
  states_norm:
    name: fim.models.sde.Standardization

  times_norm:
    name: fim.models.sde.MinMaxNormalization
    normalized_min: 0
    normalized_max: 1

  # loss regularization
  train_with_normalized_head: true
  loss_filter_nans: true

  # loss
  loss_type: "nrmse" #nll, rmse, nrmse

  # INFERENCE/PIPELINE ------------------------------------------------------------
  dt_pipeline: 0.1
  number_of_time_steps_pipeline: 128
  evaluate_with_unnormalized_heads: true

dataset:
  name: FIMSDEDataloaderIterableDataset

  max_dim: *max_dimension

  dataset_name:
    train: &train_set_name PaddedFIMSDEDataset # PaddedFIMSDEDataset, HeterogeneousFIMSDEDataset, StreamingFIMSDEDataset
    validation: *train_set_name
    test: PaddedFIMSDEDataset

  batch_size: 2

  shuffle_locations:
    train: true
    validation: true
    test: false
  shuffle_paths: true
  shuffle_elements: true # only for PaddedFIMSDEDataset and HeterogeneousFIMSDEDataset

  num_locations:
    train: 1
    validation: 1
    test: null

  num_observations:
    train: !!python/tuple [256, 512]
    validation: null
    test: null

  num_workers: 4

  files_to_load:
    obs_times: "obs_times.h5"
    obs_values: "obs_values.h5"
    locations: "locations.h5"
    drift_at_locations: "drift_at_locations.h5"
    diffusion_at_locations: "diffusion_at_locations.h5"

  data_dirs:
    train:
      - tests/resources/data/sde/train/0_test_resources_train_deg_1
      - tests/resources/data/sde/train/1_test_resources_train_deg_2
      - tests/resources/data/sde/train/2_test_resources_train_deg_3
    test:
      - tests/resources/data/sde/test/0_test_resources_test_deg_1
      - tests/resources/data/sde/test/1_test_resources_test_deg_2
      - tests/resources/data/sde/test/2_test_resources_test_deg_3
    validation:
      - tests/resources/data/sde/validation/0_test_resources_validation_deg_1
      - tests/resources/data/sde/validation/1_test_resources_validation_deg_2
      - tests/resources/data/sde/validation/2_test_resources_validation_deg_3


trainer:
  name: Trainer
  debug_iterations: 2
  precision: bf16 # null fp16 bf16 bf16_mixed fp16_mixed fp32_policy
  epochs: &epochs 2
  detect_anomaly: false
  save_every: 1
  gradient_accumulation_steps: 1
  best_metric: loss
  logging_format: "RANK_%(rank)s - %(asctime)s - %(name)s - %(levelname)s - %(message)s"
  experiment_dir: ./results/
  evaluation_epoch:
    path: fim.trainers.evaluation_epochs.TestEvaluationEpoch
  schedulers: !!python/tuple
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      beta: 1.0
      label: gauss_nll
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: init_cross_entropy
      beta: 1.0
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: missing_link
      beta: 1.0

optimizers: !!python/tuple
  - optimizer_d: # name of the optimizer
      name: torch.optim.AdamW
      lr: 0.0001
      weight_decay: 0.0001
      # gradient_norm_clipping: 10
      # schedulers: !!python/tuple
      #   - name: torch.optim.lr_scheduler.CosineAnnealingLR
      #     T_max: *epochs
      #     eta_min: 0.0000001
      #     last_epoch: -1
