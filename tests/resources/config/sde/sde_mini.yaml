experiment:
  name: sde
  name_add_date: False # if true, the current date & time will be added to the experiment name
  seed: 10
  device_map: cpu # auto, cuda, cpu

distributed:
  enabled: false
  sharding_strategy: NO_SHARD # SHARD_GRAD_OP, NO_SHARD, HYBRID_SHARD
  wrap_policy: SIZE_BAZED # NO_POLICY # MODEL_SPECIFIC, SIZE_BAZED
  min_num_params: 1e5
  checkpoint_type: full_state # full_state, local_state
  activation_chekpoint: false

model:
  name: FIMSDE
  ###################################################################################
  # Input data (corrected by dataloader)
  max_dimension: &max_dimension 0
  max_time_steps: &max_time_steps 0
  max_location_size: &max_location_size 0
  max_num_paths: &max_num_of_paths 0

  # General model config
  non_negative_diffusion_by: "clip"
  model_embedding_size: &model_embedding_size 16

  # General transformer config
  residual_ff_size: &residual_ff_size 16
  transformer_layer_activation: &transformer_layer_activation "relu"

  # General MLP config
  hidden_layers: &hidden_layers !!python/tuple [16, 16]
  dropout_rate: &dropout_rate 0.1
  hidden_act: &hidden_act
    name: &activation torch.nn.ReLU

  # Networks
  phi_0x:
    hidden_layers: *hidden_layers
    hidden_act: *hidden_act
    dropout: *dropout_rate

  phi_0_combination: "add" # concatenate, all

  psi_1_transformer_layer:
    nhead: 2
    dim_feedforward: *residual_ff_size
    dropout: *dropout_rate
    activation: *transformer_layer_activation

  psi_1_transformer_encoder:
    num_layers: 2

  phi_1x:
    hidden_layers: *hidden_layers
    hidden_act: *hidden_act
    dropout: *dropout_rate

  operator_specificity: "per_head" # all, per_concept, per_head

  operator:
    attention:
      locations_as_final_query: false
      nhead: 2
      dim_feedforward: *residual_ff_size
      dropout: *dropout_rate
      activation: *activation

    projection:
      hidden_layers: *hidden_layers
      hidden_act: *hidden_act
      dropout: *dropout_rate


  # instance normalization
  values_norm_min: -1
  values_norm_max: 1
  times_norm_min: 0
  times_norm_max: 1

  # loss regularization
  add_delta_x_to_value_encoder: true
  diffusion_loss_scale: 0.01
  train_with_normalized_head: true
  loss_threshold: 100.0
  loss_filter_nans: true

  # loss
  loss_type: "nll" #nll, rmse
  clip_grad: true
  clip_max_norm: 10.

  # INFERENCE/PIPELINE ------------------------------------------------------------
  dt_pipeline: 0.1
  number_of_time_steps_pipeline: 128
  evaluate_with_unnormalized_heads: true

dataset:
  # data loading
  name: FIMSDEDataloader
  dataset_description: SDE_linear_SNR_01_05_1_5_DELTA_3D

  # dataset config
  data_type: synthetic # synthetic, theory

  data_in_files: &data_in_files
    obs_times: "obs_times.h5"
    obs_values: "obs_values.h5"
    locations: "hypercube_locations.h5"
    drift_at_locations: "drift_functions_at_hypercube.h5"
    diffusion_at_locations: "scaled_diffusion_functions_at_hypercube.h5"

  data_paths:
    train:
      - tests/resources/data/sde/state_sde/data-snr_01_05_1_5/linear/dim-1/1
      - tests/resources/data/sde/state_sde/data-snr_01_05_1_5/linear/dim-2/1
      - tests/resources/data/sde/state_sde/data-snr_01_05_1_5/linear/dim-3/1
    test:
      - tests/resources/data/sde/state_sde/data-snr_01_05_1_5/linear/dim-2/1
    validation:
      - tests/resources/data/sde/state_sde/data-snr_01_05_1_5/linear/dim-2/1


  # dataloader config
  batch_size:
    train: 16
    test: 4
    validation: 4

  random_grids:
    train: true
    test: false
    validation: false

  min_num_grid_points: 2
  max_num_grid_points: 10

  random_paths: true
  min_num_paths: 1
  max_num_paths: 5




trainer:
  name: Trainer
  debug_iterations: 3
  precision: bf16 # null fp16 bf16 bf16_mixed fp16_mixed fp32_policy
  epochs: &epochs 3
  detect_anomaly: false
  save_every: 1
  gradient_accumulation_steps: 1
  best_metric: loss
  logging_format: "RANK_%(rank)s - %(asctime)s - %(name)s - %(levelname)s - %(message)s"
  experiment_dir: ./results/
  evaluation_epoch:
    path: fim.trainers.evaluation_epochs.TestEvaluationEpoch
  schedulers: !!python/tuple
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      beta: 1.0
      label: gauss_nll
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: init_cross_entropy
      beta: 1.0
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: missing_link
      beta: 1.0

optimizers: !!python/tuple
  - optimizer_d: # name of the optimizer
      name: torch.optim.AdamW
      lr: 0.0001
      weight_decay: 0.0001
      # gradient_norm_clipping: 10
      # schedulers: !!python/tuple
      #   - name: torch.optim.lr_scheduler.CosineAnnealingLR
      #     T_max: *epochs
      #     eta_min: 0.0000001
      #     last_epoch: -1
