experiment:
  name: sde
  name_add_date: False # if true, the current date & time will be added to the experiment name
  seed: 10
  device_map: cpu # auto, cuda, cpu

distributed:
  enabled: false
  sharding_strategy: NO_SHARD # SHARD_GRAD_OP, NO_SHARD, HYBRID_SHARD
  wrap_policy: SIZE_BAZED # NO_POLICY # MODEL_SPECIFIC, SIZE_BAZED
  min_num_params: 1e5
  checkpoint_type: full_state # full_state, local_state
  activation_chekpoint: false

model:
  name: FIMSDE
  ###################################################################################
  # Input data (corrected by dataloader)
  max_dimension: &max_dimension 0
  max_time_steps: &max_time_steps 0
  max_location_size: &max_location_size 0
  max_num_paths: &max_num_of_paths 0

  # Model
  # phi_0^t
  temporal_embedding_size: &temporal_embedding_size 21

  # phi_0^s
  spatial_embedding_size: &spatial_embedding_size 19
  spatial_embedding_hidden_layers: &spatial_embedding_hidden_layers [50] #  if null, this will just be dense layer

  # psi_1
  sequence_encoding_tokenizer: &sequence_encoding_tokenizer 10
  sequence_encoding_transformer_hidden_size: &sequence_encoding_transformer_hidden_size 28
  sequence_encoding_transformer_heads: &sequence_encoding_transformer_heads 1
  sequence_encoding_transformer_layers: &sequence_encoding_transformer_layers 1

  # Omega_1
  combining_transformer_hidden_size: &combining_transformer_hidden_size 28
  combining_transformer_heads: &combining_transformer_heads 2
  combining_transformer_layers: &combining_transformer_layers 1

  # phi_1
  trunk_net_size: &trunk_net_size 28
  trunk_net_hidden_layers: &trunk_net_hidden_layers [25]

  # instance normalization
  values_norm_min: -1
  values_norm_max: 1
  times_norm_min: 0
  times_norm_max: 1

  # training
  # optimizer + regularising
  num_epochs: 2
  add_delta_x_to_value_encoder: &add_delta_x_to_value_encoder true
  log_images_every_n_epochs: 2
  learning_rate: &learning_rate 1.0e-5
  weight_decay: &weight_decay 1.0e-4
  dropout_rate: &dropout_rate 0.1

  # loss
  loss_type: "rmse" #var, rmse
  clip_grad: true
  clip_max_norm: 10.

  # loss regularization
  diffusion_loss_scale: 1.0
  train_with_normalized_head: true
  loss_threshold: &loss_threshold 100.0
  loss_filter_nans: True

  # INFERENCE/PIPELINE ------------------------------------------------------------
  dt_pipeline: 0.01
  number_of_time_steps_pipeline: 128
  evaluate_with_unnormalized_heads: True

dataset:
  # data loading
  name: FIMSDEDataloader
  dataset_description: SDE_linear_SNR_01_05_1_5_DELTA_3D

  # dataset config
  data_type: synthetic # synthetic, theory

  data_in_files: &data_in_files
    obs_times: "obs_times.h5"
    obs_values: "obs_values.h5"
    locations: "hypercube_locations.h5"
    drift_at_locations: "drift_functions_at_hypercube.h5"
    diffusion_at_locations: "scaled_diffusion_functions_at_hypercube.h5"

  data_paths:
    train:
      - tests/resources/data/sde/state_sde/data-snr_01_05_1_5/linear/dim-1/1
      - tests/resources/data/sde/state_sde/data-snr_01_05_1_5/linear/dim-2/1
      - tests/resources/data/sde/state_sde/data-snr_01_05_1_5/linear/dim-3/1
    test:
      - tests/resources/data/sde/state_sde/data-snr_01_05_1_5/linear/dim-2/1
    validation:
      - tests/resources/data/sde/state_sde/data-snr_01_05_1_5/linear/dim-2/1


  # dataloader config
  batch_size:
    train: 16
    test: 4
    validation: 4

  random_grids:
    train: true
    test: false
    validation: false

  min_num_grid_points: 2
  max_num_grid_points: 10

  random_paths: true
  min_num_paths: 1
  max_num_paths: 5




trainer:
  name: Trainer
  debug_iterations: 3
  precision: bf16 # null fp16 bf16 bf16_mixed fp16_mixed fp32_policy
  epochs: &epochs 3
  detect_anomaly: false
  save_every: 1
  gradient_accumulation_steps: 1
  best_metric: loss
  logging_format: "RANK_%(rank)s - %(asctime)s - %(name)s - %(levelname)s - %(message)s"
  experiment_dir: ./results/
  evaluation_epoch:
    path: fim.trainers.evaluation_epochs.TestEvaluationEpoch
  schedulers: !!python/tuple
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      beta: 1.0
      label: gauss_nll
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: init_cross_entropy
      beta: 1.0
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: missing_link
      beta: 1.0

optimizers: !!python/tuple
  - optimizer_d: # name of the optimizer
      name: torch.optim.AdamW
      lr: 0.0001
      weight_decay: 0.0001
      # gradient_norm_clipping: 10
      # schedulers: !!python/tuple
      #   - name: torch.optim.lr_scheduler.CosineAnnealingLR
      #     T_max: *epochs
      #     eta_min: 0.0000001
      #     last_epoch: -1
