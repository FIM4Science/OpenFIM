method: lora
r: 8
lora_alpha: 16
lora_dropout: 0.05
# Adjust target modules to match Hawkes model layers
# You can refine these if needed.
target_modules:
  - functional_attention
  - context_self_attn
  - W_q
  - W_k
  - W_v
  - W_o
bias: none
task_type: SEQ_CLS
