experiment:
  name: 30K_deg_3_drift_ablation_delta_tau_1e-1_to_1e-3
  name_add_date: true # if true, the current date & time will be added to the experiment name
  seed: 10
  device_map: cuda # auto, cuda, cpu

distributed:
  enabled: false # true for multi gpu training
  sharding_strategy: NO_SHARD # SHARD_GRAD_OP, NO_SHARD (data parallelism), HYBRID_SHARD
  wrap_policy: SIZE_BAZED # NO_POLICY # MODEL_SPECIFIC, SIZE_BAZED (decides how to split parameters along GPUs)
  min_num_params: 1e5
  checkpoint_type: full_state # full_state, local_state
  activation_chekpoint: false

model:
  model_type: fimsde
  ###################################################################################
  # Input data (corrected by dataloader)
  max_dimension: &max_dimension 3

  # General model config
  non_negative_diffusion_by: softplus
  model_embedding_size: &model_embedding_size 128

  # General transformer config
  residual_ff_size: &residual_ff_size 512
  transformer_layer_activation: &transformer_layer_activation "gelu"

  # General MLP config
  hidden_layers: &hidden_layers !!python/tuple [128, 128]
  dropout_rate: &dropout_rate 0.1
  hidden_act: &hidden_act
    name: &activation torch.nn.GELU

  # Networks
  layer_norms_in_phi_0: false
  separate_phi_0_encoders: true
  delta_time_only: true

  phi_0t:
    name: torch.nn.Linear

  phi_0x:
    name: torch.nn.Linear

  psi_1:
    name: "CombinedPathTransformer"
    num_layers: 2
    layer:
      attn_method: "linear"
      lin_feature_map: "softmax"
      lin_normalize: true
      nhead: 4
      dim_feedforward: *residual_ff_size
      dropout: *dropout_rate
      activation: *activation

  phi_1x:
    name: torch.nn.Linear

  learn_vf_var: false
  operator_specificity: "per_concept" # all, per_concept, per_head

  operator: &operator
    paths_block_attention: false
    num_res_layers: 4
    attention:
      nhead: 4
      dim_feedforward: *residual_ff_size
      dropout: *dropout_rate
      activation: *activation

    projection:
      name: fim.models.blocks.base.MLP
      hidden_layers: *hidden_layers
      hidden_act: *hidden_act
      dropout: *dropout_rate


  # instance normalization
  states_norm:
    name: fim.models.sde.Standardization

  times_norm_on_deltas: True
  times_norm:
    name: fim.models.sde.DeltaLogCentering

  # loss regularization
  train_with_normalized_head: true
  loss_filter_nans: true

  # learnable_loss_scales: *operator
  learnable_loss_scales:
    paths_block_attention: false
    num_res_layers: 4
    attention:
      nhead: 4
      dim_feedforward: *residual_ff_size
      dropout: *dropout_rate
      activation: *activation

    projection:
      name: fim.models.blocks.base.MLP
      hidden_layers: *hidden_layers
      hidden_act: *hidden_act
      dropout: *dropout_rate


  # loss
  use_kl: false
  loss_type: "mse" #nll, rmse, nrmse
  single_learnable_loss_scale_head: true
  detach_learnable_loss_scale_heads: true
  data_delta_t: 0.003906  # 10 / 128 / 20


  # INFERENCE/PIPELINE ------------------------------------------------------------
  dt_pipeline: 0.1
  number_of_time_steps_pipeline: 128
  evaluate_with_unnormalized_heads: true

dataset:
  name: FIMSDEDataloaderIterableDataset

  max_dim: 3

  dataset_name:
    train: &train_set_name StreamingFIMSDEDataset # PaddedFIMSDEDataset, HeterogeneousFIMSDEDataset, StreamingFIMSDEDataset
    validation: *train_set_name
    test: HeterogeneousFIMSDEDataset

  batch_size:
    train: 32
    validation: 32
    test: 128

  shuffle_locations:
    train: true
    validation: true
    test: false
  shuffle_paths: true
  shuffle_elements: true # only for PaddedFIMSDEDataset and HeterogeneousFIMSDEDataset

  num_locations:
    train: 32
    validation: 128
    test: null

  num_observations:
    train: !!python/tuple [128, 6400]
    validation: null
    test: null

  num_workers:
    train: 8
    validation: 2
    test: 0

  files_to_load:
    obs_times: "obs_times.h5"
    obs_values: "obs_noisy_values.h5"
    locations: "locations.h5"
    drift_at_locations: "drift_at_locations.h5"
    diffusion_at_locations: "diffusion_at_locations.h5"

  data_dirs:
    train: !!python/tuple [
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/train/train_deg_1/delta_tau_1e-1_100_paths_length_128_num_realiz_3333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/train/train_deg_1/delta_tau_1e-2_25_paths_length_512_num_realiz_3333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/train/train_deg_1/delta_tau_1e-3_12_paths_length_1024_num_realiz_3333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/train/train_deg_2/delta_tau_1e-1_100_paths_length_128_num_realiz_3333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/train/train_deg_2/delta_tau_1e-2_25_paths_length_512_num_realiz_3333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/train/train_deg_2/delta_tau_1e-3_12_paths_length_1024_num_realiz_3333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/train/train_deg_3/delta_tau_1e-1_100_paths_length_128_num_realiz_3333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/train/train_deg_3/delta_tau_1e-2_25_paths_length_512_num_realiz_3333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/train/train_deg_3/delta_tau_1e-3_12_paths_length_1024_num_realiz_3333,]
    test: !!python/tuple [
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/test/test_deg_1/delta_tau_1e-1_100_paths_length_128_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/test/test_deg_1/delta_tau_1e-2_25_paths_length_512_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/test/test_deg_1/delta_tau_1e-3_12_paths_length_1024_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/test/test_deg_2/delta_tau_1e-1_100_paths_length_128_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/test/test_deg_2/delta_tau_1e-2_25_paths_length_512_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/test/test_deg_2/delta_tau_1e-3_12_paths_length_1024_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/test/test_deg_3/delta_tau_1e-1_100_paths_length_128_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/test/test_deg_3/delta_tau_1e-2_25_paths_length_512_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/test/test_deg_3/delta_tau_1e-3_12_paths_length_1024_num_realiz_333,]
    validation: !!python/tuple [
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/validation/val_deg_1/delta_tau_1e-1_100_paths_length_128_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/validation/val_deg_1/delta_tau_1e-2_25_paths_length_512_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/validation/val_deg_1/delta_tau_1e-3_12_paths_length_1024_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/validation/val_deg_2/delta_tau_1e-1_100_paths_length_128_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/validation/val_deg_2/delta_tau_1e-2_25_paths_length_512_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/validation/val_deg_2/delta_tau_1e-3_12_paths_length_1024_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/validation/val_deg_3/delta_tau_1e-1_100_paths_length_128_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/validation/val_deg_3/delta_tau_1e-2_25_paths_length_512_num_realiz_333,
     /lustre/scratch/data/seifnerp_hpc-fim_data/data/processed/train/30k_drift_deg_3_ablation_studies/delta_tau_1e-1_to_1e-3/validation/val_deg_3/delta_tau_1e-3_12_paths_length_1024_num_realiz_333,]


trainer:
  name: Trainer
  debug_iterations: null
  precision: bf16_mixed # null fp16 bf16 bf16_mixed fp16_mixed fp32_policy (stick to bf16, maybe try bf16_mixed for more accurate gradients)
  epochs: &epochs 1000
  detect_anomaly: false
  save_every: 25
  gradient_accumulation_steps: 1
  best_metric: loss
  logging_format: "RANK_%(rank)s - %(asctime)s - %(name)s - %(levelname)s - %(message)s"
  experiment_dir: ./results/
  # profiler:
  #   num_batches: 40
  #   trace_path: "/home/seifner/repos/FIM/results/profiler/trace.json"

  evaluation_epoch:
    path: fim.trainers.evaluation_epochs.SDEEvaluationPlots
    plot_frequency: 5
    plot_paths_count: 50

  schedulers: !!python/tuple
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: loss_threshold
      beta: 1000000000.0

    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: vector_field_max_norm
      beta: 1000000000.0

    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: diffusion_loss_scale
      beta: 1.0

optimizers: !!python/tuple
  - optimizer_d: # name of the optimizer
      name: torch.optim.AdamW
      lr: &lr_rate 1.0e-4
      weight_decay: 0
