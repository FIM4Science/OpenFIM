experiment:
  name: "latent_sde_lorenz_github_setup_base"
  name_add_date: true # if true, the current date & time will be added to the experiment name
  seed: 10
  device_map: cuda # auto, cuda, cpu

distributed:
  enabled: false # true for multi gpu training
  sharding_strategy: NO_SHARD # SHARD_GRAD_OP, NO_SHARD (data parallelism), HYBRID_SHARD
  wrap_policy: SIZE_BAZED # NO_POLICY # MODEL_SPECIFIC, SIZE_BAZED (decides how to split parameters along GPUs)
  min_num_params: 1e5
  checkpoint_type: full_state # full_state, local_state
  activation_chekpoint: false

model:
  model_type: &model_type latentsde

  data_size: 3

  hidden_size: 128 # paper: 100, github: 128
  context_size: 64 # paper: 1, github: 64
  latent_size: 4 # paper: 4, github: 4
  activation: "softplus" # paper: sigmoid, github: softplus

  learn_projection: True

  solver_adjoint: False
  solver_method: "euler"
  solver_dt: 0.02 # paper: 0.01, github: 0.02

  noise_std: 0.01 # paper: 0.01, github: 0.01

dataset:
  name: FIMSDEDataloaderIterableDataset

  max_dim: 3

  dataset_name:
    train:  JsonSDEDataset
    validation: JsonSDEDataset
    test: JsonSDEDataset

  batch_size:
    train: 1024
    validation: 128
    test: 128

  num_workers: 1

  keys_to_load:
    obs_times: "obs_grid"
    obs_values: "noisy_obs_values"

  json_paths:
    train: !!python/tuple [
      /home/seifner/repos/FIM/data/processed/test/20250629_lorenz_system_with_vector_fields_at_locations/neural_sde_paper/set_0/train_data.json
    ]

    test: !!python/tuple [
      /home/seifner/repos/FIM/data/processed/test/20250629_lorenz_system_with_vector_fields_at_locations/neural_sde_paper/set_0/validation_data.json
    ]

    validation: !!python/tuple [
      /home/seifner/repos/FIM/data/processed/test/20250629_lorenz_system_with_vector_fields_at_locations/neural_sde_paper/set_0/validation_data.json
    ]


trainer:
  name: Trainer
  debug_iterations: null
  precision: null # null fp16 bf16 bf16_mixed fp16_mixed fp32_policy (stick to bf16, maybe try bf16_mixed for more accurate gradients)
  epochs: &epochs 5000
  detect_anomaly: false
  save_every: 10
  gradient_accumulation_steps: 1
  best_metric: loss
  logging_format: "RANK_%(rank)s - %(asctime)s - %(name)s - %(levelname)s - %(message)s"
  experiment_dir: ./results/
  # profiler:
  #   num_batches: 40
  #   trace_path: "/home/seifner/repos/FIM/results/profiler/trace.json"

  evaluation_epoch:
    path: fim.trainers.evaluation_epochs.LorenzEvaluationEpoch
    plot_frequency: 20
    model_type: *model_type

  schedulers: !!python/tuple
    - name: fim.utils.param_scheduler.LinearScheduler
      label: kl_scale
      start_step: 0
      end_step: 1000 # paper: 50, github: 1000
      start_value: 0
      end_value: 1


optimizers: !!python/tuple
  - optimizer_d:
      name: torch.optim.Adam
      lr: &lr_rate 1.0e-2 # paper: 0.01, github: 0.01
      schedulers: !!python/tuple
        - name: torch.optim.lr_scheduler.SequentialLR
          step_type: minibatch
          schedulers: !!python/tuple
            - name: torch.optim.lr_scheduler.ExponentialLR
              gamma: 0.997 # paper: 0.999, github: 0.997
          milestones: !!python/tuple []
