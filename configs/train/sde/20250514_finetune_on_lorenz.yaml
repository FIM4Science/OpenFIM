experiment:
  name: finetune_fim_05-03-2033_epoch_139_on_128_constant_diffusion_lorenz_paths_with_likelihood_on_all_points_lr_1e-6

  name_add_date: true # if true, the current date & time will be added to the experiment name
  seed: 10
  device_map: cuda # auto, cuda, cpu

distributed:
  enabled: true # true for multi gpu training
  sharding_strategy: NO_SHARD # SHARD_GRAD_OP, NO_SHARD (data parallelism), HYBRID_SHARD
  wrap_policy: SIZE_BAZED # NO_POLICY # MODEL_SPECIFIC, SIZE_BAZED (decides how to split parameters along GPUs)
  min_num_params: 1e5
  checkpoint_type: full_state # full_state, local_state
  activation_chekpoint: false

model:
  model_type: fimsde
  ###################################################################################
  # Input data (corrected by dataloader)
  max_dimension: &max_dimension 3
  # max_time_steps: &max_time_steps 128
  # max_location_size: &max_location_size 1024
  # max_num_paths: &max_num_of_paths 300

  # finetune config
  finetune: true
  finetune_only_on_one_step_ahead: true
  finetune_samples_count: 1
  finetune_em_steps: 1
  finetune_detach_diffusion: false
  finetune_on_likelihood: true

  # General model config
  non_negative_diffusion_by: softplus
  model_embedding_size: &model_embedding_size 256

  # General transformer config
  residual_ff_size: &residual_ff_size 1024
  transformer_layer_activation: &transformer_layer_activation "gelu"

  # General MLP config
  hidden_layers: &hidden_layers !!python/tuple [256, 256]
  dropout_rate: &dropout_rate 0.1
  hidden_act: &hidden_act
    name: &activation torch.nn.GELU

  # Networks
  layer_norms_in_phi_0: false
  separate_phi_0_encoders: true
  delta_time_only: true

  phi_0t:
    name: torch.nn.Linear

  phi_0x:
    name: torch.nn.Linear

  psi_1:
    name: "CombinedPathTransformer"
    num_layers: 2
    layer:
      attn_method: "linear"
      lin_feature_map: "softmax"
      lin_normalize: false
      nhead: &nhead 8
      dim_feedforward: *residual_ff_size
      dropout: *dropout_rate
      activation: *activation

  phi_1x:
    name: torch.nn.Linear

  learn_vf_var: false
  operator_specificity: "per_concept" # all, per_concept, per_head

  operator: &operator
    paths_block_attention: false
    num_res_layers: 8
    attention:
      nhead: *nhead
      dim_feedforward: *residual_ff_size
      dropout: *dropout_rate
      activation: *activation

    projection:
      name: fim.models.blocks.base.MLP
      hidden_layers: *hidden_layers
      hidden_act: *hidden_act
      dropout: *dropout_rate


  # instance normalization
  states_norm:
    name: fim.models.sde.Standardization

  times_norm_on_deltas: True
  times_norm:
    name: fim.models.sde.DeltaLogCentering

  # loss regularization
  train_with_normalized_head: true
  loss_filter_nans: true

  # learnable_loss_scales: *operator
  learnable_loss_scales:
    paths_block_attention: false
    num_res_layers: 8
    attention:
      nhead: *nhead
      dim_feedforward: *residual_ff_size
      dropout: *dropout_rate
      activation: *activation

    projection:
      name: fim.models.blocks.base.MLP
      hidden_layers: *hidden_layers
      hidden_act: *hidden_act
      dropout: *dropout_rate


  # loss
  loss_type: "mse" #nll, rmse, nrmse
  divide_drift_loss_by_diffusion: false
  single_learnable_loss_scale_head: true
  detach_learnable_loss_scale_heads: true


  # INFERENCE/PIPELINE ------------------------------------------------------------
  dt_pipeline: 0.1
  number_of_time_steps_pipeline: 128
  evaluate_with_unnormalized_heads: true

dataset:
  name: FIMSDEDataloaderIterableDataset

  max_dim: 3

  dataset_name:
    train: &train_set_name StreamingFIMSDEDataset # PaddedFIMSDEDataset, HeterogeneousFIMSDEDataset, StreamingFIMSDEDataset
    validation: *train_set_name
    test: HeterogeneousFIMSDEDataset

  batch_size:
    train: &train_batch_size 43
    validation: *train_batch_size
    test: 43

  shard:
    train: true
    validation: true
    test: false

  # data_limit:
  #   train: 10
  #   validation: 10
  #   test: 10

  shuffle_locations:
    train: false
    validation: false
    test: false

  shuffle_paths: false
  shuffle_elements: false # only for PaddedFIMSDEDataset and HeterogeneousFIMSDEDataset

  num_locations:
    train: 32
    validation: 32
    test: null

  num_observations:
    train: null #&train_num_obs !!python/tuple [125, 126]
    validation: null #*train_num_obs
    test: null

  num_workers:
    train: 4
    validation: 2
    test: 0

  files_to_load:
    obs_times: "obs_times.h5"
    obs_values: "obs_values.h5"
    # obs_times: "obs_grid_test.h5"
    # obs_values: "obs_values_test.h5" # for clean data, obs_noisy_values is obs_values
    # obs_mask: "observation_mask.h5"
    # locations: "locations.h5"
    # drift_at_locations: "drift_at_locations.h5"
    # diffusion_at_locations: "diffusion_at_locations.h5"

  data_dirs:
    train: !!python/tuple [
      "/home/seifner/repos/FIM/data/processed/test/20250515224733_lorenz_train_data/constant_diffusion_data/128_paths/",
    ]

    test: !!python/tuple [
      "/home/seifner/repos/FIM/data/processed/test/20250515224733_lorenz_train_data/constant_diffusion_data/128_paths/",
    ]

    validation: !!python/tuple [
      "/home/seifner/repos/FIM/data/processed/test/20250515224733_lorenz_train_data/constant_diffusion_data/128_paths/",
       ]

trainer:
  name: Trainer
  debug_iterations: null
  precision: bf16mixed # null fp16 bf16 bf16_mixed fp16_mixed fp32_policy (stick to bf16, maybe try bf16_mixed for more accurate gradients)
  epochs: &epochs 700
  detect_anomaly: false
  save_every: 1
  gradient_accumulation_steps: 1
  best_metric: loss
  logging_format: "RANK_%(rank)s - %(asctime)s - %(name)s - %(levelname)s - %(message)s"
  experiment_dir: ./results/
  # profiler:
  #   num_batches: 40
  #   trace_path: "/home/seifner/repos/FIM/results/profiler/trace.json"

  # evaluation_epoch:
  #   path: fim.trainers.evaluation_epochs.SDEEvaluationPlots
  #   plot_frequency: 1
  #   plot_paths_count: 50

  schedulers: !!python/tuple
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: loss_threshold
      beta: 1000000000.0

    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: vector_field_max_norm
      beta: 1000000000.0

    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: drift_loss_scale
      beta: 1.0

    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: diffusion_loss_scale
      beta: 1.0

    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: kl_loss_scale
      beta: 0.0

    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: short_time_transition_log_likelihood_loss_scale
      beta: 0.0

optimizers: !!python/tuple
  - optimizer_d: # name of the optimizer
      name: torch.optim.AdamW
      lr: &lr_rate 1.0e-6
      weight_decay: 0 # 1.0e-3
      gradient_norm_clipping: 1
      # schedulers: !!python/tuple
      #   - name: torch.optim.lr_scheduler.SequentialLR
      #     step_type: minibatch
      #     schedulers: !!python/tuple
      #       - name: fim.trainers.lr_warmup.LinearWarmupScheduler
      #         warmup_steps: &warmup_steps 10000
      #         target_lr: *lr_rate
      #       # - name: torch.optim.lr_scheduler.CosineAnnealingLR
      #       #   T_max: 450000
      #       #   eta_min: 1.0e-7
      #       #   last_epoch: -1
      #     milestones: !!python/tuple []
