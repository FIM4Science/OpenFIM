experiment:
  name: FIM_Hawkes_1-3st_optimized_mixed_rmse_norm_2000_paths_mixed_250_events_mixed
  name_add_date: true # if true, the current date & time will be added to the experiment name
  seed: [10]
  device_map: auto # auto, cuda, cpu

distributed:
  enabled: false
  sharding_strategy: NO_SHARD # SHARD_GRAD_OP, NO_SHARD, HYBRID_SHARD
  wrap_policy: SIZE_BAZED # NO_POLICY # MODEL_SPECIFIC, SIZE_BAZED
  min_num_params: 1e5
  checkpoint_type: full_state # full_state, local_state
  activation_chekpoint: false

dataset:
  name: HawkesDataLoader
  path:
    train: !!python/tuple
      - data/synthetic_data/hawkes/1k_2D_1k_paths_diag_only_old_params/train
      # - data/synthetic_data/hawkes/1k_3D_1k_paths_diag_only_old_params/train
      # - data/synthetic_data/hawkes/2D_small/val
    validation: !!python/tuple
      - data/synthetic_data/hawkes/1k_2D_1k_paths_diag_only_old_params/val
      # - data/synthetic_data/hawkes/1k_3D_1k_paths_diag_only_old_params/val
      # - data/synthetic_data/hawkes/2D_small/val

  loader_kwargs:
    batch_size: 1
    num_workers: 8
    test_batch_size: 1

    num_inference_paths: 2

    # Training settings - variable paths and sequence lengths
    variable_num_of_paths: false
    min_path_count: 100
    max_path_count: 1000
    max_number_of_minibatch_sizes: 20

    variable_sequence_lens:
      train: false          # Variable sequence lengths for training
      validation: false    # Fixed sequence lengths for consistent validation
    min_sequence_len: 10
    max_sequence_len: 250

    num_inference_times: 50 # Number of times at which to evaluate the intensity function per inference path

  dataset_kwargs:
    files_to_load:
      base_intensity_functions: "base_intensity_functions.pt"
      event_times: "event_times.pt"
      event_types: "event_types.pt"
      kernel_functions: "kernel_functions.pt"
    # data_limit: 50
    field_name_for_dimension_grouping:
      ["base_intensity_functions", "kernel_functions"]

model:
  model_type: fimhawkes
  normalize_times: false
  normalize_by_max_time: false # Otherwise normalize by the delta times

  max_num_marks: &max_num_marks 2
  hidden_dim: &hidden_dim 64

  hidden_act: &hidden_act
    name: &activation torch.nn.GELU

  mark_encoder:
    name: torch.nn.Linear
    out_features: *hidden_dim

  time_encoder:
    name: torch.nn.Linear
    out_features: *hidden_dim

  delta_time_encoder:
    name: torch.nn.Linear
    out_features: *hidden_dim

  intensity_evaluation_time_encoder:
    name: torch.nn.Linear

  evaluation_mark_encoder:
    name: torch.nn.Linear

  context_ts_encoder:
    name: torch.nn.TransformerEncoder
    num_layers: 4

    encoder_layer:
      name: torch.nn.TransformerEncoderLayer
      dropout: 0.1
      nhead: 4
      batch_first: true

  inference_ts_encoder:
    name: torch.nn.TransformerEncoder
    num_layers: 4

    encoder_layer:
      name: torch.nn.TransformerEncoderLayer
      dropout: 0.1
      nhead: 4
      batch_first: true

  functional_attention:
    paths_block_attention: false
    num_res_layers: 2 # TODO: Set this higher for larger models
    attention:
      nhead: 1
    projection:
      name: fim.models.blocks.base.MLP
      hidden_layers: !!python/tuple [*hidden_dim, *hidden_dim]

  # New attention configurations for the paper architecture
  context_self_attention:
    num_heads: 4
    dropout: 0.1
    batch_first: true

  # Separate decoders for Hawkes parameters
  mu_decoder:
    name: fim.models.blocks.base.MLP
    hidden_layers: !!python/tuple [*hidden_dim, *hidden_dim]
    hidden_act: *hidden_act

  alpha_decoder:
    name: fim.models.blocks.base.MLP
    hidden_layers: !!python/tuple [*hidden_dim, *hidden_dim]
    hidden_act: *hidden_act

  beta_decoder:
    name: fim.models.blocks.base.MLP
    hidden_layers: !!python/tuple [*hidden_dim, *hidden_dim]
    hidden_act: *hidden_act

  intensity_decoder:
    name: fim.models.blocks.base.MLP
    hidden_layers: !!python/tuple [*hidden_dim, *hidden_dim]
    hidden_act: *hidden_act

  thinning: null

trainer:
  name: Trainer
  debug_iterations: null
  precision: bf16_mixed # null fp16 bf16 bf16_mixed fp16_mixed fp32_policy
  epochs: &epochs 5000
  detect_anomaly: false
  save_every: 10
  gradient_accumulation_steps: 4
  # hub_model_id: FIM4Science/fim-hawkes-small
  best_metric: loss
  logging_format: "RANK_%(rank)s - %(asctime)s - %(name)s - %(levelname)s - %(message)s"
  experiment_dir: ./results/

  # Add evaluation epoch for TensorBoard plotting
  evaluation_epoch:
    path: fim.trainers.evaluation_epochs.HawkesEvaluationPlots
    plot_frequency: 1
    iterator_name: validation  # Use validation data for plotting
    inference_path_idx: 0  # Which inference path to plot (0 = first path)

  schedulers: !!python/tuple
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      beta: 1.0
      label: gauss_nll
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: init_cross_entropy
      beta: 1.0
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: missing_link
      beta: 1.0

optimizers: !!python/tuple
  - optimizer_d: # name of the optimizer
      name: torch.optim.AdamW
      lr: 0.00001
      weight_decay: 0.0001
      # gradient_norm_clipping: 10
      # schedulers: !!python/tuple
      #   - name: torch.optim.lr_scheduler.CosineAnnealingLR
      #     T_max: *epochs
      #     eta_min: 0.0000001
      #     last_epoch: -1
