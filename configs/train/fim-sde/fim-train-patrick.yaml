experiment:
  name: equal_loss_scale_operator_all_heads_linear_data
  name_add_date: true # if true, the current date & time will be added to the experiment name
  seed: 10
  device_map: cpu # auto, cuda, cpu

distributed:
  enabled: false
  sharding_strategy: NO_SHARD # SHARD_GRAD_OP, NO_SHARD, HYBRID_SHARD
  wrap_policy: SIZE_BAZED # NO_POLICY # MODEL_SPECIFIC, SIZE_BAZED
  min_num_params: 1e5
  checkpoint_type: full_state # full_state, local_state
  activation_chekpoint: false

model:
  model_type: fimsde
  ###################################################################################
  # Input data (corrected by dataloader) (basically not used in FIM libaray, I think)
  max_dimension: &max_dimension 3
  max_time_steps: &max_time_steps 128
  max_location_size: &max_location_size 1024
  max_num_paths: &max_num_of_paths 300

  # General model config
  non_negative_diffusion_by: "clip"
  model_embedding_size: &model_embedding_size 64

  # General transformer config
  residual_ff_size: &residual_ff_size 256
  transformer_layer_activation: &transformer_layer_activation "relu"

  # General MLP config
  hidden_layers: &hidden_layers !!python/tuple [128, 128]
  dropout_rate: &dropout_rate 0.1
  hidden_act: &hidden_act
    name: &activation torch.nn.ReLU

  # Networks
  phi_0x:
    hidden_layers: *hidden_layers
    hidden_act: *hidden_act
    dropout: *dropout_rate

  phi_0_combination: "concatenate" # concatenate, all

  psi_1_transformer_layer:
    nhead: 4
    dim_feedforward: *residual_ff_size
    dropout: *dropout_rate
    activation: *transformer_layer_activation

  psi_1_transformer_encoder:
    num_layers: 4

  phi_1x:
    hidden_layers: *hidden_layers
    hidden_act: *hidden_act
    dropout: *dropout_rate

  operator_specificity: "all" # all, per_concept, per_head

  operator:
    attention:
      locations_as_final_query: false
      nhead: 4
      dim_feedforward: *residual_ff_size
      dropout: *dropout_rate
      activation: *activation

    projection:
      hidden_layers: *hidden_layers
      hidden_act: *hidden_act
      dropout: *dropout_rate


  # instance normalization
  values_norm_min: -1
  values_norm_max: 1
  times_norm_min: 0
  times_norm_max: 1

  # loss regularization
  add_delta_x_to_value_encoder: true
  diffusion_loss_scale: 1.0
  train_with_normalized_head: true
  loss_threshold: 100.0
  loss_filter_nans: true

  # loss
  loss_type: "nll" #nll, rmse
  clip_grad: true
  clip_max_norm: 10.

  # INFERENCE/PIPELINE ------------------------------------------------------------
  dt_pipeline: 0.1
  number_of_time_steps_pipeline: 128
  evaluate_with_unnormalized_heads: true

dataset:
  name: FIMSDEDataloader
  dataset_description: linear

  num_workers: 32

  ###### For data from dynamical systems class
  data_type: theory # synthetic, theory
  data_paths: configs/train/fim-sde/sde-polynomial-systems-hyperparameter.yaml

  ###### For data from files
  # data_type: synthetic # synthetic, theory
  # data_in_files:
  #   obs_times: "obs_times.h5"
  #   obs_values: "obs_values.h5"
  #   locations: "hypercube_locations.h5"
  #   drift_at_locations: "drift_functions_at_hypercube.h5"
  #   diffusion_at_locations: "scaled_diffusion_functions_at_hypercube.h5"
  #
  # data_paths:
  #   train: !!python/tuple [
  #     processed\state_sde\linear\dim-1\1,
  #     processed\state_sde\linear\dim-2\1,
  #     processed\state_sde\linear\dim-3\1]
  #   test: !!python/tuple [processed\state_sde\linear\dim-2\1]
  #   validation: !!python/tuple [processed\state_sde\linear\dim-2\1]

  batch_size: 32

  random_grids:
    train: true
    test: false
    validation: false

  min_num_grid_points: 1
  max_num_grid_points: 1

  random_paths:
    train: true
    test: false
    validation: false

  min_num_paths: 10
  max_num_paths: 100


trainer:
  name: Trainer
  debug_iterations: null
  precision: bf16 # null fp16 bf16 bf16_mixed fp16_mixed fp32_policy
  epochs: &epochs 1000
  detect_anomaly: false
  save_every: 5
  gradient_accumulation_steps: 1
  best_metric: loss
  logging_format: "RANK_%(rank)s - %(asctime)s - %(name)s - %(levelname)s - %(message)s"
  experiment_dir: ./results/
  evaluation_epoch:
    path: fim.trainers.evaluation_epochs.SDEEvaluationPlots
    plot_frequency: 3

  schedulers: !!python/tuple
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      beta: 1.0
      label: gauss_nll
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: init_cross_entropy
      beta: 1.0
    - name: fim.utils.param_scheduler.ConstantScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: missing_link
      beta: 1.0

optimizers: !!python/tuple
  - optimizer_d: # name of the optimizer
      name: torch.optim.AdamW
      lr: 1.0e-4
      weight_decay: 1.0e-4
      # gradient_norm_clipping: 10
      # schedulers: !!python/tuple
      #   - name: torch.optim.lr_scheduler.CosineAnnealingLR
      #     T_max: *epochs
      #     eta_min: 0.0000001
      #     last_epoch: -1
