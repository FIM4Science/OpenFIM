experiment:
  name: DecoderOnly_Test_05-08-1127
  seed: [1]
  device_map: cpu # auto, cuda, cpu

distributed:
  enabled: false

model:
  name: DecoderOnly
  load_in_8bit: false
  load_in_4bit: false
  use_bf16: false

  residual_block_input:
    name: fim.models.blocks.ResidualBlock
    in_features: &patch_len_in 16
    hidden_features: !!python/tuple [&latent_dim 20]
    out_features: *latent_dim
    hidden_act:
      name: torch.nn.ReLU
    output_act:
      name: torch.nn.Identity
    dropout: &dropout 0.1

  positional_encoding:
    name: fim.models.blocks.PositionalEncoding
    d_model: *latent_dim
    max_len: 100

  decoder_block:
    name: fim.models.blocks.DecoderBlock
    number_decoder_blocks: 1
    d_model: *latent_dim
    num_heads: 4
    dropout: *dropout

  residual_block_output:
    name: fim.models.blocks.ResidualBlock
    in_features: *latent_dim
    hidden_features: !!python/tuple [*latent_dim]
    out_features: &prediction_len 8
    hidden_act:
      name: torch.nn.ReLU
    output_act:
      name: torch.nn.Identity
    dropout: *dropout

dataset:
  name: patched_dataloader
  path: monash_tsf
  ds_name: cif_2016 # if null it uses the default cache path set for huggingface
  batch_size: 17
  test_batch_size: 19
  output_fields: !!python/tuple ["input", "output", "mask_point_level", "mask_token_level", "time_feat"]
  loader_kwargs:
    num_workers: 1
  # split: "train"
  dataset_kwargs:
    max_context_len: 48
    patch_len_out: *prediction_len
    patch_len_in: *patch_len_in
    overlap_context_windows: 0

optimizers: !!python/tuple
  - optimizer_d: # name of the optimizer
      name: torch.optim.AdamW
      lr: 0.0001
      weight_decay: 0.0
      gradient_norm_clipping: null
      # schedulers: !!python/tuple
      #   - name: torch.optim.lr_scheduler.StepLR
      #     step_size: 1
      #     gamma: 0.8
      #     step_type: epoch

trainer:
  name: Trainer
  debug_iterations: null
  precision: bf16 # null fp16 bf16 bf16_mixed fp16_mixed fp32_policy
  epochs: 400
  detect_anomaly: false
  save_every: 1
  gradient_accumulation_steps: 1
  best_metric: rmse
  logging_format: "RANK_%(rank)s - %(asctime)s - %(name)s - %(levelname)s - %(message)s"
  experiment_dir: ./results/
  schedulers: !!python/tuple
    - name: fim.utils.param_scheduler.PeriodicScheduler # ExponentialIncrease, ConstantScheduler, PeriodicScheduler
      label: beta_scheduler
    - name: fim.utils.param_scheduler.ExponentialSchedulerGumbel
      label: temperature_scheduler
      init_temperature: 1
      min_temperature: 0.5
      training_fraction_to_reach_min: 0.7
