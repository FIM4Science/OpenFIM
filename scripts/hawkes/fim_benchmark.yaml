# Config for next_event_eval_grid.py
# - 'datasets' accepts local folders (repo Hawkes .pt layout) or HuggingFace ids (e.g., easytpp/retweet)
datasets:
  - taobao
  # - stackoverflow
  # - amazon
  # - taxi
  # - retweet
  # - data/synthetic_data/hawkes/EVAL_10D_2k_context_paths_100_inference_paths_const_base_exp_kernel
  # - data/synthetic_data/hawkes/EVAL_10D_2k_context_paths_100_inference_paths_const_base_exp_kernel_no_interactions
  # - data/synthetic_data/hawkes/EVAL_10D_2k_context_paths_100_inference_paths_poisson
  # - data/synthetic_data/hawkes/EVAL_10D_2k_context_paths_100_inference_paths_sin_base_exp_kernel

# - 'checkpoints' list of FIM-Hawkes checkpoint directories
checkpoints:
  # - results/FIM_Hawkes_10-22st_2000_paths_mixed_100_events_mixed-experiment-seed-10-dataset-dataset_kwargs-field_name_for_dimension_grouping-base_intensity_functions_09-13-1555/checkpoints/best-model
  # Tip: leave the list empty to start from scratch per dataset via fine-tuning.
  # In that case set fine_tune: true and provide finetune_config below.

# Inference-only hyperparameters
# task: next_event | long_horizon | both
task: next_event
# Sampling method for next-event time: thinning | inverse_transform
sampling_method: thinning
context_size: 2000   # null => use full train split
inference_size: null  # null => use full test split
max_num_events: 100   # null => no truncation
sample_idx: 0         # used only for local datasets
num_integration_points: 5000
num_bootstrap_samples: 1000

# Long-horizon parameters (used when task == long_horizon)
forecast_horizon_size: 20           # N future events to generate
num_ensemble_trajectories: 5        # number of independent trajectories for ensembling

# Output
results_root: results/fim_benchmark

# Parallelism
parallel: 1

# Fine-tuning (optional). If enabled, model is fine-tuned per dataset before evaluation
# - For next_event: HF EasyTPP dataset id is used for fine-tune (e.g., easytpp/taobao)
# - For long_horizon: local CDiff dataset path is used (resolved from data/external/CDiff_dataset/<name> if needed)
# - The best validation checkpoint (best-model) is automatically selected for evaluation
# - Each dataset starts fine-tune from the base checkpoint again (no carry-over)
fine_tune: true
# Required when fine_tune: true â€” path to a train YAML used by fim_finetune.py
# When 'checkpoints' is empty, this is also required to build the model from scratch.
finetune_config: configs/train/hawkes/david.yaml
# Fine-tune hyperparameters
fine_tune_lr: 5e-5
fine_tune_epochs: 5000
finetune_val_every: 100 # Only evaluate on val set every 100 epochs
# Optional custom save root for fine-tuning artifacts
# finetune_save_root: results/finetuned_cdiff
